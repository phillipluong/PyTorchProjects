{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import math\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, from_numpy, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Dataset I want to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61874</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61875</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61876</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61877</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61878</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "id                                                                              \n",
       "61874       1       0       0       1       1       0       0       0       0   \n",
       "61875       4       0       0       0       0       0       0       0       0   \n",
       "61876       0       0       0       0       0       0       0       3       1   \n",
       "61877       1       0       0       0       0       0       0       0       0   \n",
       "61878       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       feat_10  ...  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  \\\n",
       "id              ...                                                         \n",
       "61874        0  ...        1        0        0        0        0        0   \n",
       "61875        0  ...        0        2        0        0        2        0   \n",
       "61876        0  ...        0        3        1        0        0        0   \n",
       "61877        0  ...        0        0        0        0        1        0   \n",
       "61878        0  ...        0        0        0        0        0        0   \n",
       "\n",
       "       feat_91  feat_92  feat_93   target  \n",
       "id                                         \n",
       "61874        0        2        0  Class_9  \n",
       "61875        0        1        0  Class_9  \n",
       "61876        0        0        0  Class_9  \n",
       "61877        3       10        0  Class_9  \n",
       "61878        0        2        0  Class_9  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otto.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zero_vals = []\n",
    "for i in range(1,94):\n",
    "    zero_vals.append(otto_train['feat_'+str(i)].value_counts(normalize=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20658340188654256"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-np.mean(zero_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feat_1      int64\n",
       "feat_2      int64\n",
       "feat_3      int64\n",
       "feat_4      int64\n",
       "feat_5      int64\n",
       "feat_6      int64\n",
       "feat_7      int64\n",
       "feat_8      int64\n",
       "feat_9      int64\n",
       "feat_10     int64\n",
       "feat_11     int64\n",
       "feat_12     int64\n",
       "feat_13     int64\n",
       "feat_14     int64\n",
       "feat_15     int64\n",
       "feat_16     int64\n",
       "feat_17     int64\n",
       "feat_18     int64\n",
       "feat_19     int64\n",
       "feat_20     int64\n",
       "feat_21     int64\n",
       "feat_22     int64\n",
       "feat_23     int64\n",
       "feat_24     int64\n",
       "feat_25     int64\n",
       "feat_26     int64\n",
       "feat_27     int64\n",
       "feat_28     int64\n",
       "feat_29     int64\n",
       "feat_30     int64\n",
       "            ...  \n",
       "feat_65     int64\n",
       "feat_66     int64\n",
       "feat_67     int64\n",
       "feat_68     int64\n",
       "feat_69     int64\n",
       "feat_70     int64\n",
       "feat_71     int64\n",
       "feat_72     int64\n",
       "feat_73     int64\n",
       "feat_74     int64\n",
       "feat_75     int64\n",
       "feat_76     int64\n",
       "feat_77     int64\n",
       "feat_78     int64\n",
       "feat_79     int64\n",
       "feat_80     int64\n",
       "feat_81     int64\n",
       "feat_82     int64\n",
       "feat_83     int64\n",
       "feat_84     int64\n",
       "feat_85     int64\n",
       "feat_86     int64\n",
       "feat_87     int64\n",
       "feat_88     int64\n",
       "feat_89     int64\n",
       "feat_90     int64\n",
       "feat_91     int64\n",
       "feat_92     int64\n",
       "feat_93     int64\n",
       "target     object\n",
       "Length: 94, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otto.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "otto_train = pd.read_csv('otto data/train.csv',index_col=0)\n",
    "otto_test = pd.read_csv('otto data/test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = otto.values\n",
    "testx = from_numpy(otto.iloc[:,:-1].values)\n",
    "#testy = from_numpy(test.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61878"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otto.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Class_1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Class_1']\n"
     ]
    }
   ],
   "source": [
    "for i in testy:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "testyy = np.array([int(i[0][-1]) for i in testy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61878,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testyy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(testx[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61878, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OttoDataset(Dataset):\n",
    "    \"\"\"Otto Dataset\"\"\"\n",
    "    def __init__(self):\n",
    "        # data loading\n",
    "        if type == test\n",
    "        xy = pd.read_csv('otto data/train.csv',index_col=0)\n",
    "        x = xy.iloc[:,:-1].values\n",
    "        y = np.array([float(i[-1]) for i in xy.iloc[:,-1].values])\n",
    "        \n",
    "        self.x = from_numpy(x[:,:-1]) # n_samples, n_classes\n",
    "        self.y = from_numpy(np.reshape(y,[-1,1])) # n_samples, 1\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        # dataset[0]\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  2,  0,\n",
      "         0,  0,  0,  1,  0,  4,  1,  1,  0,  0,  2,  0,  0,  0,  0,  0,  1,  0,\n",
      "         0,  0,  0,  1,  0,  5,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  1,\n",
      "         0,  0,  2,  0,  0, 11,  0,  1,  1,  0,  1,  0,  7,  0,  0,  0,  1,  0,\n",
      "         0,  0,  0,  0,  0,  0,  2,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
      "         0,  0]) tensor([1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dataset = OttoDataset()\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OttoDataset()\n",
    "train_loader = DataLoader(dataset = dataset, batch_size = 100, shuffle = True)\n",
    "\n",
    "#for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#    data, target = Variable(data), Variable(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 2.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 2.],\n",
      "        [0., 0., 0.,  ..., 4., 0., 2.]], dtype=torch.float64) tensor([[2.],\n",
      "        [3.],\n",
      "        [9.],\n",
      "        [5.],\n",
      "        [2.],\n",
      "        [9.],\n",
      "        [8.],\n",
      "        [2.],\n",
      "        [6.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [6.],\n",
      "        [4.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [6.],\n",
      "        [6.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [6.],\n",
      "        [9.],\n",
      "        [8.],\n",
      "        [6.],\n",
      "        [2.],\n",
      "        [9.],\n",
      "        [2.],\n",
      "        [7.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [8.],\n",
      "        [3.],\n",
      "        [9.],\n",
      "        [2.],\n",
      "        [9.],\n",
      "        [2.],\n",
      "        [8.],\n",
      "        [8.],\n",
      "        [6.],\n",
      "        [3.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [3.],\n",
      "        [8.],\n",
      "        [4.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [6.],\n",
      "        [9.],\n",
      "        [8.],\n",
      "        [9.],\n",
      "        [7.],\n",
      "        [9.],\n",
      "        [3.],\n",
      "        [9.],\n",
      "        [1.],\n",
      "        [9.],\n",
      "        [2.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [4.],\n",
      "        [6.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [3.],\n",
      "        [6.],\n",
      "        [6.],\n",
      "        [3.],\n",
      "        [2.],\n",
      "        [8.],\n",
      "        [2.],\n",
      "        [6.],\n",
      "        [2.],\n",
      "        [6.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [6.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [6.],\n",
      "        [6.],\n",
      "        [2.],\n",
      "        [8.],\n",
      "        [9.],\n",
      "        [7.],\n",
      "        [6.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [6.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "features, labels = data\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 92]), torch.Size([100]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features), type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features[0][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61878 619\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iters = math.ceil(len(dataset)/100)\n",
    "print(total_samples, n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/2, step 50/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 1/2, step 100/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 1/2, step 150/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 1/2, step 200/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 1/2, step 250/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 1/2, step 300/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 1/2, step 350/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 1/2, step 400/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 1/2, step 450/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 1/2, step 500/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 1/2, step 550/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 1/2, step 600/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 1/2, step 619/619, inputs: torch.Size([78, 92]), labels: torch.Size([78, 1])\n",
      "epoch 2/2, step 50/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 2/2, step 100/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 2/2, step 150/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 2/2, step 200/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 2/2, step 250/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 2/2, step 300/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 2/2, step 350/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 2/2, step 400/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 2/2, step 450/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 2/2, step 500/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 2/2, step 550/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 2/2, step 600/619, inputs: torch.Size([100, 92]), labels: torch.Size([100, 1])\n",
      "epoch 2/2, step 619/619, inputs: torch.Size([78, 92]), labels: torch.Size([78, 1])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # forward, backwartd pass , update\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iters}, inputs: {inputs.shape}, labels: {labels.shape}')\n",
    "        elif i == 618:\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iters}, inputs: {inputs.shape}, labels: {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(93, 64)\n",
    "        self.l2 = nn.Linear(64, 32)\n",
    "        self.l3 = nn.Linear(32, 16)\n",
    "        self.l4 = nn.Linear(16, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 92) \n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        return self.l4(x)\n",
    "    \n",
    "model = Net()\n",
    "#model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.float())\n",
    "        loss = criterion(output.float(), target.flatten().long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        #data, target = data.to(device), target.to(device)\n",
    "        output = model(data.float())\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output.float(), target.flatten().long()).item()\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim = True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred).long()).cpu().sum()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'==========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
    "          f'({100. * correct / len(test_loader.dataset):0f}%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/61878 (0%) | Loss: 0.570013\n",
      "Train Epoch: 1 | Batch Status: 10000/61878 (16%) | Loss: 0.419442\n",
      "Train Epoch: 1 | Batch Status: 20000/61878 (32%) | Loss: 0.411547\n",
      "Train Epoch: 1 | Batch Status: 30000/61878 (48%) | Loss: 0.595786\n",
      "Train Epoch: 1 | Batch Status: 40000/61878 (65%) | Loss: 0.503838\n",
      "Train Epoch: 1 | Batch Status: 50000/61878 (81%) | Loss: 0.378311\n",
      "Train Epoch: 1 | Batch Status: 60000/61878 (97%) | Loss: 0.507220\n",
      "Training time: 0m 3s\n",
      "==========================\n",
      "Test set: Average loss: 0.0051, Accuracy: 49296/61878(79.000000%)\n",
      "Testing time: 0m 4s\n",
      "Train Epoch: 2 | Batch Status: 0/61878 (0%) | Loss: 0.470518\n",
      "Train Epoch: 2 | Batch Status: 10000/61878 (16%) | Loss: 0.673569\n",
      "Train Epoch: 2 | Batch Status: 20000/61878 (32%) | Loss: 0.622158\n",
      "Train Epoch: 2 | Batch Status: 30000/61878 (48%) | Loss: 0.378887\n",
      "Train Epoch: 2 | Batch Status: 40000/61878 (65%) | Loss: 0.452009\n",
      "Train Epoch: 2 | Batch Status: 50000/61878 (81%) | Loss: 0.617561\n",
      "Train Epoch: 2 | Batch Status: 60000/61878 (97%) | Loss: 0.646164\n",
      "Training time: 0m 3s\n",
      "==========================\n",
      "Test set: Average loss: 0.0051, Accuracy: 49488/61878(79.000000%)\n",
      "Testing time: 0m 4s\n",
      "Train Epoch: 3 | Batch Status: 0/61878 (0%) | Loss: 0.666786\n",
      "Train Epoch: 3 | Batch Status: 10000/61878 (16%) | Loss: 0.511700\n",
      "Train Epoch: 3 | Batch Status: 20000/61878 (32%) | Loss: 0.391995\n",
      "Train Epoch: 3 | Batch Status: 30000/61878 (48%) | Loss: 0.526961\n",
      "Train Epoch: 3 | Batch Status: 40000/61878 (65%) | Loss: 0.535162\n",
      "Train Epoch: 3 | Batch Status: 50000/61878 (81%) | Loss: 0.559079\n",
      "Train Epoch: 3 | Batch Status: 60000/61878 (97%) | Loss: 0.495559\n",
      "Training time: 0m 3s\n",
      "==========================\n",
      "Test set: Average loss: 0.0050, Accuracy: 49572/61878(80.000000%)\n",
      "Testing time: 0m 4s\n",
      "Train Epoch: 4 | Batch Status: 0/61878 (0%) | Loss: 0.358139\n",
      "Train Epoch: 4 | Batch Status: 10000/61878 (16%) | Loss: 0.557337\n",
      "Train Epoch: 4 | Batch Status: 20000/61878 (32%) | Loss: 0.534414\n",
      "Train Epoch: 4 | Batch Status: 30000/61878 (48%) | Loss: 0.426408\n",
      "Train Epoch: 4 | Batch Status: 40000/61878 (65%) | Loss: 0.463751\n",
      "Train Epoch: 4 | Batch Status: 50000/61878 (81%) | Loss: 0.641544\n",
      "Train Epoch: 4 | Batch Status: 60000/61878 (97%) | Loss: 0.569394\n",
      "Training time: 0m 3s\n",
      "==========================\n",
      "Test set: Average loss: 0.0050, Accuracy: 49613/61878(80.000000%)\n",
      "Testing time: 0m 4s\n",
      "Train Epoch: 5 | Batch Status: 0/61878 (0%) | Loss: 0.441062\n",
      "Train Epoch: 5 | Batch Status: 10000/61878 (16%) | Loss: 0.481566\n",
      "Train Epoch: 5 | Batch Status: 20000/61878 (32%) | Loss: 0.370460\n",
      "Train Epoch: 5 | Batch Status: 30000/61878 (48%) | Loss: 0.464109\n",
      "Train Epoch: 5 | Batch Status: 40000/61878 (65%) | Loss: 0.469708\n",
      "Train Epoch: 5 | Batch Status: 50000/61878 (81%) | Loss: 0.478872\n",
      "Train Epoch: 5 | Batch Status: 60000/61878 (97%) | Loss: 0.594341\n",
      "Training time: 0m 3s\n",
      "==========================\n",
      "Test set: Average loss: 0.0050, Accuracy: 49735/61878(80.000000%)\n",
      "Testing time: 0m 4s\n",
      "Train Epoch: 6 | Batch Status: 0/61878 (0%) | Loss: 0.636835\n",
      "Train Epoch: 6 | Batch Status: 10000/61878 (16%) | Loss: 0.517751\n",
      "Train Epoch: 6 | Batch Status: 20000/61878 (32%) | Loss: 0.396079\n",
      "Train Epoch: 6 | Batch Status: 30000/61878 (48%) | Loss: 0.485199\n",
      "Train Epoch: 6 | Batch Status: 40000/61878 (65%) | Loss: 0.501944\n",
      "Train Epoch: 6 | Batch Status: 50000/61878 (81%) | Loss: 0.491264\n",
      "Train Epoch: 6 | Batch Status: 60000/61878 (97%) | Loss: 0.444828\n",
      "Training time: 0m 3s\n",
      "==========================\n",
      "Test set: Average loss: 0.0050, Accuracy: 49478/61878(79.000000%)\n",
      "Testing time: 0m 4s\n",
      "Train Epoch: 7 | Batch Status: 0/61878 (0%) | Loss: 0.491316\n",
      "Train Epoch: 7 | Batch Status: 10000/61878 (16%) | Loss: 0.456602\n",
      "Train Epoch: 7 | Batch Status: 20000/61878 (32%) | Loss: 0.495287\n",
      "Train Epoch: 7 | Batch Status: 30000/61878 (48%) | Loss: 0.521980\n",
      "Train Epoch: 7 | Batch Status: 40000/61878 (65%) | Loss: 0.482126\n",
      "Train Epoch: 7 | Batch Status: 50000/61878 (81%) | Loss: 0.540107\n",
      "Train Epoch: 7 | Batch Status: 60000/61878 (97%) | Loss: 0.552080\n",
      "Training time: 0m 3s\n",
      "==========================\n",
      "Test set: Average loss: 0.0049, Accuracy: 49933/61878(80.000000%)\n",
      "Testing time: 0m 4s\n",
      "Train Epoch: 8 | Batch Status: 0/61878 (0%) | Loss: 0.431612\n",
      "Train Epoch: 8 | Batch Status: 10000/61878 (16%) | Loss: 0.634023\n",
      "Train Epoch: 8 | Batch Status: 20000/61878 (32%) | Loss: 0.566013\n",
      "Train Epoch: 8 | Batch Status: 30000/61878 (48%) | Loss: 0.497302\n",
      "Train Epoch: 8 | Batch Status: 40000/61878 (65%) | Loss: 0.469564\n",
      "Train Epoch: 8 | Batch Status: 50000/61878 (81%) | Loss: 0.547908\n",
      "Train Epoch: 8 | Batch Status: 60000/61878 (97%) | Loss: 0.582873\n",
      "Training time: 0m 3s\n",
      "==========================\n",
      "Test set: Average loss: 0.0048, Accuracy: 50024/61878(80.000000%)\n",
      "Testing time: 0m 4s\n",
      "Train Epoch: 9 | Batch Status: 0/61878 (0%) | Loss: 0.336349\n",
      "Train Epoch: 9 | Batch Status: 10000/61878 (16%) | Loss: 0.424005\n",
      "Train Epoch: 9 | Batch Status: 20000/61878 (32%) | Loss: 0.511268\n",
      "Train Epoch: 9 | Batch Status: 30000/61878 (48%) | Loss: 0.519834\n",
      "Train Epoch: 9 | Batch Status: 40000/61878 (65%) | Loss: 0.613280\n",
      "Train Epoch: 9 | Batch Status: 50000/61878 (81%) | Loss: 0.463191\n",
      "Train Epoch: 9 | Batch Status: 60000/61878 (97%) | Loss: 0.366863\n",
      "Training time: 0m 3s\n",
      "==========================\n",
      "Test set: Average loss: 0.0049, Accuracy: 49819/61878(80.000000%)\n",
      "Testing time: 0m 4s\n",
      "Total Time: 0m 39s\n",
      "Model was trained on cpu!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    since = time.time()\n",
    "    for epoch in range(1,10):\n",
    "        epoch_start = time.time()\n",
    "        train(epoch)\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
    "        test()\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
    "    m, s = divmod(time.time() - since, 60)\n",
    "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on cpu!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2145, -0.0859,  0.1038,  0.0694, -0.2115,  0.1153, -0.2023, -0.1296,\n",
       "          0.1964]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(features.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2549, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(model(features.float()), labels.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset, batch_size = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 2., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64) tensor([[2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [2.],\n",
      "        [6.],\n",
      "        [3.],\n",
      "        [8.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [9.],\n",
      "        [6.],\n",
      "        [2.],\n",
      "        [8.],\n",
      "        [2.],\n",
      "        [5.],\n",
      "        [2.],\n",
      "        [7.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [6.],\n",
      "        [3.],\n",
      "        [5.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [8.],\n",
      "        [9.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [6.],\n",
      "        [9.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [2.],\n",
      "        [9.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [9.],\n",
      "        [2.],\n",
      "        [7.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [6.],\n",
      "        [2.],\n",
      "        [8.],\n",
      "        [3.],\n",
      "        [5.],\n",
      "        [8.],\n",
      "        [8.],\n",
      "        [3.],\n",
      "        [2.],\n",
      "        [6.],\n",
      "        [6.],\n",
      "        [6.],\n",
      "        [6.],\n",
      "        [2.],\n",
      "        [9.],\n",
      "        [8.],\n",
      "        [6.],\n",
      "        [2.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [2.],\n",
      "        [6.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [9.],\n",
      "        [6.],\n",
      "        [6.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [2.],\n",
      "        [7.],\n",
      "        [4.],\n",
      "        [2.],\n",
      "        [6.],\n",
      "        [8.],\n",
      "        [8.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [6.],\n",
      "        [8.],\n",
      "        [6.],\n",
      "        [2.],\n",
      "        [6.],\n",
      "        [9.],\n",
      "        [8.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [2.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "features, labels = data\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 9])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(features.float()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.long().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.],\n",
       "        [6.],\n",
       "        [8.],\n",
       "        [8.],\n",
       "        [6.],\n",
       "        [8.],\n",
       "        [3.],\n",
       "        [2.],\n",
       "        [8.],\n",
       "        [8.],\n",
       "        [8.],\n",
       "        [6.],\n",
       "        [9.],\n",
       "        [9.],\n",
       "        [6.],\n",
       "        [5.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [6.],\n",
       "        [2.],\n",
       "        [6.],\n",
       "        [2.],\n",
       "        [6.],\n",
       "        [1.],\n",
       "        [9.],\n",
       "        [3.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [5.],\n",
       "        [8.],\n",
       "        [9.],\n",
       "        [5.],\n",
       "        [8.],\n",
       "        [8.],\n",
       "        [3.],\n",
       "        [5.],\n",
       "        [8.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [6.],\n",
       "        [9.],\n",
       "        [2.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [2.],\n",
       "        [6.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [2.],\n",
       "        [9.],\n",
       "        [2.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [6.],\n",
       "        [9.],\n",
       "        [2.],\n",
       "        [9.],\n",
       "        [2.],\n",
       "        [8.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [2.],\n",
       "        [7.],\n",
       "        [6.],\n",
       "        [8.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [8.],\n",
       "        [3.],\n",
       "        [9.],\n",
       "        [9.],\n",
       "        [8.],\n",
       "        [6.],\n",
       "        [2.],\n",
       "        [8.],\n",
       "        [9.],\n",
       "        [7.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [6.],\n",
       "        [2.],\n",
       "        [6.],\n",
       "        [8.],\n",
       "        [3.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [2.],\n",
       "        [6.],\n",
       "        [2.],\n",
       "        [8.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [2.],\n",
       "        [8.],\n",
       "        [8.],\n",
       "        [5.],\n",
       "        [2.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4, 2, 6, 3, 8, 2, 2, 2, 2, 9, 6, 2, 8, 2, 5, 2, 7, 2, 2, 6, 3, 5,\n",
       "        3, 3, 5, 6, 8, 9, 2, 1, 2, 1, 6, 7, 6, 9, 5, 6, 2, 9, 2, 2, 2, 9, 2, 7,\n",
       "        2, 2, 6, 2, 8, 3, 5, 8, 8, 3, 2, 6, 6, 6, 6, 2, 9, 8, 6, 2, 6, 7, 2, 6,\n",
       "        5, 6, 9, 6, 6, 2, 2, 2, 3, 2, 7, 4, 2, 6, 8, 8, 2, 2, 6, 8, 6, 2, 6, 9,\n",
       "        8, 2, 2, 2])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.flatten().long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 100])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(features.float()).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at C:\\w\\1\\s\\windows\\pytorch\\aten\\src\\THNN/generic/ClassNLLCriterion.c:94",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-52fd963073a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1994\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1996\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1822\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   1823\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1824\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1825\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1826\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at C:\\w\\1\\s\\windows\\pytorch\\aten\\src\\THNN/generic/ClassNLLCriterion.c:94"
     ]
    }
   ],
   "source": [
    "F.cross_entropy(model(features.float()), labels.flatten().long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "output = Variable(torch.randn(10, 120).float())\n",
    "target = Variable(torch.FloatTensor(10).uniform_(0, 120).long())\n",
    "\n",
    "loss = criterion(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 120])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0061)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('otto data/train.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61878, 94)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-183-80e815586337>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-183-80e815586337>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "y = np.array([float(i[-1]) for i in test_data.iloc[:,-1].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean it up a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class OttoDataset(Dataset):\n",
    "    \"\"\"Otto Dataset\"\"\"\n",
    "    def __init__(self):\n",
    "        # data loading\n",
    "        xy = pd.read_csv('otto data/train.csv',index_col=0)\n",
    "        x = xy.iloc[:,:-1].values\n",
    "        y = np.array([float(i[-1]) for i in xy.iloc[:,-1].values])\n",
    "        \n",
    "        self.x = from_numpy(x[:,:-1]) # n_samples, n_classes\n",
    "        self.y = from_numpy(np.reshape(y,[-1,1])) # n_samples, 1\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        # dataset[0]\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OttoDataset(Dataset):\n",
    "    \"\"\"Otto Dataset\"\"\"\n",
    "    def __init__(self,data):\n",
    "        # data loading\n",
    "        xy = data\n",
    "        x = xy.iloc[:,:-1].values\n",
    "        y = np.array([float(i[-1])-1 for i in xy.iloc[:,-1].values])\n",
    "        \n",
    "        self.x = from_numpy(x) # n_samples, n_classes\n",
    "        self.y = from_numpy(np.reshape(y,[-1,1])) # n_samples, 1\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        # dataset[0]\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(93, 64)\n",
    "        self.l2 = nn.Linear(64, 32)\n",
    "        self.l3 = nn.Linear(32, 16)\n",
    "        self.l4 = nn.Linear(16, 9)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 93) \n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        return self.l4(x)\n",
    "    \n",
    "model = Net()\n",
    "#model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.float())\n",
    "        loss = criterion(output.float(), target.flatten().long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        #data, target = data.to(device), target.to(device)\n",
    "        output = model(data.float())\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output.float(), target.flatten().long()).item()\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim = True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred).long()).cpu().sum()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'==========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
    "          f'({100. * correct / len(test_loader.dataset):0f}%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61878, 94)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otto.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "otto_data = pd.read_csv('otto data/train.csv',index_col=0)\n",
    "separation = int(otto_data.shape[0] * 0.8)\n",
    "\n",
    "train_d = otto_data[:separation]\n",
    "test_d = otto_data[separation:]\n",
    "\n",
    "train_dataset = OttoDataset(train_d)\n",
    "test_dataset = OttoDataset(test_d)\n",
    "\n",
    "#train_dataset = OttoDataset(ttype = 'train' )\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 10, shuffle = True)\n",
    "\n",
    "#test_dataset = OttoDataset(ttype = 'test' )\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61878, 94)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otto_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([61878, 93])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_numpy(otto_data.iloc[:,:-1].values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49502, 12376)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49502"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61878"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61878"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/49502 (0%) | Loss: 1.020064\n",
      "Train Epoch: 1 | Batch Status: 1000/49502 (2%) | Loss: 0.621129\n",
      "Train Epoch: 1 | Batch Status: 2000/49502 (4%) | Loss: 0.898892\n",
      "Train Epoch: 1 | Batch Status: 3000/49502 (6%) | Loss: 0.675087\n",
      "Train Epoch: 1 | Batch Status: 4000/49502 (8%) | Loss: 0.311570\n",
      "Train Epoch: 1 | Batch Status: 5000/49502 (10%) | Loss: 0.478637\n",
      "Train Epoch: 1 | Batch Status: 6000/49502 (12%) | Loss: 1.561888\n",
      "Train Epoch: 1 | Batch Status: 7000/49502 (14%) | Loss: 0.437606\n",
      "Train Epoch: 1 | Batch Status: 8000/49502 (16%) | Loss: 0.672012\n",
      "Train Epoch: 1 | Batch Status: 9000/49502 (18%) | Loss: 0.743315\n",
      "Train Epoch: 1 | Batch Status: 10000/49502 (20%) | Loss: 0.579570\n",
      "Train Epoch: 1 | Batch Status: 11000/49502 (22%) | Loss: 0.558787\n",
      "Train Epoch: 1 | Batch Status: 12000/49502 (24%) | Loss: 1.047848\n",
      "Train Epoch: 1 | Batch Status: 13000/49502 (26%) | Loss: 0.675954\n",
      "Train Epoch: 1 | Batch Status: 14000/49502 (28%) | Loss: 0.381573\n",
      "Train Epoch: 1 | Batch Status: 15000/49502 (30%) | Loss: 0.341895\n",
      "Train Epoch: 1 | Batch Status: 16000/49502 (32%) | Loss: 0.530717\n",
      "Train Epoch: 1 | Batch Status: 17000/49502 (34%) | Loss: 0.831877\n",
      "Train Epoch: 1 | Batch Status: 18000/49502 (36%) | Loss: 0.739717\n",
      "Train Epoch: 1 | Batch Status: 19000/49502 (38%) | Loss: 0.727374\n",
      "Train Epoch: 1 | Batch Status: 20000/49502 (40%) | Loss: 0.496091\n",
      "Train Epoch: 1 | Batch Status: 21000/49502 (42%) | Loss: 1.025456\n",
      "Train Epoch: 1 | Batch Status: 22000/49502 (44%) | Loss: 0.470766\n",
      "Train Epoch: 1 | Batch Status: 23000/49502 (46%) | Loss: 0.580161\n",
      "Train Epoch: 1 | Batch Status: 24000/49502 (48%) | Loss: 0.494525\n",
      "Train Epoch: 1 | Batch Status: 25000/49502 (50%) | Loss: 0.369950\n",
      "Train Epoch: 1 | Batch Status: 26000/49502 (53%) | Loss: 0.896293\n",
      "Train Epoch: 1 | Batch Status: 27000/49502 (55%) | Loss: 0.461582\n",
      "Train Epoch: 1 | Batch Status: 28000/49502 (57%) | Loss: 0.391932\n",
      "Train Epoch: 1 | Batch Status: 29000/49502 (59%) | Loss: 0.115655\n",
      "Train Epoch: 1 | Batch Status: 30000/49502 (61%) | Loss: 0.453892\n",
      "Train Epoch: 1 | Batch Status: 31000/49502 (63%) | Loss: 0.444074\n",
      "Train Epoch: 1 | Batch Status: 32000/49502 (65%) | Loss: 0.282518\n",
      "Train Epoch: 1 | Batch Status: 33000/49502 (67%) | Loss: 0.490178\n",
      "Train Epoch: 1 | Batch Status: 34000/49502 (69%) | Loss: 0.528390\n",
      "Train Epoch: 1 | Batch Status: 35000/49502 (71%) | Loss: 1.378523\n",
      "Train Epoch: 1 | Batch Status: 36000/49502 (73%) | Loss: 0.240463\n",
      "Train Epoch: 1 | Batch Status: 37000/49502 (75%) | Loss: 0.313007\n",
      "Train Epoch: 1 | Batch Status: 38000/49502 (77%) | Loss: 0.394055\n",
      "Train Epoch: 1 | Batch Status: 39000/49502 (79%) | Loss: 0.668739\n",
      "Train Epoch: 1 | Batch Status: 40000/49502 (81%) | Loss: 0.595004\n",
      "Train Epoch: 1 | Batch Status: 41000/49502 (83%) | Loss: 0.327543\n",
      "Train Epoch: 1 | Batch Status: 42000/49502 (85%) | Loss: 0.556877\n",
      "Train Epoch: 1 | Batch Status: 43000/49502 (87%) | Loss: 0.471502\n",
      "Train Epoch: 1 | Batch Status: 44000/49502 (89%) | Loss: 1.083374\n",
      "Train Epoch: 1 | Batch Status: 45000/49502 (91%) | Loss: 0.844792\n",
      "Train Epoch: 1 | Batch Status: 46000/49502 (93%) | Loss: 0.239570\n",
      "Train Epoch: 1 | Batch Status: 47000/49502 (95%) | Loss: 0.353849\n",
      "Train Epoch: 1 | Batch Status: 48000/49502 (97%) | Loss: 0.235489\n",
      "Train Epoch: 1 | Batch Status: 49000/49502 (99%) | Loss: 0.345795\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.6649, Accuracy: 4236/12376(34.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 2 | Batch Status: 0/49502 (0%) | Loss: 1.019869\n",
      "Train Epoch: 2 | Batch Status: 1000/49502 (2%) | Loss: 0.176422\n",
      "Train Epoch: 2 | Batch Status: 2000/49502 (4%) | Loss: 0.531119\n",
      "Train Epoch: 2 | Batch Status: 3000/49502 (6%) | Loss: 0.144604\n",
      "Train Epoch: 2 | Batch Status: 4000/49502 (8%) | Loss: 0.266424\n",
      "Train Epoch: 2 | Batch Status: 5000/49502 (10%) | Loss: 0.665551\n",
      "Train Epoch: 2 | Batch Status: 6000/49502 (12%) | Loss: 0.237935\n",
      "Train Epoch: 2 | Batch Status: 7000/49502 (14%) | Loss: 0.287231\n",
      "Train Epoch: 2 | Batch Status: 8000/49502 (16%) | Loss: 1.236327\n",
      "Train Epoch: 2 | Batch Status: 9000/49502 (18%) | Loss: 0.962938\n",
      "Train Epoch: 2 | Batch Status: 10000/49502 (20%) | Loss: 0.352831\n",
      "Train Epoch: 2 | Batch Status: 11000/49502 (22%) | Loss: 0.421484\n",
      "Train Epoch: 2 | Batch Status: 12000/49502 (24%) | Loss: 0.515391\n",
      "Train Epoch: 2 | Batch Status: 13000/49502 (26%) | Loss: 0.335439\n",
      "Train Epoch: 2 | Batch Status: 14000/49502 (28%) | Loss: 0.748980\n",
      "Train Epoch: 2 | Batch Status: 15000/49502 (30%) | Loss: 0.405868\n",
      "Train Epoch: 2 | Batch Status: 16000/49502 (32%) | Loss: 0.443001\n",
      "Train Epoch: 2 | Batch Status: 17000/49502 (34%) | Loss: 0.859155\n",
      "Train Epoch: 2 | Batch Status: 18000/49502 (36%) | Loss: 0.762037\n",
      "Train Epoch: 2 | Batch Status: 19000/49502 (38%) | Loss: 0.670449\n",
      "Train Epoch: 2 | Batch Status: 20000/49502 (40%) | Loss: 0.373973\n",
      "Train Epoch: 2 | Batch Status: 21000/49502 (42%) | Loss: 0.746526\n",
      "Train Epoch: 2 | Batch Status: 22000/49502 (44%) | Loss: 0.735891\n",
      "Train Epoch: 2 | Batch Status: 23000/49502 (46%) | Loss: 0.433175\n",
      "Train Epoch: 2 | Batch Status: 24000/49502 (48%) | Loss: 0.378870\n",
      "Train Epoch: 2 | Batch Status: 25000/49502 (50%) | Loss: 0.649610\n",
      "Train Epoch: 2 | Batch Status: 26000/49502 (53%) | Loss: 0.779192\n",
      "Train Epoch: 2 | Batch Status: 27000/49502 (55%) | Loss: 0.250761\n",
      "Train Epoch: 2 | Batch Status: 28000/49502 (57%) | Loss: 0.719642\n",
      "Train Epoch: 2 | Batch Status: 29000/49502 (59%) | Loss: 0.564372\n",
      "Train Epoch: 2 | Batch Status: 30000/49502 (61%) | Loss: 0.265126\n",
      "Train Epoch: 2 | Batch Status: 31000/49502 (63%) | Loss: 0.378177\n",
      "Train Epoch: 2 | Batch Status: 32000/49502 (65%) | Loss: 0.275033\n",
      "Train Epoch: 2 | Batch Status: 33000/49502 (67%) | Loss: 0.666867\n",
      "Train Epoch: 2 | Batch Status: 34000/49502 (69%) | Loss: 0.763979\n",
      "Train Epoch: 2 | Batch Status: 35000/49502 (71%) | Loss: 0.304659\n",
      "Train Epoch: 2 | Batch Status: 36000/49502 (73%) | Loss: 0.536352\n",
      "Train Epoch: 2 | Batch Status: 37000/49502 (75%) | Loss: 0.560336\n",
      "Train Epoch: 2 | Batch Status: 38000/49502 (77%) | Loss: 0.432172\n",
      "Train Epoch: 2 | Batch Status: 39000/49502 (79%) | Loss: 0.225325\n",
      "Train Epoch: 2 | Batch Status: 40000/49502 (81%) | Loss: 0.258258\n",
      "Train Epoch: 2 | Batch Status: 41000/49502 (83%) | Loss: 0.380803\n",
      "Train Epoch: 2 | Batch Status: 42000/49502 (85%) | Loss: 0.456059\n",
      "Train Epoch: 2 | Batch Status: 43000/49502 (87%) | Loss: 0.221210\n",
      "Train Epoch: 2 | Batch Status: 44000/49502 (89%) | Loss: 0.631526\n",
      "Train Epoch: 2 | Batch Status: 45000/49502 (91%) | Loss: 1.060625\n",
      "Train Epoch: 2 | Batch Status: 46000/49502 (93%) | Loss: 0.431107\n",
      "Train Epoch: 2 | Batch Status: 47000/49502 (95%) | Loss: 0.825057\n",
      "Train Epoch: 2 | Batch Status: 48000/49502 (97%) | Loss: 0.543662\n",
      "Train Epoch: 2 | Batch Status: 49000/49502 (99%) | Loss: 0.731716\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.7782, Accuracy: 4470/12376(36.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 3 | Batch Status: 0/49502 (0%) | Loss: 0.908848\n",
      "Train Epoch: 3 | Batch Status: 1000/49502 (2%) | Loss: 0.519616\n",
      "Train Epoch: 3 | Batch Status: 2000/49502 (4%) | Loss: 0.693649\n",
      "Train Epoch: 3 | Batch Status: 3000/49502 (6%) | Loss: 0.337949\n",
      "Train Epoch: 3 | Batch Status: 4000/49502 (8%) | Loss: 0.369528\n",
      "Train Epoch: 3 | Batch Status: 5000/49502 (10%) | Loss: 0.133340\n",
      "Train Epoch: 3 | Batch Status: 6000/49502 (12%) | Loss: 0.696933\n",
      "Train Epoch: 3 | Batch Status: 7000/49502 (14%) | Loss: 0.415176\n",
      "Train Epoch: 3 | Batch Status: 8000/49502 (16%) | Loss: 0.132822\n",
      "Train Epoch: 3 | Batch Status: 9000/49502 (18%) | Loss: 0.418607\n",
      "Train Epoch: 3 | Batch Status: 10000/49502 (20%) | Loss: 0.405414\n",
      "Train Epoch: 3 | Batch Status: 11000/49502 (22%) | Loss: 0.483308\n",
      "Train Epoch: 3 | Batch Status: 12000/49502 (24%) | Loss: 0.219390\n",
      "Train Epoch: 3 | Batch Status: 13000/49502 (26%) | Loss: 0.124032\n",
      "Train Epoch: 3 | Batch Status: 14000/49502 (28%) | Loss: 0.728228\n",
      "Train Epoch: 3 | Batch Status: 15000/49502 (30%) | Loss: 0.141156\n",
      "Train Epoch: 3 | Batch Status: 16000/49502 (32%) | Loss: 0.721937\n",
      "Train Epoch: 3 | Batch Status: 17000/49502 (34%) | Loss: 0.358369\n",
      "Train Epoch: 3 | Batch Status: 18000/49502 (36%) | Loss: 0.445239\n",
      "Train Epoch: 3 | Batch Status: 19000/49502 (38%) | Loss: 0.470150\n",
      "Train Epoch: 3 | Batch Status: 20000/49502 (40%) | Loss: 0.113036\n",
      "Train Epoch: 3 | Batch Status: 21000/49502 (42%) | Loss: 0.592158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 | Batch Status: 22000/49502 (44%) | Loss: 0.389745\n",
      "Train Epoch: 3 | Batch Status: 23000/49502 (46%) | Loss: 0.281074\n",
      "Train Epoch: 3 | Batch Status: 24000/49502 (48%) | Loss: 0.412938\n",
      "Train Epoch: 3 | Batch Status: 25000/49502 (50%) | Loss: 0.408824\n",
      "Train Epoch: 3 | Batch Status: 26000/49502 (53%) | Loss: 0.567568\n",
      "Train Epoch: 3 | Batch Status: 27000/49502 (55%) | Loss: 0.340418\n",
      "Train Epoch: 3 | Batch Status: 28000/49502 (57%) | Loss: 0.482592\n",
      "Train Epoch: 3 | Batch Status: 29000/49502 (59%) | Loss: 0.634702\n",
      "Train Epoch: 3 | Batch Status: 30000/49502 (61%) | Loss: 0.553387\n",
      "Train Epoch: 3 | Batch Status: 31000/49502 (63%) | Loss: 0.431890\n",
      "Train Epoch: 3 | Batch Status: 32000/49502 (65%) | Loss: 0.859478\n",
      "Train Epoch: 3 | Batch Status: 33000/49502 (67%) | Loss: 0.444045\n",
      "Train Epoch: 3 | Batch Status: 34000/49502 (69%) | Loss: 0.301146\n",
      "Train Epoch: 3 | Batch Status: 35000/49502 (71%) | Loss: 0.337806\n",
      "Train Epoch: 3 | Batch Status: 36000/49502 (73%) | Loss: 0.372957\n",
      "Train Epoch: 3 | Batch Status: 37000/49502 (75%) | Loss: 0.334796\n",
      "Train Epoch: 3 | Batch Status: 38000/49502 (77%) | Loss: 0.605694\n",
      "Train Epoch: 3 | Batch Status: 39000/49502 (79%) | Loss: 0.371240\n",
      "Train Epoch: 3 | Batch Status: 40000/49502 (81%) | Loss: 0.545899\n",
      "Train Epoch: 3 | Batch Status: 41000/49502 (83%) | Loss: 0.640529\n",
      "Train Epoch: 3 | Batch Status: 42000/49502 (85%) | Loss: 0.713917\n",
      "Train Epoch: 3 | Batch Status: 43000/49502 (87%) | Loss: 1.142283\n",
      "Train Epoch: 3 | Batch Status: 44000/49502 (89%) | Loss: 0.935960\n",
      "Train Epoch: 3 | Batch Status: 45000/49502 (91%) | Loss: 0.479069\n",
      "Train Epoch: 3 | Batch Status: 46000/49502 (93%) | Loss: 1.425414\n",
      "Train Epoch: 3 | Batch Status: 47000/49502 (95%) | Loss: 1.585152\n",
      "Train Epoch: 3 | Batch Status: 48000/49502 (97%) | Loss: 1.301655\n",
      "Train Epoch: 3 | Batch Status: 49000/49502 (99%) | Loss: 0.209602\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.6990, Accuracy: 6360/12376(51.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 4 | Batch Status: 0/49502 (0%) | Loss: 0.599875\n",
      "Train Epoch: 4 | Batch Status: 1000/49502 (2%) | Loss: 0.674473\n",
      "Train Epoch: 4 | Batch Status: 2000/49502 (4%) | Loss: 0.795833\n",
      "Train Epoch: 4 | Batch Status: 3000/49502 (6%) | Loss: 0.194376\n",
      "Train Epoch: 4 | Batch Status: 4000/49502 (8%) | Loss: 0.736957\n",
      "Train Epoch: 4 | Batch Status: 5000/49502 (10%) | Loss: 0.982560\n",
      "Train Epoch: 4 | Batch Status: 6000/49502 (12%) | Loss: 1.182090\n",
      "Train Epoch: 4 | Batch Status: 7000/49502 (14%) | Loss: 0.558315\n",
      "Train Epoch: 4 | Batch Status: 8000/49502 (16%) | Loss: 0.563195\n",
      "Train Epoch: 4 | Batch Status: 9000/49502 (18%) | Loss: 0.694190\n",
      "Train Epoch: 4 | Batch Status: 10000/49502 (20%) | Loss: 0.456234\n",
      "Train Epoch: 4 | Batch Status: 11000/49502 (22%) | Loss: 0.612564\n",
      "Train Epoch: 4 | Batch Status: 12000/49502 (24%) | Loss: 0.418204\n",
      "Train Epoch: 4 | Batch Status: 13000/49502 (26%) | Loss: 0.572861\n",
      "Train Epoch: 4 | Batch Status: 14000/49502 (28%) | Loss: 0.246929\n",
      "Train Epoch: 4 | Batch Status: 15000/49502 (30%) | Loss: 0.826269\n",
      "Train Epoch: 4 | Batch Status: 16000/49502 (32%) | Loss: 0.535491\n",
      "Train Epoch: 4 | Batch Status: 17000/49502 (34%) | Loss: 0.385135\n",
      "Train Epoch: 4 | Batch Status: 18000/49502 (36%) | Loss: 0.746795\n",
      "Train Epoch: 4 | Batch Status: 19000/49502 (38%) | Loss: 0.345742\n",
      "Train Epoch: 4 | Batch Status: 20000/49502 (40%) | Loss: 0.260208\n",
      "Train Epoch: 4 | Batch Status: 21000/49502 (42%) | Loss: 0.365798\n",
      "Train Epoch: 4 | Batch Status: 22000/49502 (44%) | Loss: 0.366499\n",
      "Train Epoch: 4 | Batch Status: 23000/49502 (46%) | Loss: 0.153966\n",
      "Train Epoch: 4 | Batch Status: 24000/49502 (48%) | Loss: 0.268695\n",
      "Train Epoch: 4 | Batch Status: 25000/49502 (50%) | Loss: 0.682196\n",
      "Train Epoch: 4 | Batch Status: 26000/49502 (53%) | Loss: 0.549092\n",
      "Train Epoch: 4 | Batch Status: 27000/49502 (55%) | Loss: 0.486217\n",
      "Train Epoch: 4 | Batch Status: 28000/49502 (57%) | Loss: 0.694529\n",
      "Train Epoch: 4 | Batch Status: 29000/49502 (59%) | Loss: 0.186967\n",
      "Train Epoch: 4 | Batch Status: 30000/49502 (61%) | Loss: 0.483776\n",
      "Train Epoch: 4 | Batch Status: 31000/49502 (63%) | Loss: 0.391268\n",
      "Train Epoch: 4 | Batch Status: 32000/49502 (65%) | Loss: 0.372495\n",
      "Train Epoch: 4 | Batch Status: 33000/49502 (67%) | Loss: 0.417429\n",
      "Train Epoch: 4 | Batch Status: 34000/49502 (69%) | Loss: 0.687869\n",
      "Train Epoch: 4 | Batch Status: 35000/49502 (71%) | Loss: 0.396312\n",
      "Train Epoch: 4 | Batch Status: 36000/49502 (73%) | Loss: 0.404332\n",
      "Train Epoch: 4 | Batch Status: 37000/49502 (75%) | Loss: 1.070771\n",
      "Train Epoch: 4 | Batch Status: 38000/49502 (77%) | Loss: 0.268743\n",
      "Train Epoch: 4 | Batch Status: 39000/49502 (79%) | Loss: 1.025884\n",
      "Train Epoch: 4 | Batch Status: 40000/49502 (81%) | Loss: 0.632399\n",
      "Train Epoch: 4 | Batch Status: 41000/49502 (83%) | Loss: 0.357503\n",
      "Train Epoch: 4 | Batch Status: 42000/49502 (85%) | Loss: 0.490598\n",
      "Train Epoch: 4 | Batch Status: 43000/49502 (87%) | Loss: 0.283133\n",
      "Train Epoch: 4 | Batch Status: 44000/49502 (89%) | Loss: 0.539690\n",
      "Train Epoch: 4 | Batch Status: 45000/49502 (91%) | Loss: 0.583860\n",
      "Train Epoch: 4 | Batch Status: 46000/49502 (93%) | Loss: 0.650713\n",
      "Train Epoch: 4 | Batch Status: 47000/49502 (95%) | Loss: 0.741384\n",
      "Train Epoch: 4 | Batch Status: 48000/49502 (97%) | Loss: 0.333933\n",
      "Train Epoch: 4 | Batch Status: 49000/49502 (99%) | Loss: 0.261360\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.7157, Accuracy: 5338/12376(43.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 5 | Batch Status: 0/49502 (0%) | Loss: 0.596141\n",
      "Train Epoch: 5 | Batch Status: 1000/49502 (2%) | Loss: 0.992345\n",
      "Train Epoch: 5 | Batch Status: 2000/49502 (4%) | Loss: 0.626287\n",
      "Train Epoch: 5 | Batch Status: 3000/49502 (6%) | Loss: 0.636530\n",
      "Train Epoch: 5 | Batch Status: 4000/49502 (8%) | Loss: 0.236145\n",
      "Train Epoch: 5 | Batch Status: 5000/49502 (10%) | Loss: 0.592450\n",
      "Train Epoch: 5 | Batch Status: 6000/49502 (12%) | Loss: 0.290814\n",
      "Train Epoch: 5 | Batch Status: 7000/49502 (14%) | Loss: 1.428189\n",
      "Train Epoch: 5 | Batch Status: 8000/49502 (16%) | Loss: 0.692116\n",
      "Train Epoch: 5 | Batch Status: 9000/49502 (18%) | Loss: 0.643267\n",
      "Train Epoch: 5 | Batch Status: 10000/49502 (20%) | Loss: 0.219331\n",
      "Train Epoch: 5 | Batch Status: 11000/49502 (22%) | Loss: 0.224542\n",
      "Train Epoch: 5 | Batch Status: 12000/49502 (24%) | Loss: 0.575441\n",
      "Train Epoch: 5 | Batch Status: 13000/49502 (26%) | Loss: 0.812284\n",
      "Train Epoch: 5 | Batch Status: 14000/49502 (28%) | Loss: 0.997162\n",
      "Train Epoch: 5 | Batch Status: 15000/49502 (30%) | Loss: 1.199899\n",
      "Train Epoch: 5 | Batch Status: 16000/49502 (32%) | Loss: 0.863741\n",
      "Train Epoch: 5 | Batch Status: 17000/49502 (34%) | Loss: 0.532670\n",
      "Train Epoch: 5 | Batch Status: 18000/49502 (36%) | Loss: 0.117036\n",
      "Train Epoch: 5 | Batch Status: 19000/49502 (38%) | Loss: 0.343910\n",
      "Train Epoch: 5 | Batch Status: 20000/49502 (40%) | Loss: 0.348841\n",
      "Train Epoch: 5 | Batch Status: 21000/49502 (42%) | Loss: 0.625392\n",
      "Train Epoch: 5 | Batch Status: 22000/49502 (44%) | Loss: 0.401053\n",
      "Train Epoch: 5 | Batch Status: 23000/49502 (46%) | Loss: 0.285816\n",
      "Train Epoch: 5 | Batch Status: 24000/49502 (48%) | Loss: 0.359251\n",
      "Train Epoch: 5 | Batch Status: 25000/49502 (50%) | Loss: 0.452885\n",
      "Train Epoch: 5 | Batch Status: 26000/49502 (53%) | Loss: 0.347513\n",
      "Train Epoch: 5 | Batch Status: 27000/49502 (55%) | Loss: 0.324473\n",
      "Train Epoch: 5 | Batch Status: 28000/49502 (57%) | Loss: 0.611522\n",
      "Train Epoch: 5 | Batch Status: 29000/49502 (59%) | Loss: 0.804645\n",
      "Train Epoch: 5 | Batch Status: 30000/49502 (61%) | Loss: 0.397583\n",
      "Train Epoch: 5 | Batch Status: 31000/49502 (63%) | Loss: 0.352461\n",
      "Train Epoch: 5 | Batch Status: 32000/49502 (65%) | Loss: 0.479664\n",
      "Train Epoch: 5 | Batch Status: 33000/49502 (67%) | Loss: 0.696745\n",
      "Train Epoch: 5 | Batch Status: 34000/49502 (69%) | Loss: 0.372440\n",
      "Train Epoch: 5 | Batch Status: 35000/49502 (71%) | Loss: 0.566629\n",
      "Train Epoch: 5 | Batch Status: 36000/49502 (73%) | Loss: 0.563055\n",
      "Train Epoch: 5 | Batch Status: 37000/49502 (75%) | Loss: 1.030226\n",
      "Train Epoch: 5 | Batch Status: 38000/49502 (77%) | Loss: 0.251868\n",
      "Train Epoch: 5 | Batch Status: 39000/49502 (79%) | Loss: 0.471716\n",
      "Train Epoch: 5 | Batch Status: 40000/49502 (81%) | Loss: 0.784378\n",
      "Train Epoch: 5 | Batch Status: 41000/49502 (83%) | Loss: 0.819944\n",
      "Train Epoch: 5 | Batch Status: 42000/49502 (85%) | Loss: 0.543147\n",
      "Train Epoch: 5 | Batch Status: 43000/49502 (87%) | Loss: 0.440247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 | Batch Status: 44000/49502 (89%) | Loss: 0.316411\n",
      "Train Epoch: 5 | Batch Status: 45000/49502 (91%) | Loss: 0.286108\n",
      "Train Epoch: 5 | Batch Status: 46000/49502 (93%) | Loss: 0.339663\n",
      "Train Epoch: 5 | Batch Status: 47000/49502 (95%) | Loss: 0.290468\n",
      "Train Epoch: 5 | Batch Status: 48000/49502 (97%) | Loss: 0.712580\n",
      "Train Epoch: 5 | Batch Status: 49000/49502 (99%) | Loss: 0.478507\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.7679, Accuracy: 5497/12376(44.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 6 | Batch Status: 0/49502 (0%) | Loss: 0.681140\n",
      "Train Epoch: 6 | Batch Status: 1000/49502 (2%) | Loss: 0.516837\n",
      "Train Epoch: 6 | Batch Status: 2000/49502 (4%) | Loss: 0.775009\n",
      "Train Epoch: 6 | Batch Status: 3000/49502 (6%) | Loss: 0.276829\n",
      "Train Epoch: 6 | Batch Status: 4000/49502 (8%) | Loss: 0.219327\n",
      "Train Epoch: 6 | Batch Status: 5000/49502 (10%) | Loss: 0.621556\n",
      "Train Epoch: 6 | Batch Status: 6000/49502 (12%) | Loss: 0.365552\n",
      "Train Epoch: 6 | Batch Status: 7000/49502 (14%) | Loss: 0.373073\n",
      "Train Epoch: 6 | Batch Status: 8000/49502 (16%) | Loss: 0.138189\n",
      "Train Epoch: 6 | Batch Status: 9000/49502 (18%) | Loss: 0.622773\n",
      "Train Epoch: 6 | Batch Status: 10000/49502 (20%) | Loss: 0.223908\n",
      "Train Epoch: 6 | Batch Status: 11000/49502 (22%) | Loss: 0.928793\n",
      "Train Epoch: 6 | Batch Status: 12000/49502 (24%) | Loss: 0.143362\n",
      "Train Epoch: 6 | Batch Status: 13000/49502 (26%) | Loss: 0.399864\n",
      "Train Epoch: 6 | Batch Status: 14000/49502 (28%) | Loss: 0.406782\n",
      "Train Epoch: 6 | Batch Status: 15000/49502 (30%) | Loss: 0.167911\n",
      "Train Epoch: 6 | Batch Status: 16000/49502 (32%) | Loss: 0.402376\n",
      "Train Epoch: 6 | Batch Status: 17000/49502 (34%) | Loss: 0.445740\n",
      "Train Epoch: 6 | Batch Status: 18000/49502 (36%) | Loss: 0.287312\n",
      "Train Epoch: 6 | Batch Status: 19000/49502 (38%) | Loss: 0.342216\n",
      "Train Epoch: 6 | Batch Status: 20000/49502 (40%) | Loss: 0.633559\n",
      "Train Epoch: 6 | Batch Status: 21000/49502 (42%) | Loss: 0.287065\n",
      "Train Epoch: 6 | Batch Status: 22000/49502 (44%) | Loss: 0.288443\n",
      "Train Epoch: 6 | Batch Status: 23000/49502 (46%) | Loss: 0.419079\n",
      "Train Epoch: 6 | Batch Status: 24000/49502 (48%) | Loss: 0.129457\n",
      "Train Epoch: 6 | Batch Status: 25000/49502 (50%) | Loss: 0.705682\n",
      "Train Epoch: 6 | Batch Status: 26000/49502 (53%) | Loss: 0.502495\n",
      "Train Epoch: 6 | Batch Status: 27000/49502 (55%) | Loss: 0.303073\n",
      "Train Epoch: 6 | Batch Status: 28000/49502 (57%) | Loss: 0.279005\n",
      "Train Epoch: 6 | Batch Status: 29000/49502 (59%) | Loss: 0.260487\n",
      "Train Epoch: 6 | Batch Status: 30000/49502 (61%) | Loss: 0.389434\n",
      "Train Epoch: 6 | Batch Status: 31000/49502 (63%) | Loss: 0.686401\n",
      "Train Epoch: 6 | Batch Status: 32000/49502 (65%) | Loss: 0.578495\n",
      "Train Epoch: 6 | Batch Status: 33000/49502 (67%) | Loss: 0.901697\n",
      "Train Epoch: 6 | Batch Status: 34000/49502 (69%) | Loss: 0.437720\n",
      "Train Epoch: 6 | Batch Status: 35000/49502 (71%) | Loss: 0.270007\n",
      "Train Epoch: 6 | Batch Status: 36000/49502 (73%) | Loss: 0.836400\n",
      "Train Epoch: 6 | Batch Status: 37000/49502 (75%) | Loss: 0.231352\n",
      "Train Epoch: 6 | Batch Status: 38000/49502 (77%) | Loss: 0.461363\n",
      "Train Epoch: 6 | Batch Status: 39000/49502 (79%) | Loss: 0.507212\n",
      "Train Epoch: 6 | Batch Status: 40000/49502 (81%) | Loss: 0.170857\n",
      "Train Epoch: 6 | Batch Status: 41000/49502 (83%) | Loss: 0.290875\n",
      "Train Epoch: 6 | Batch Status: 42000/49502 (85%) | Loss: 0.199699\n",
      "Train Epoch: 6 | Batch Status: 43000/49502 (87%) | Loss: 0.185669\n",
      "Train Epoch: 6 | Batch Status: 44000/49502 (89%) | Loss: 0.822618\n",
      "Train Epoch: 6 | Batch Status: 45000/49502 (91%) | Loss: 0.185905\n",
      "Train Epoch: 6 | Batch Status: 46000/49502 (93%) | Loss: 0.749707\n",
      "Train Epoch: 6 | Batch Status: 47000/49502 (95%) | Loss: 0.315785\n",
      "Train Epoch: 6 | Batch Status: 48000/49502 (97%) | Loss: 0.649460\n",
      "Train Epoch: 6 | Batch Status: 49000/49502 (99%) | Loss: 0.525925\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.7787, Accuracy: 5611/12376(45.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 7 | Batch Status: 0/49502 (0%) | Loss: 0.308305\n",
      "Train Epoch: 7 | Batch Status: 1000/49502 (2%) | Loss: 0.698209\n",
      "Train Epoch: 7 | Batch Status: 2000/49502 (4%) | Loss: 0.165273\n",
      "Train Epoch: 7 | Batch Status: 3000/49502 (6%) | Loss: 0.266174\n",
      "Train Epoch: 7 | Batch Status: 4000/49502 (8%) | Loss: 0.558121\n",
      "Train Epoch: 7 | Batch Status: 5000/49502 (10%) | Loss: 0.674594\n",
      "Train Epoch: 7 | Batch Status: 6000/49502 (12%) | Loss: 0.421815\n",
      "Train Epoch: 7 | Batch Status: 7000/49502 (14%) | Loss: 0.375407\n",
      "Train Epoch: 7 | Batch Status: 8000/49502 (16%) | Loss: 0.723761\n",
      "Train Epoch: 7 | Batch Status: 9000/49502 (18%) | Loss: 0.338988\n",
      "Train Epoch: 7 | Batch Status: 10000/49502 (20%) | Loss: 0.329889\n",
      "Train Epoch: 7 | Batch Status: 11000/49502 (22%) | Loss: 0.309012\n",
      "Train Epoch: 7 | Batch Status: 12000/49502 (24%) | Loss: 0.438812\n",
      "Train Epoch: 7 | Batch Status: 13000/49502 (26%) | Loss: 0.094017\n",
      "Train Epoch: 7 | Batch Status: 14000/49502 (28%) | Loss: 0.682209\n",
      "Train Epoch: 7 | Batch Status: 15000/49502 (30%) | Loss: 0.488873\n",
      "Train Epoch: 7 | Batch Status: 16000/49502 (32%) | Loss: 0.381403\n",
      "Train Epoch: 7 | Batch Status: 17000/49502 (34%) | Loss: 0.565028\n",
      "Train Epoch: 7 | Batch Status: 18000/49502 (36%) | Loss: 0.400132\n",
      "Train Epoch: 7 | Batch Status: 19000/49502 (38%) | Loss: 0.426930\n",
      "Train Epoch: 7 | Batch Status: 20000/49502 (40%) | Loss: 0.338547\n",
      "Train Epoch: 7 | Batch Status: 21000/49502 (42%) | Loss: 0.576904\n",
      "Train Epoch: 7 | Batch Status: 22000/49502 (44%) | Loss: 0.616983\n",
      "Train Epoch: 7 | Batch Status: 23000/49502 (46%) | Loss: 0.422262\n",
      "Train Epoch: 7 | Batch Status: 24000/49502 (48%) | Loss: 0.158182\n",
      "Train Epoch: 7 | Batch Status: 25000/49502 (50%) | Loss: 0.508079\n",
      "Train Epoch: 7 | Batch Status: 26000/49502 (53%) | Loss: 0.327940\n",
      "Train Epoch: 7 | Batch Status: 27000/49502 (55%) | Loss: 0.564373\n",
      "Train Epoch: 7 | Batch Status: 28000/49502 (57%) | Loss: 0.230848\n",
      "Train Epoch: 7 | Batch Status: 29000/49502 (59%) | Loss: 0.739938\n",
      "Train Epoch: 7 | Batch Status: 30000/49502 (61%) | Loss: 0.173288\n",
      "Train Epoch: 7 | Batch Status: 31000/49502 (63%) | Loss: 0.137317\n",
      "Train Epoch: 7 | Batch Status: 32000/49502 (65%) | Loss: 0.362467\n",
      "Train Epoch: 7 | Batch Status: 33000/49502 (67%) | Loss: 0.461080\n",
      "Train Epoch: 7 | Batch Status: 34000/49502 (69%) | Loss: 0.797588\n",
      "Train Epoch: 7 | Batch Status: 35000/49502 (71%) | Loss: 0.754041\n",
      "Train Epoch: 7 | Batch Status: 36000/49502 (73%) | Loss: 0.260889\n",
      "Train Epoch: 7 | Batch Status: 37000/49502 (75%) | Loss: 0.403031\n",
      "Train Epoch: 7 | Batch Status: 38000/49502 (77%) | Loss: 0.225947\n",
      "Train Epoch: 7 | Batch Status: 39000/49502 (79%) | Loss: 0.340101\n",
      "Train Epoch: 7 | Batch Status: 40000/49502 (81%) | Loss: 1.463647\n",
      "Train Epoch: 7 | Batch Status: 41000/49502 (83%) | Loss: 0.584969\n",
      "Train Epoch: 7 | Batch Status: 42000/49502 (85%) | Loss: 0.350256\n",
      "Train Epoch: 7 | Batch Status: 43000/49502 (87%) | Loss: 0.804455\n",
      "Train Epoch: 7 | Batch Status: 44000/49502 (89%) | Loss: 0.779479\n",
      "Train Epoch: 7 | Batch Status: 45000/49502 (91%) | Loss: 0.257199\n",
      "Train Epoch: 7 | Batch Status: 46000/49502 (93%) | Loss: 0.397548\n",
      "Train Epoch: 7 | Batch Status: 47000/49502 (95%) | Loss: 0.489411\n",
      "Train Epoch: 7 | Batch Status: 48000/49502 (97%) | Loss: 0.813005\n",
      "Train Epoch: 7 | Batch Status: 49000/49502 (99%) | Loss: 1.111111\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.7837, Accuracy: 5650/12376(45.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 8 | Batch Status: 0/49502 (0%) | Loss: 0.319143\n",
      "Train Epoch: 8 | Batch Status: 1000/49502 (2%) | Loss: 0.665219\n",
      "Train Epoch: 8 | Batch Status: 2000/49502 (4%) | Loss: 0.553152\n",
      "Train Epoch: 8 | Batch Status: 3000/49502 (6%) | Loss: 0.468423\n",
      "Train Epoch: 8 | Batch Status: 4000/49502 (8%) | Loss: 0.676791\n",
      "Train Epoch: 8 | Batch Status: 5000/49502 (10%) | Loss: 0.796904\n",
      "Train Epoch: 8 | Batch Status: 6000/49502 (12%) | Loss: 0.514428\n",
      "Train Epoch: 8 | Batch Status: 7000/49502 (14%) | Loss: 0.111948\n",
      "Train Epoch: 8 | Batch Status: 8000/49502 (16%) | Loss: 1.084695\n",
      "Train Epoch: 8 | Batch Status: 9000/49502 (18%) | Loss: 0.466033\n",
      "Train Epoch: 8 | Batch Status: 10000/49502 (20%) | Loss: 0.435530\n",
      "Train Epoch: 8 | Batch Status: 11000/49502 (22%) | Loss: 0.758285\n",
      "Train Epoch: 8 | Batch Status: 12000/49502 (24%) | Loss: 0.742830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 | Batch Status: 13000/49502 (26%) | Loss: 0.500585\n",
      "Train Epoch: 8 | Batch Status: 14000/49502 (28%) | Loss: 0.667825\n",
      "Train Epoch: 8 | Batch Status: 15000/49502 (30%) | Loss: 0.223427\n",
      "Train Epoch: 8 | Batch Status: 16000/49502 (32%) | Loss: 0.488982\n",
      "Train Epoch: 8 | Batch Status: 17000/49502 (34%) | Loss: 0.519603\n",
      "Train Epoch: 8 | Batch Status: 18000/49502 (36%) | Loss: 0.915470\n",
      "Train Epoch: 8 | Batch Status: 19000/49502 (38%) | Loss: 0.523344\n",
      "Train Epoch: 8 | Batch Status: 20000/49502 (40%) | Loss: 0.369057\n",
      "Train Epoch: 8 | Batch Status: 21000/49502 (42%) | Loss: 0.382301\n",
      "Train Epoch: 8 | Batch Status: 22000/49502 (44%) | Loss: 0.176573\n",
      "Train Epoch: 8 | Batch Status: 23000/49502 (46%) | Loss: 0.720140\n",
      "Train Epoch: 8 | Batch Status: 24000/49502 (48%) | Loss: 0.247849\n",
      "Train Epoch: 8 | Batch Status: 25000/49502 (50%) | Loss: 0.205517\n",
      "Train Epoch: 8 | Batch Status: 26000/49502 (53%) | Loss: 0.404623\n",
      "Train Epoch: 8 | Batch Status: 27000/49502 (55%) | Loss: 0.601872\n",
      "Train Epoch: 8 | Batch Status: 28000/49502 (57%) | Loss: 0.307975\n",
      "Train Epoch: 8 | Batch Status: 29000/49502 (59%) | Loss: 0.224434\n",
      "Train Epoch: 8 | Batch Status: 30000/49502 (61%) | Loss: 0.129364\n",
      "Train Epoch: 8 | Batch Status: 31000/49502 (63%) | Loss: 0.432043\n",
      "Train Epoch: 8 | Batch Status: 32000/49502 (65%) | Loss: 0.868873\n",
      "Train Epoch: 8 | Batch Status: 33000/49502 (67%) | Loss: 0.774963\n",
      "Train Epoch: 8 | Batch Status: 34000/49502 (69%) | Loss: 0.943810\n",
      "Train Epoch: 8 | Batch Status: 35000/49502 (71%) | Loss: 0.295833\n",
      "Train Epoch: 8 | Batch Status: 36000/49502 (73%) | Loss: 0.529496\n",
      "Train Epoch: 8 | Batch Status: 37000/49502 (75%) | Loss: 0.437876\n",
      "Train Epoch: 8 | Batch Status: 38000/49502 (77%) | Loss: 0.462823\n",
      "Train Epoch: 8 | Batch Status: 39000/49502 (79%) | Loss: 0.262803\n",
      "Train Epoch: 8 | Batch Status: 40000/49502 (81%) | Loss: 0.194620\n",
      "Train Epoch: 8 | Batch Status: 41000/49502 (83%) | Loss: 0.564317\n",
      "Train Epoch: 8 | Batch Status: 42000/49502 (85%) | Loss: 0.193828\n",
      "Train Epoch: 8 | Batch Status: 43000/49502 (87%) | Loss: 0.381328\n",
      "Train Epoch: 8 | Batch Status: 44000/49502 (89%) | Loss: 0.652049\n",
      "Train Epoch: 8 | Batch Status: 45000/49502 (91%) | Loss: 0.841490\n",
      "Train Epoch: 8 | Batch Status: 46000/49502 (93%) | Loss: 0.427219\n",
      "Train Epoch: 8 | Batch Status: 47000/49502 (95%) | Loss: 0.623193\n",
      "Train Epoch: 8 | Batch Status: 48000/49502 (97%) | Loss: 0.282567\n",
      "Train Epoch: 8 | Batch Status: 49000/49502 (99%) | Loss: 0.134627\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.7856, Accuracy: 6318/12376(51.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 9 | Batch Status: 0/49502 (0%) | Loss: 0.181509\n",
      "Train Epoch: 9 | Batch Status: 1000/49502 (2%) | Loss: 0.732786\n",
      "Train Epoch: 9 | Batch Status: 2000/49502 (4%) | Loss: 0.497031\n",
      "Train Epoch: 9 | Batch Status: 3000/49502 (6%) | Loss: 0.453823\n",
      "Train Epoch: 9 | Batch Status: 4000/49502 (8%) | Loss: 0.629582\n",
      "Train Epoch: 9 | Batch Status: 5000/49502 (10%) | Loss: 0.420568\n",
      "Train Epoch: 9 | Batch Status: 6000/49502 (12%) | Loss: 0.832907\n",
      "Train Epoch: 9 | Batch Status: 7000/49502 (14%) | Loss: 0.506668\n",
      "Train Epoch: 9 | Batch Status: 8000/49502 (16%) | Loss: 0.211691\n",
      "Train Epoch: 9 | Batch Status: 9000/49502 (18%) | Loss: 0.592769\n",
      "Train Epoch: 9 | Batch Status: 10000/49502 (20%) | Loss: 1.157898\n",
      "Train Epoch: 9 | Batch Status: 11000/49502 (22%) | Loss: 0.739874\n",
      "Train Epoch: 9 | Batch Status: 12000/49502 (24%) | Loss: 1.024035\n",
      "Train Epoch: 9 | Batch Status: 13000/49502 (26%) | Loss: 0.604207\n",
      "Train Epoch: 9 | Batch Status: 14000/49502 (28%) | Loss: 0.394622\n",
      "Train Epoch: 9 | Batch Status: 15000/49502 (30%) | Loss: 0.510175\n",
      "Train Epoch: 9 | Batch Status: 16000/49502 (32%) | Loss: 0.503110\n",
      "Train Epoch: 9 | Batch Status: 17000/49502 (34%) | Loss: 0.359554\n",
      "Train Epoch: 9 | Batch Status: 18000/49502 (36%) | Loss: 0.245411\n",
      "Train Epoch: 9 | Batch Status: 19000/49502 (38%) | Loss: 1.103591\n",
      "Train Epoch: 9 | Batch Status: 20000/49502 (40%) | Loss: 0.902451\n",
      "Train Epoch: 9 | Batch Status: 21000/49502 (42%) | Loss: 0.540208\n",
      "Train Epoch: 9 | Batch Status: 22000/49502 (44%) | Loss: 0.350636\n",
      "Train Epoch: 9 | Batch Status: 23000/49502 (46%) | Loss: 1.490948\n",
      "Train Epoch: 9 | Batch Status: 24000/49502 (48%) | Loss: 0.591886\n",
      "Train Epoch: 9 | Batch Status: 25000/49502 (50%) | Loss: 1.224760\n",
      "Train Epoch: 9 | Batch Status: 26000/49502 (53%) | Loss: 0.237626\n",
      "Train Epoch: 9 | Batch Status: 27000/49502 (55%) | Loss: 0.634981\n",
      "Train Epoch: 9 | Batch Status: 28000/49502 (57%) | Loss: 0.538306\n",
      "Train Epoch: 9 | Batch Status: 29000/49502 (59%) | Loss: 0.921761\n",
      "Train Epoch: 9 | Batch Status: 30000/49502 (61%) | Loss: 0.209924\n",
      "Train Epoch: 9 | Batch Status: 31000/49502 (63%) | Loss: 0.264394\n",
      "Train Epoch: 9 | Batch Status: 32000/49502 (65%) | Loss: 0.347856\n",
      "Train Epoch: 9 | Batch Status: 33000/49502 (67%) | Loss: 0.741297\n",
      "Train Epoch: 9 | Batch Status: 34000/49502 (69%) | Loss: 0.456980\n",
      "Train Epoch: 9 | Batch Status: 35000/49502 (71%) | Loss: 0.042291\n",
      "Train Epoch: 9 | Batch Status: 36000/49502 (73%) | Loss: 0.189270\n",
      "Train Epoch: 9 | Batch Status: 37000/49502 (75%) | Loss: 0.713075\n",
      "Train Epoch: 9 | Batch Status: 38000/49502 (77%) | Loss: 0.492440\n",
      "Train Epoch: 9 | Batch Status: 39000/49502 (79%) | Loss: 0.770557\n",
      "Train Epoch: 9 | Batch Status: 40000/49502 (81%) | Loss: 0.194176\n",
      "Train Epoch: 9 | Batch Status: 41000/49502 (83%) | Loss: 0.800335\n",
      "Train Epoch: 9 | Batch Status: 42000/49502 (85%) | Loss: 1.504169\n",
      "Train Epoch: 9 | Batch Status: 43000/49502 (87%) | Loss: 0.483339\n",
      "Train Epoch: 9 | Batch Status: 44000/49502 (89%) | Loss: 0.429748\n",
      "Train Epoch: 9 | Batch Status: 45000/49502 (91%) | Loss: 0.329335\n",
      "Train Epoch: 9 | Batch Status: 46000/49502 (93%) | Loss: 0.771693\n",
      "Train Epoch: 9 | Batch Status: 47000/49502 (95%) | Loss: 0.484227\n",
      "Train Epoch: 9 | Batch Status: 48000/49502 (97%) | Loss: 0.350127\n",
      "Train Epoch: 9 | Batch Status: 49000/49502 (99%) | Loss: 0.117431\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.7530, Accuracy: 5494/12376(44.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 10 | Batch Status: 0/49502 (0%) | Loss: 0.479737\n",
      "Train Epoch: 10 | Batch Status: 1000/49502 (2%) | Loss: 0.561811\n",
      "Train Epoch: 10 | Batch Status: 2000/49502 (4%) | Loss: 0.444835\n",
      "Train Epoch: 10 | Batch Status: 3000/49502 (6%) | Loss: 0.729240\n",
      "Train Epoch: 10 | Batch Status: 4000/49502 (8%) | Loss: 0.421830\n",
      "Train Epoch: 10 | Batch Status: 5000/49502 (10%) | Loss: 0.607135\n",
      "Train Epoch: 10 | Batch Status: 6000/49502 (12%) | Loss: 0.168122\n",
      "Train Epoch: 10 | Batch Status: 7000/49502 (14%) | Loss: 0.293078\n",
      "Train Epoch: 10 | Batch Status: 8000/49502 (16%) | Loss: 0.150122\n",
      "Train Epoch: 10 | Batch Status: 9000/49502 (18%) | Loss: 0.339691\n",
      "Train Epoch: 10 | Batch Status: 10000/49502 (20%) | Loss: 0.287403\n",
      "Train Epoch: 10 | Batch Status: 11000/49502 (22%) | Loss: 0.794311\n",
      "Train Epoch: 10 | Batch Status: 12000/49502 (24%) | Loss: 0.546329\n",
      "Train Epoch: 10 | Batch Status: 13000/49502 (26%) | Loss: 0.364411\n",
      "Train Epoch: 10 | Batch Status: 14000/49502 (28%) | Loss: 0.473383\n",
      "Train Epoch: 10 | Batch Status: 15000/49502 (30%) | Loss: 1.095394\n",
      "Train Epoch: 10 | Batch Status: 16000/49502 (32%) | Loss: 0.532447\n",
      "Train Epoch: 10 | Batch Status: 17000/49502 (34%) | Loss: 0.355473\n",
      "Train Epoch: 10 | Batch Status: 18000/49502 (36%) | Loss: 0.339039\n",
      "Train Epoch: 10 | Batch Status: 19000/49502 (38%) | Loss: 1.072997\n",
      "Train Epoch: 10 | Batch Status: 20000/49502 (40%) | Loss: 0.285817\n",
      "Train Epoch: 10 | Batch Status: 21000/49502 (42%) | Loss: 0.703428\n",
      "Train Epoch: 10 | Batch Status: 22000/49502 (44%) | Loss: 0.316679\n",
      "Train Epoch: 10 | Batch Status: 23000/49502 (46%) | Loss: 0.368240\n",
      "Train Epoch: 10 | Batch Status: 24000/49502 (48%) | Loss: 0.287624\n",
      "Train Epoch: 10 | Batch Status: 25000/49502 (50%) | Loss: 0.478818\n",
      "Train Epoch: 10 | Batch Status: 26000/49502 (53%) | Loss: 1.011017\n",
      "Train Epoch: 10 | Batch Status: 27000/49502 (55%) | Loss: 0.260292\n",
      "Train Epoch: 10 | Batch Status: 28000/49502 (57%) | Loss: 0.227451\n",
      "Train Epoch: 10 | Batch Status: 29000/49502 (59%) | Loss: 0.472967\n",
      "Train Epoch: 10 | Batch Status: 30000/49502 (61%) | Loss: 0.816884\n",
      "Train Epoch: 10 | Batch Status: 31000/49502 (63%) | Loss: 0.375877\n",
      "Train Epoch: 10 | Batch Status: 32000/49502 (65%) | Loss: 0.164486\n",
      "Train Epoch: 10 | Batch Status: 33000/49502 (67%) | Loss: 0.513880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 | Batch Status: 34000/49502 (69%) | Loss: 0.375992\n",
      "Train Epoch: 10 | Batch Status: 35000/49502 (71%) | Loss: 0.220309\n",
      "Train Epoch: 10 | Batch Status: 36000/49502 (73%) | Loss: 0.199491\n",
      "Train Epoch: 10 | Batch Status: 37000/49502 (75%) | Loss: 0.644823\n",
      "Train Epoch: 10 | Batch Status: 38000/49502 (77%) | Loss: 0.333920\n",
      "Train Epoch: 10 | Batch Status: 39000/49502 (79%) | Loss: 0.392132\n",
      "Train Epoch: 10 | Batch Status: 40000/49502 (81%) | Loss: 0.365319\n",
      "Train Epoch: 10 | Batch Status: 41000/49502 (83%) | Loss: 1.227705\n",
      "Train Epoch: 10 | Batch Status: 42000/49502 (85%) | Loss: 0.655992\n",
      "Train Epoch: 10 | Batch Status: 43000/49502 (87%) | Loss: 0.158453\n",
      "Train Epoch: 10 | Batch Status: 44000/49502 (89%) | Loss: 0.511802\n",
      "Train Epoch: 10 | Batch Status: 45000/49502 (91%) | Loss: 0.448291\n",
      "Train Epoch: 10 | Batch Status: 46000/49502 (93%) | Loss: 0.333536\n",
      "Train Epoch: 10 | Batch Status: 47000/49502 (95%) | Loss: 0.589598\n",
      "Train Epoch: 10 | Batch Status: 48000/49502 (97%) | Loss: 0.625090\n",
      "Train Epoch: 10 | Batch Status: 49000/49502 (99%) | Loss: 0.241195\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.7960, Accuracy: 6210/12376(50.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 11 | Batch Status: 0/49502 (0%) | Loss: 0.316754\n",
      "Train Epoch: 11 | Batch Status: 1000/49502 (2%) | Loss: 0.624653\n",
      "Train Epoch: 11 | Batch Status: 2000/49502 (4%) | Loss: 0.419699\n",
      "Train Epoch: 11 | Batch Status: 3000/49502 (6%) | Loss: 0.658618\n",
      "Train Epoch: 11 | Batch Status: 4000/49502 (8%) | Loss: 0.243073\n",
      "Train Epoch: 11 | Batch Status: 5000/49502 (10%) | Loss: 0.229012\n",
      "Train Epoch: 11 | Batch Status: 6000/49502 (12%) | Loss: 0.193492\n",
      "Train Epoch: 11 | Batch Status: 7000/49502 (14%) | Loss: 0.526393\n",
      "Train Epoch: 11 | Batch Status: 8000/49502 (16%) | Loss: 0.504178\n",
      "Train Epoch: 11 | Batch Status: 9000/49502 (18%) | Loss: 0.624783\n",
      "Train Epoch: 11 | Batch Status: 10000/49502 (20%) | Loss: 0.240114\n",
      "Train Epoch: 11 | Batch Status: 11000/49502 (22%) | Loss: 0.261765\n",
      "Train Epoch: 11 | Batch Status: 12000/49502 (24%) | Loss: 0.315710\n",
      "Train Epoch: 11 | Batch Status: 13000/49502 (26%) | Loss: 0.714635\n",
      "Train Epoch: 11 | Batch Status: 14000/49502 (28%) | Loss: 0.109086\n",
      "Train Epoch: 11 | Batch Status: 15000/49502 (30%) | Loss: 0.850347\n",
      "Train Epoch: 11 | Batch Status: 16000/49502 (32%) | Loss: 0.936734\n",
      "Train Epoch: 11 | Batch Status: 17000/49502 (34%) | Loss: 0.412098\n",
      "Train Epoch: 11 | Batch Status: 18000/49502 (36%) | Loss: 0.168548\n",
      "Train Epoch: 11 | Batch Status: 19000/49502 (38%) | Loss: 0.675805\n",
      "Train Epoch: 11 | Batch Status: 20000/49502 (40%) | Loss: 0.199601\n",
      "Train Epoch: 11 | Batch Status: 21000/49502 (42%) | Loss: 0.532778\n",
      "Train Epoch: 11 | Batch Status: 22000/49502 (44%) | Loss: 0.557041\n",
      "Train Epoch: 11 | Batch Status: 23000/49502 (46%) | Loss: 0.559110\n",
      "Train Epoch: 11 | Batch Status: 24000/49502 (48%) | Loss: 0.261223\n",
      "Train Epoch: 11 | Batch Status: 25000/49502 (50%) | Loss: 0.638312\n",
      "Train Epoch: 11 | Batch Status: 26000/49502 (53%) | Loss: 0.674570\n",
      "Train Epoch: 11 | Batch Status: 27000/49502 (55%) | Loss: 0.474811\n",
      "Train Epoch: 11 | Batch Status: 28000/49502 (57%) | Loss: 0.404111\n",
      "Train Epoch: 11 | Batch Status: 29000/49502 (59%) | Loss: 0.280651\n",
      "Train Epoch: 11 | Batch Status: 30000/49502 (61%) | Loss: 0.704050\n",
      "Train Epoch: 11 | Batch Status: 31000/49502 (63%) | Loss: 0.559418\n",
      "Train Epoch: 11 | Batch Status: 32000/49502 (65%) | Loss: 0.864334\n",
      "Train Epoch: 11 | Batch Status: 33000/49502 (67%) | Loss: 0.184028\n",
      "Train Epoch: 11 | Batch Status: 34000/49502 (69%) | Loss: 0.351361\n",
      "Train Epoch: 11 | Batch Status: 35000/49502 (71%) | Loss: 0.647212\n",
      "Train Epoch: 11 | Batch Status: 36000/49502 (73%) | Loss: 0.291977\n",
      "Train Epoch: 11 | Batch Status: 37000/49502 (75%) | Loss: 0.829065\n",
      "Train Epoch: 11 | Batch Status: 38000/49502 (77%) | Loss: 0.368546\n",
      "Train Epoch: 11 | Batch Status: 39000/49502 (79%) | Loss: 0.530296\n",
      "Train Epoch: 11 | Batch Status: 40000/49502 (81%) | Loss: 0.976901\n",
      "Train Epoch: 11 | Batch Status: 41000/49502 (83%) | Loss: 0.814400\n",
      "Train Epoch: 11 | Batch Status: 42000/49502 (85%) | Loss: 0.521850\n",
      "Train Epoch: 11 | Batch Status: 43000/49502 (87%) | Loss: 0.412547\n",
      "Train Epoch: 11 | Batch Status: 44000/49502 (89%) | Loss: 0.522494\n",
      "Train Epoch: 11 | Batch Status: 45000/49502 (91%) | Loss: 0.437597\n",
      "Train Epoch: 11 | Batch Status: 46000/49502 (93%) | Loss: 0.147399\n",
      "Train Epoch: 11 | Batch Status: 47000/49502 (95%) | Loss: 0.470106\n",
      "Train Epoch: 11 | Batch Status: 48000/49502 (97%) | Loss: 0.893773\n",
      "Train Epoch: 11 | Batch Status: 49000/49502 (99%) | Loss: 0.799342\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.8130, Accuracy: 5623/12376(45.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 12 | Batch Status: 0/49502 (0%) | Loss: 0.512475\n",
      "Train Epoch: 12 | Batch Status: 1000/49502 (2%) | Loss: 0.237789\n",
      "Train Epoch: 12 | Batch Status: 2000/49502 (4%) | Loss: 0.125329\n",
      "Train Epoch: 12 | Batch Status: 3000/49502 (6%) | Loss: 0.438381\n",
      "Train Epoch: 12 | Batch Status: 4000/49502 (8%) | Loss: 1.326189\n",
      "Train Epoch: 12 | Batch Status: 5000/49502 (10%) | Loss: 0.361519\n",
      "Train Epoch: 12 | Batch Status: 6000/49502 (12%) | Loss: 1.394908\n",
      "Train Epoch: 12 | Batch Status: 7000/49502 (14%) | Loss: 0.886482\n",
      "Train Epoch: 12 | Batch Status: 8000/49502 (16%) | Loss: 0.807649\n",
      "Train Epoch: 12 | Batch Status: 9000/49502 (18%) | Loss: 0.273509\n",
      "Train Epoch: 12 | Batch Status: 10000/49502 (20%) | Loss: 0.246349\n",
      "Train Epoch: 12 | Batch Status: 11000/49502 (22%) | Loss: 0.206330\n",
      "Train Epoch: 12 | Batch Status: 12000/49502 (24%) | Loss: 0.406229\n",
      "Train Epoch: 12 | Batch Status: 13000/49502 (26%) | Loss: 0.999527\n",
      "Train Epoch: 12 | Batch Status: 14000/49502 (28%) | Loss: 0.258068\n",
      "Train Epoch: 12 | Batch Status: 15000/49502 (30%) | Loss: 0.329419\n",
      "Train Epoch: 12 | Batch Status: 16000/49502 (32%) | Loss: 0.382385\n",
      "Train Epoch: 12 | Batch Status: 17000/49502 (34%) | Loss: 0.424177\n",
      "Train Epoch: 12 | Batch Status: 18000/49502 (36%) | Loss: 0.426602\n",
      "Train Epoch: 12 | Batch Status: 19000/49502 (38%) | Loss: 0.447240\n",
      "Train Epoch: 12 | Batch Status: 20000/49502 (40%) | Loss: 0.322898\n",
      "Train Epoch: 12 | Batch Status: 21000/49502 (42%) | Loss: 0.211968\n",
      "Train Epoch: 12 | Batch Status: 22000/49502 (44%) | Loss: 0.319839\n",
      "Train Epoch: 12 | Batch Status: 23000/49502 (46%) | Loss: 0.648070\n",
      "Train Epoch: 12 | Batch Status: 24000/49502 (48%) | Loss: 0.205307\n",
      "Train Epoch: 12 | Batch Status: 25000/49502 (50%) | Loss: 0.306666\n",
      "Train Epoch: 12 | Batch Status: 26000/49502 (53%) | Loss: 0.125367\n",
      "Train Epoch: 12 | Batch Status: 27000/49502 (55%) | Loss: 0.361653\n",
      "Train Epoch: 12 | Batch Status: 28000/49502 (57%) | Loss: 0.273824\n",
      "Train Epoch: 12 | Batch Status: 29000/49502 (59%) | Loss: 0.536270\n",
      "Train Epoch: 12 | Batch Status: 30000/49502 (61%) | Loss: 0.700344\n",
      "Train Epoch: 12 | Batch Status: 31000/49502 (63%) | Loss: 0.331366\n",
      "Train Epoch: 12 | Batch Status: 32000/49502 (65%) | Loss: 0.286038\n",
      "Train Epoch: 12 | Batch Status: 33000/49502 (67%) | Loss: 1.007854\n",
      "Train Epoch: 12 | Batch Status: 34000/49502 (69%) | Loss: 0.521351\n",
      "Train Epoch: 12 | Batch Status: 35000/49502 (71%) | Loss: 0.204499\n",
      "Train Epoch: 12 | Batch Status: 36000/49502 (73%) | Loss: 0.876825\n",
      "Train Epoch: 12 | Batch Status: 37000/49502 (75%) | Loss: 0.157000\n",
      "Train Epoch: 12 | Batch Status: 38000/49502 (77%) | Loss: 0.468329\n",
      "Train Epoch: 12 | Batch Status: 39000/49502 (79%) | Loss: 0.574606\n",
      "Train Epoch: 12 | Batch Status: 40000/49502 (81%) | Loss: 0.524794\n",
      "Train Epoch: 12 | Batch Status: 41000/49502 (83%) | Loss: 0.530304\n",
      "Train Epoch: 12 | Batch Status: 42000/49502 (85%) | Loss: 0.322153\n",
      "Train Epoch: 12 | Batch Status: 43000/49502 (87%) | Loss: 0.159722\n",
      "Train Epoch: 12 | Batch Status: 44000/49502 (89%) | Loss: 0.416932\n",
      "Train Epoch: 12 | Batch Status: 45000/49502 (91%) | Loss: 0.152598\n",
      "Train Epoch: 12 | Batch Status: 46000/49502 (93%) | Loss: 0.840677\n",
      "Train Epoch: 12 | Batch Status: 47000/49502 (95%) | Loss: 0.309681\n",
      "Train Epoch: 12 | Batch Status: 48000/49502 (97%) | Loss: 0.692629\n",
      "Train Epoch: 12 | Batch Status: 49000/49502 (99%) | Loss: 0.303959\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.8173, Accuracy: 5553/12376(44.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 13 | Batch Status: 0/49502 (0%) | Loss: 0.239327\n",
      "Train Epoch: 13 | Batch Status: 1000/49502 (2%) | Loss: 0.759918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 | Batch Status: 2000/49502 (4%) | Loss: 0.608163\n",
      "Train Epoch: 13 | Batch Status: 3000/49502 (6%) | Loss: 0.763257\n",
      "Train Epoch: 13 | Batch Status: 4000/49502 (8%) | Loss: 0.438857\n",
      "Train Epoch: 13 | Batch Status: 5000/49502 (10%) | Loss: 0.563320\n",
      "Train Epoch: 13 | Batch Status: 6000/49502 (12%) | Loss: 0.372017\n",
      "Train Epoch: 13 | Batch Status: 7000/49502 (14%) | Loss: 0.382558\n",
      "Train Epoch: 13 | Batch Status: 8000/49502 (16%) | Loss: 0.951193\n",
      "Train Epoch: 13 | Batch Status: 9000/49502 (18%) | Loss: 0.401347\n",
      "Train Epoch: 13 | Batch Status: 10000/49502 (20%) | Loss: 0.456005\n",
      "Train Epoch: 13 | Batch Status: 11000/49502 (22%) | Loss: 1.052363\n",
      "Train Epoch: 13 | Batch Status: 12000/49502 (24%) | Loss: 0.156871\n",
      "Train Epoch: 13 | Batch Status: 13000/49502 (26%) | Loss: 0.436724\n",
      "Train Epoch: 13 | Batch Status: 14000/49502 (28%) | Loss: 0.502051\n",
      "Train Epoch: 13 | Batch Status: 15000/49502 (30%) | Loss: 0.285003\n",
      "Train Epoch: 13 | Batch Status: 16000/49502 (32%) | Loss: 0.232125\n",
      "Train Epoch: 13 | Batch Status: 17000/49502 (34%) | Loss: 0.316105\n",
      "Train Epoch: 13 | Batch Status: 18000/49502 (36%) | Loss: 0.859824\n",
      "Train Epoch: 13 | Batch Status: 19000/49502 (38%) | Loss: 0.642080\n",
      "Train Epoch: 13 | Batch Status: 20000/49502 (40%) | Loss: 0.209273\n",
      "Train Epoch: 13 | Batch Status: 21000/49502 (42%) | Loss: 0.583055\n",
      "Train Epoch: 13 | Batch Status: 22000/49502 (44%) | Loss: 0.230571\n",
      "Train Epoch: 13 | Batch Status: 23000/49502 (46%) | Loss: 0.568321\n",
      "Train Epoch: 13 | Batch Status: 24000/49502 (48%) | Loss: 0.684077\n",
      "Train Epoch: 13 | Batch Status: 25000/49502 (50%) | Loss: 0.055814\n",
      "Train Epoch: 13 | Batch Status: 26000/49502 (53%) | Loss: 0.705837\n",
      "Train Epoch: 13 | Batch Status: 27000/49502 (55%) | Loss: 1.800407\n",
      "Train Epoch: 13 | Batch Status: 28000/49502 (57%) | Loss: 0.784745\n",
      "Train Epoch: 13 | Batch Status: 29000/49502 (59%) | Loss: 0.133408\n",
      "Train Epoch: 13 | Batch Status: 30000/49502 (61%) | Loss: 0.220460\n",
      "Train Epoch: 13 | Batch Status: 31000/49502 (63%) | Loss: 0.358340\n",
      "Train Epoch: 13 | Batch Status: 32000/49502 (65%) | Loss: 0.169843\n",
      "Train Epoch: 13 | Batch Status: 33000/49502 (67%) | Loss: 0.832970\n",
      "Train Epoch: 13 | Batch Status: 34000/49502 (69%) | Loss: 0.539182\n",
      "Train Epoch: 13 | Batch Status: 35000/49502 (71%) | Loss: 0.445366\n",
      "Train Epoch: 13 | Batch Status: 36000/49502 (73%) | Loss: 0.313645\n",
      "Train Epoch: 13 | Batch Status: 37000/49502 (75%) | Loss: 0.279515\n",
      "Train Epoch: 13 | Batch Status: 38000/49502 (77%) | Loss: 0.736999\n",
      "Train Epoch: 13 | Batch Status: 39000/49502 (79%) | Loss: 0.274867\n",
      "Train Epoch: 13 | Batch Status: 40000/49502 (81%) | Loss: 0.175825\n",
      "Train Epoch: 13 | Batch Status: 41000/49502 (83%) | Loss: 0.557747\n",
      "Train Epoch: 13 | Batch Status: 42000/49502 (85%) | Loss: 0.414764\n",
      "Train Epoch: 13 | Batch Status: 43000/49502 (87%) | Loss: 0.697405\n",
      "Train Epoch: 13 | Batch Status: 44000/49502 (89%) | Loss: 0.504936\n",
      "Train Epoch: 13 | Batch Status: 45000/49502 (91%) | Loss: 0.196166\n",
      "Train Epoch: 13 | Batch Status: 46000/49502 (93%) | Loss: 0.673582\n",
      "Train Epoch: 13 | Batch Status: 47000/49502 (95%) | Loss: 0.816880\n",
      "Train Epoch: 13 | Batch Status: 48000/49502 (97%) | Loss: 0.416606\n",
      "Train Epoch: 13 | Batch Status: 49000/49502 (99%) | Loss: 0.435407\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.8583, Accuracy: 4610/12376(37.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 14 | Batch Status: 0/49502 (0%) | Loss: 0.282934\n",
      "Train Epoch: 14 | Batch Status: 1000/49502 (2%) | Loss: 0.428000\n",
      "Train Epoch: 14 | Batch Status: 2000/49502 (4%) | Loss: 0.687835\n",
      "Train Epoch: 14 | Batch Status: 3000/49502 (6%) | Loss: 0.558900\n",
      "Train Epoch: 14 | Batch Status: 4000/49502 (8%) | Loss: 0.305457\n",
      "Train Epoch: 14 | Batch Status: 5000/49502 (10%) | Loss: 1.067113\n",
      "Train Epoch: 14 | Batch Status: 6000/49502 (12%) | Loss: 0.540295\n",
      "Train Epoch: 14 | Batch Status: 7000/49502 (14%) | Loss: 0.636935\n",
      "Train Epoch: 14 | Batch Status: 8000/49502 (16%) | Loss: 0.392545\n",
      "Train Epoch: 14 | Batch Status: 9000/49502 (18%) | Loss: 0.539682\n",
      "Train Epoch: 14 | Batch Status: 10000/49502 (20%) | Loss: 0.368449\n",
      "Train Epoch: 14 | Batch Status: 11000/49502 (22%) | Loss: 0.870478\n",
      "Train Epoch: 14 | Batch Status: 12000/49502 (24%) | Loss: 0.825207\n",
      "Train Epoch: 14 | Batch Status: 13000/49502 (26%) | Loss: 0.243210\n",
      "Train Epoch: 14 | Batch Status: 14000/49502 (28%) | Loss: 0.280390\n",
      "Train Epoch: 14 | Batch Status: 15000/49502 (30%) | Loss: 0.287241\n",
      "Train Epoch: 14 | Batch Status: 16000/49502 (32%) | Loss: 0.714430\n",
      "Train Epoch: 14 | Batch Status: 17000/49502 (34%) | Loss: 0.446111\n",
      "Train Epoch: 14 | Batch Status: 18000/49502 (36%) | Loss: 0.449268\n",
      "Train Epoch: 14 | Batch Status: 19000/49502 (38%) | Loss: 1.070110\n",
      "Train Epoch: 14 | Batch Status: 20000/49502 (40%) | Loss: 0.451248\n",
      "Train Epoch: 14 | Batch Status: 21000/49502 (42%) | Loss: 0.742333\n",
      "Train Epoch: 14 | Batch Status: 22000/49502 (44%) | Loss: 0.728632\n",
      "Train Epoch: 14 | Batch Status: 23000/49502 (46%) | Loss: 0.225435\n",
      "Train Epoch: 14 | Batch Status: 24000/49502 (48%) | Loss: 0.462873\n",
      "Train Epoch: 14 | Batch Status: 25000/49502 (50%) | Loss: 1.178668\n",
      "Train Epoch: 14 | Batch Status: 26000/49502 (53%) | Loss: 0.340982\n",
      "Train Epoch: 14 | Batch Status: 27000/49502 (55%) | Loss: 0.497047\n",
      "Train Epoch: 14 | Batch Status: 28000/49502 (57%) | Loss: 0.408016\n",
      "Train Epoch: 14 | Batch Status: 29000/49502 (59%) | Loss: 0.308856\n",
      "Train Epoch: 14 | Batch Status: 30000/49502 (61%) | Loss: 0.545643\n",
      "Train Epoch: 14 | Batch Status: 31000/49502 (63%) | Loss: 0.369220\n",
      "Train Epoch: 14 | Batch Status: 32000/49502 (65%) | Loss: 0.361867\n",
      "Train Epoch: 14 | Batch Status: 33000/49502 (67%) | Loss: 0.369723\n",
      "Train Epoch: 14 | Batch Status: 34000/49502 (69%) | Loss: 0.518879\n",
      "Train Epoch: 14 | Batch Status: 35000/49502 (71%) | Loss: 0.556416\n",
      "Train Epoch: 14 | Batch Status: 36000/49502 (73%) | Loss: 0.664142\n",
      "Train Epoch: 14 | Batch Status: 37000/49502 (75%) | Loss: 0.691149\n",
      "Train Epoch: 14 | Batch Status: 38000/49502 (77%) | Loss: 0.732512\n",
      "Train Epoch: 14 | Batch Status: 39000/49502 (79%) | Loss: 0.436035\n",
      "Train Epoch: 14 | Batch Status: 40000/49502 (81%) | Loss: 0.508568\n",
      "Train Epoch: 14 | Batch Status: 41000/49502 (83%) | Loss: 0.610886\n",
      "Train Epoch: 14 | Batch Status: 42000/49502 (85%) | Loss: 0.422641\n",
      "Train Epoch: 14 | Batch Status: 43000/49502 (87%) | Loss: 0.359710\n",
      "Train Epoch: 14 | Batch Status: 44000/49502 (89%) | Loss: 0.724451\n",
      "Train Epoch: 14 | Batch Status: 45000/49502 (91%) | Loss: 0.387193\n",
      "Train Epoch: 14 | Batch Status: 46000/49502 (93%) | Loss: 1.306996\n",
      "Train Epoch: 14 | Batch Status: 47000/49502 (95%) | Loss: 0.275593\n",
      "Train Epoch: 14 | Batch Status: 48000/49502 (97%) | Loss: 0.282453\n",
      "Train Epoch: 14 | Batch Status: 49000/49502 (99%) | Loss: 0.455794\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.9207, Accuracy: 5659/12376(45.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 15 | Batch Status: 0/49502 (0%) | Loss: 0.056820\n",
      "Train Epoch: 15 | Batch Status: 1000/49502 (2%) | Loss: 0.287334\n",
      "Train Epoch: 15 | Batch Status: 2000/49502 (4%) | Loss: 0.916387\n",
      "Train Epoch: 15 | Batch Status: 3000/49502 (6%) | Loss: 0.219536\n",
      "Train Epoch: 15 | Batch Status: 4000/49502 (8%) | Loss: 0.530022\n",
      "Train Epoch: 15 | Batch Status: 5000/49502 (10%) | Loss: 0.155498\n",
      "Train Epoch: 15 | Batch Status: 6000/49502 (12%) | Loss: 0.600593\n",
      "Train Epoch: 15 | Batch Status: 7000/49502 (14%) | Loss: 0.976414\n",
      "Train Epoch: 15 | Batch Status: 8000/49502 (16%) | Loss: 0.269059\n",
      "Train Epoch: 15 | Batch Status: 9000/49502 (18%) | Loss: 0.133483\n",
      "Train Epoch: 15 | Batch Status: 10000/49502 (20%) | Loss: 0.878131\n",
      "Train Epoch: 15 | Batch Status: 11000/49502 (22%) | Loss: 1.320477\n",
      "Train Epoch: 15 | Batch Status: 12000/49502 (24%) | Loss: 0.006706\n",
      "Train Epoch: 15 | Batch Status: 13000/49502 (26%) | Loss: 0.194466\n",
      "Train Epoch: 15 | Batch Status: 14000/49502 (28%) | Loss: 0.849726\n",
      "Train Epoch: 15 | Batch Status: 15000/49502 (30%) | Loss: 0.401541\n",
      "Train Epoch: 15 | Batch Status: 16000/49502 (32%) | Loss: 0.307517\n",
      "Train Epoch: 15 | Batch Status: 17000/49502 (34%) | Loss: 0.944353\n",
      "Train Epoch: 15 | Batch Status: 18000/49502 (36%) | Loss: 0.334134\n",
      "Train Epoch: 15 | Batch Status: 19000/49502 (38%) | Loss: 0.293298\n",
      "Train Epoch: 15 | Batch Status: 20000/49502 (40%) | Loss: 0.743829\n",
      "Train Epoch: 15 | Batch Status: 21000/49502 (42%) | Loss: 0.593743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 | Batch Status: 22000/49502 (44%) | Loss: 0.134336\n",
      "Train Epoch: 15 | Batch Status: 23000/49502 (46%) | Loss: 0.334730\n",
      "Train Epoch: 15 | Batch Status: 24000/49502 (48%) | Loss: 0.391510\n",
      "Train Epoch: 15 | Batch Status: 25000/49502 (50%) | Loss: 0.088560\n",
      "Train Epoch: 15 | Batch Status: 26000/49502 (53%) | Loss: 1.121294\n",
      "Train Epoch: 15 | Batch Status: 27000/49502 (55%) | Loss: 0.957775\n",
      "Train Epoch: 15 | Batch Status: 28000/49502 (57%) | Loss: 0.219140\n",
      "Train Epoch: 15 | Batch Status: 29000/49502 (59%) | Loss: 0.713248\n",
      "Train Epoch: 15 | Batch Status: 30000/49502 (61%) | Loss: 0.431583\n",
      "Train Epoch: 15 | Batch Status: 31000/49502 (63%) | Loss: 0.456727\n",
      "Train Epoch: 15 | Batch Status: 32000/49502 (65%) | Loss: 0.323418\n",
      "Train Epoch: 15 | Batch Status: 33000/49502 (67%) | Loss: 1.085128\n",
      "Train Epoch: 15 | Batch Status: 34000/49502 (69%) | Loss: 0.280434\n",
      "Train Epoch: 15 | Batch Status: 35000/49502 (71%) | Loss: 0.534147\n",
      "Train Epoch: 15 | Batch Status: 36000/49502 (73%) | Loss: 0.256064\n",
      "Train Epoch: 15 | Batch Status: 37000/49502 (75%) | Loss: 0.192400\n",
      "Train Epoch: 15 | Batch Status: 38000/49502 (77%) | Loss: 0.336792\n",
      "Train Epoch: 15 | Batch Status: 39000/49502 (79%) | Loss: 0.335470\n",
      "Train Epoch: 15 | Batch Status: 40000/49502 (81%) | Loss: 0.365546\n",
      "Train Epoch: 15 | Batch Status: 41000/49502 (83%) | Loss: 0.432458\n",
      "Train Epoch: 15 | Batch Status: 42000/49502 (85%) | Loss: 0.821533\n",
      "Train Epoch: 15 | Batch Status: 43000/49502 (87%) | Loss: 0.475403\n",
      "Train Epoch: 15 | Batch Status: 44000/49502 (89%) | Loss: 0.080435\n",
      "Train Epoch: 15 | Batch Status: 45000/49502 (91%) | Loss: 0.302493\n",
      "Train Epoch: 15 | Batch Status: 46000/49502 (93%) | Loss: 0.210077\n",
      "Train Epoch: 15 | Batch Status: 47000/49502 (95%) | Loss: 0.704977\n",
      "Train Epoch: 15 | Batch Status: 48000/49502 (97%) | Loss: 0.604299\n",
      "Train Epoch: 15 | Batch Status: 49000/49502 (99%) | Loss: 0.733608\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.8664, Accuracy: 5757/12376(46.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 16 | Batch Status: 0/49502 (0%) | Loss: 0.393524\n",
      "Train Epoch: 16 | Batch Status: 1000/49502 (2%) | Loss: 0.344172\n",
      "Train Epoch: 16 | Batch Status: 2000/49502 (4%) | Loss: 0.133507\n",
      "Train Epoch: 16 | Batch Status: 3000/49502 (6%) | Loss: 0.192448\n",
      "Train Epoch: 16 | Batch Status: 4000/49502 (8%) | Loss: 0.385436\n",
      "Train Epoch: 16 | Batch Status: 5000/49502 (10%) | Loss: 0.532290\n",
      "Train Epoch: 16 | Batch Status: 6000/49502 (12%) | Loss: 0.391665\n",
      "Train Epoch: 16 | Batch Status: 7000/49502 (14%) | Loss: 0.142439\n",
      "Train Epoch: 16 | Batch Status: 8000/49502 (16%) | Loss: 0.369609\n",
      "Train Epoch: 16 | Batch Status: 9000/49502 (18%) | Loss: 0.203145\n",
      "Train Epoch: 16 | Batch Status: 10000/49502 (20%) | Loss: 0.539535\n",
      "Train Epoch: 16 | Batch Status: 11000/49502 (22%) | Loss: 0.312108\n",
      "Train Epoch: 16 | Batch Status: 12000/49502 (24%) | Loss: 0.293261\n",
      "Train Epoch: 16 | Batch Status: 13000/49502 (26%) | Loss: 0.371468\n",
      "Train Epoch: 16 | Batch Status: 14000/49502 (28%) | Loss: 0.444116\n",
      "Train Epoch: 16 | Batch Status: 15000/49502 (30%) | Loss: 0.789427\n",
      "Train Epoch: 16 | Batch Status: 16000/49502 (32%) | Loss: 0.572599\n",
      "Train Epoch: 16 | Batch Status: 17000/49502 (34%) | Loss: 0.289160\n",
      "Train Epoch: 16 | Batch Status: 18000/49502 (36%) | Loss: 0.634526\n",
      "Train Epoch: 16 | Batch Status: 19000/49502 (38%) | Loss: 0.197819\n",
      "Train Epoch: 16 | Batch Status: 20000/49502 (40%) | Loss: 0.710227\n",
      "Train Epoch: 16 | Batch Status: 21000/49502 (42%) | Loss: 0.247285\n",
      "Train Epoch: 16 | Batch Status: 22000/49502 (44%) | Loss: 0.726614\n",
      "Train Epoch: 16 | Batch Status: 23000/49502 (46%) | Loss: 0.288561\n",
      "Train Epoch: 16 | Batch Status: 24000/49502 (48%) | Loss: 0.571851\n",
      "Train Epoch: 16 | Batch Status: 25000/49502 (50%) | Loss: 0.867418\n",
      "Train Epoch: 16 | Batch Status: 26000/49502 (53%) | Loss: 0.315591\n",
      "Train Epoch: 16 | Batch Status: 27000/49502 (55%) | Loss: 0.488471\n",
      "Train Epoch: 16 | Batch Status: 28000/49502 (57%) | Loss: 0.349160\n",
      "Train Epoch: 16 | Batch Status: 29000/49502 (59%) | Loss: 0.568727\n",
      "Train Epoch: 16 | Batch Status: 30000/49502 (61%) | Loss: 0.125026\n",
      "Train Epoch: 16 | Batch Status: 31000/49502 (63%) | Loss: 0.254993\n",
      "Train Epoch: 16 | Batch Status: 32000/49502 (65%) | Loss: 1.132070\n",
      "Train Epoch: 16 | Batch Status: 33000/49502 (67%) | Loss: 0.872597\n",
      "Train Epoch: 16 | Batch Status: 34000/49502 (69%) | Loss: 0.929849\n",
      "Train Epoch: 16 | Batch Status: 35000/49502 (71%) | Loss: 0.273863\n",
      "Train Epoch: 16 | Batch Status: 36000/49502 (73%) | Loss: 0.954032\n",
      "Train Epoch: 16 | Batch Status: 37000/49502 (75%) | Loss: 0.759747\n",
      "Train Epoch: 16 | Batch Status: 38000/49502 (77%) | Loss: 0.439753\n",
      "Train Epoch: 16 | Batch Status: 39000/49502 (79%) | Loss: 0.308857\n",
      "Train Epoch: 16 | Batch Status: 40000/49502 (81%) | Loss: 1.089223\n",
      "Train Epoch: 16 | Batch Status: 41000/49502 (83%) | Loss: 0.356679\n",
      "Train Epoch: 16 | Batch Status: 42000/49502 (85%) | Loss: 0.501531\n",
      "Train Epoch: 16 | Batch Status: 43000/49502 (87%) | Loss: 0.382619\n",
      "Train Epoch: 16 | Batch Status: 44000/49502 (89%) | Loss: 0.441129\n",
      "Train Epoch: 16 | Batch Status: 45000/49502 (91%) | Loss: 0.143939\n",
      "Train Epoch: 16 | Batch Status: 46000/49502 (93%) | Loss: 0.756067\n",
      "Train Epoch: 16 | Batch Status: 47000/49502 (95%) | Loss: 0.474436\n",
      "Train Epoch: 16 | Batch Status: 48000/49502 (97%) | Loss: 0.524200\n",
      "Train Epoch: 16 | Batch Status: 49000/49502 (99%) | Loss: 0.272925\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.8562, Accuracy: 5833/12376(47.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 17 | Batch Status: 0/49502 (0%) | Loss: 0.412643\n",
      "Train Epoch: 17 | Batch Status: 1000/49502 (2%) | Loss: 0.692154\n",
      "Train Epoch: 17 | Batch Status: 2000/49502 (4%) | Loss: 0.230855\n",
      "Train Epoch: 17 | Batch Status: 3000/49502 (6%) | Loss: 0.416414\n",
      "Train Epoch: 17 | Batch Status: 4000/49502 (8%) | Loss: 0.886939\n",
      "Train Epoch: 17 | Batch Status: 5000/49502 (10%) | Loss: 0.404685\n",
      "Train Epoch: 17 | Batch Status: 6000/49502 (12%) | Loss: 0.685534\n",
      "Train Epoch: 17 | Batch Status: 7000/49502 (14%) | Loss: 0.613848\n",
      "Train Epoch: 17 | Batch Status: 8000/49502 (16%) | Loss: 1.076815\n",
      "Train Epoch: 17 | Batch Status: 9000/49502 (18%) | Loss: 0.214121\n",
      "Train Epoch: 17 | Batch Status: 10000/49502 (20%) | Loss: 0.449686\n",
      "Train Epoch: 17 | Batch Status: 11000/49502 (22%) | Loss: 0.620120\n",
      "Train Epoch: 17 | Batch Status: 12000/49502 (24%) | Loss: 0.347920\n",
      "Train Epoch: 17 | Batch Status: 13000/49502 (26%) | Loss: 0.685177\n",
      "Train Epoch: 17 | Batch Status: 14000/49502 (28%) | Loss: 0.302705\n",
      "Train Epoch: 17 | Batch Status: 15000/49502 (30%) | Loss: 0.248349\n",
      "Train Epoch: 17 | Batch Status: 16000/49502 (32%) | Loss: 0.402227\n",
      "Train Epoch: 17 | Batch Status: 17000/49502 (34%) | Loss: 0.366933\n",
      "Train Epoch: 17 | Batch Status: 18000/49502 (36%) | Loss: 0.460996\n",
      "Train Epoch: 17 | Batch Status: 19000/49502 (38%) | Loss: 0.294513\n",
      "Train Epoch: 17 | Batch Status: 20000/49502 (40%) | Loss: 0.686382\n",
      "Train Epoch: 17 | Batch Status: 21000/49502 (42%) | Loss: 0.488844\n",
      "Train Epoch: 17 | Batch Status: 22000/49502 (44%) | Loss: 0.343102\n",
      "Train Epoch: 17 | Batch Status: 23000/49502 (46%) | Loss: 0.522143\n",
      "Train Epoch: 17 | Batch Status: 24000/49502 (48%) | Loss: 0.898337\n",
      "Train Epoch: 17 | Batch Status: 25000/49502 (50%) | Loss: 0.486871\n",
      "Train Epoch: 17 | Batch Status: 26000/49502 (53%) | Loss: 0.374742\n",
      "Train Epoch: 17 | Batch Status: 27000/49502 (55%) | Loss: 0.126214\n",
      "Train Epoch: 17 | Batch Status: 28000/49502 (57%) | Loss: 0.274457\n",
      "Train Epoch: 17 | Batch Status: 29000/49502 (59%) | Loss: 0.522493\n",
      "Train Epoch: 17 | Batch Status: 30000/49502 (61%) | Loss: 0.516086\n",
      "Train Epoch: 17 | Batch Status: 31000/49502 (63%) | Loss: 0.532256\n",
      "Train Epoch: 17 | Batch Status: 32000/49502 (65%) | Loss: 0.322256\n",
      "Train Epoch: 17 | Batch Status: 33000/49502 (67%) | Loss: 0.515834\n",
      "Train Epoch: 17 | Batch Status: 34000/49502 (69%) | Loss: 0.245895\n",
      "Train Epoch: 17 | Batch Status: 35000/49502 (71%) | Loss: 0.262824\n",
      "Train Epoch: 17 | Batch Status: 36000/49502 (73%) | Loss: 0.534243\n",
      "Train Epoch: 17 | Batch Status: 37000/49502 (75%) | Loss: 0.186247\n",
      "Train Epoch: 17 | Batch Status: 38000/49502 (77%) | Loss: 0.360982\n",
      "Train Epoch: 17 | Batch Status: 39000/49502 (79%) | Loss: 0.204455\n",
      "Train Epoch: 17 | Batch Status: 40000/49502 (81%) | Loss: 0.329714\n",
      "Train Epoch: 17 | Batch Status: 41000/49502 (83%) | Loss: 0.291757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 | Batch Status: 42000/49502 (85%) | Loss: 0.265911\n",
      "Train Epoch: 17 | Batch Status: 43000/49502 (87%) | Loss: 0.577246\n",
      "Train Epoch: 17 | Batch Status: 44000/49502 (89%) | Loss: 0.318717\n",
      "Train Epoch: 17 | Batch Status: 45000/49502 (91%) | Loss: 0.334856\n",
      "Train Epoch: 17 | Batch Status: 46000/49502 (93%) | Loss: 0.303621\n",
      "Train Epoch: 17 | Batch Status: 47000/49502 (95%) | Loss: 0.115979\n",
      "Train Epoch: 17 | Batch Status: 48000/49502 (97%) | Loss: 0.585151\n",
      "Train Epoch: 17 | Batch Status: 49000/49502 (99%) | Loss: 0.562908\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.8796, Accuracy: 5648/12376(45.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 18 | Batch Status: 0/49502 (0%) | Loss: 0.625019\n",
      "Train Epoch: 18 | Batch Status: 1000/49502 (2%) | Loss: 0.807361\n",
      "Train Epoch: 18 | Batch Status: 2000/49502 (4%) | Loss: 0.685563\n",
      "Train Epoch: 18 | Batch Status: 3000/49502 (6%) | Loss: 0.190109\n",
      "Train Epoch: 18 | Batch Status: 4000/49502 (8%) | Loss: 0.335657\n",
      "Train Epoch: 18 | Batch Status: 5000/49502 (10%) | Loss: 0.435960\n",
      "Train Epoch: 18 | Batch Status: 6000/49502 (12%) | Loss: 1.372819\n",
      "Train Epoch: 18 | Batch Status: 7000/49502 (14%) | Loss: 0.343005\n",
      "Train Epoch: 18 | Batch Status: 8000/49502 (16%) | Loss: 0.426340\n",
      "Train Epoch: 18 | Batch Status: 9000/49502 (18%) | Loss: 0.290393\n",
      "Train Epoch: 18 | Batch Status: 10000/49502 (20%) | Loss: 0.391618\n",
      "Train Epoch: 18 | Batch Status: 11000/49502 (22%) | Loss: 0.225406\n",
      "Train Epoch: 18 | Batch Status: 12000/49502 (24%) | Loss: 0.315762\n",
      "Train Epoch: 18 | Batch Status: 13000/49502 (26%) | Loss: 0.356232\n",
      "Train Epoch: 18 | Batch Status: 14000/49502 (28%) | Loss: 1.163664\n",
      "Train Epoch: 18 | Batch Status: 15000/49502 (30%) | Loss: 0.509887\n",
      "Train Epoch: 18 | Batch Status: 16000/49502 (32%) | Loss: 0.460062\n",
      "Train Epoch: 18 | Batch Status: 17000/49502 (34%) | Loss: 0.713579\n",
      "Train Epoch: 18 | Batch Status: 18000/49502 (36%) | Loss: 0.497052\n",
      "Train Epoch: 18 | Batch Status: 19000/49502 (38%) | Loss: 0.333163\n",
      "Train Epoch: 18 | Batch Status: 20000/49502 (40%) | Loss: 0.374328\n",
      "Train Epoch: 18 | Batch Status: 21000/49502 (42%) | Loss: 0.083665\n",
      "Train Epoch: 18 | Batch Status: 22000/49502 (44%) | Loss: 0.185704\n",
      "Train Epoch: 18 | Batch Status: 23000/49502 (46%) | Loss: 0.594074\n",
      "Train Epoch: 18 | Batch Status: 24000/49502 (48%) | Loss: 0.311636\n",
      "Train Epoch: 18 | Batch Status: 25000/49502 (50%) | Loss: 0.502197\n",
      "Train Epoch: 18 | Batch Status: 26000/49502 (53%) | Loss: 0.345387\n",
      "Train Epoch: 18 | Batch Status: 27000/49502 (55%) | Loss: 0.259004\n",
      "Train Epoch: 18 | Batch Status: 28000/49502 (57%) | Loss: 0.785434\n",
      "Train Epoch: 18 | Batch Status: 29000/49502 (59%) | Loss: 0.575514\n",
      "Train Epoch: 18 | Batch Status: 30000/49502 (61%) | Loss: 0.143088\n",
      "Train Epoch: 18 | Batch Status: 31000/49502 (63%) | Loss: 0.528943\n",
      "Train Epoch: 18 | Batch Status: 32000/49502 (65%) | Loss: 0.433562\n",
      "Train Epoch: 18 | Batch Status: 33000/49502 (67%) | Loss: 0.890043\n",
      "Train Epoch: 18 | Batch Status: 34000/49502 (69%) | Loss: 0.356733\n",
      "Train Epoch: 18 | Batch Status: 35000/49502 (71%) | Loss: 0.694414\n",
      "Train Epoch: 18 | Batch Status: 36000/49502 (73%) | Loss: 0.762473\n",
      "Train Epoch: 18 | Batch Status: 37000/49502 (75%) | Loss: 0.366510\n",
      "Train Epoch: 18 | Batch Status: 38000/49502 (77%) | Loss: 0.231653\n",
      "Train Epoch: 18 | Batch Status: 39000/49502 (79%) | Loss: 0.304285\n",
      "Train Epoch: 18 | Batch Status: 40000/49502 (81%) | Loss: 0.182525\n",
      "Train Epoch: 18 | Batch Status: 41000/49502 (83%) | Loss: 0.350510\n",
      "Train Epoch: 18 | Batch Status: 42000/49502 (85%) | Loss: 0.637774\n",
      "Train Epoch: 18 | Batch Status: 43000/49502 (87%) | Loss: 0.460234\n",
      "Train Epoch: 18 | Batch Status: 44000/49502 (89%) | Loss: 0.438186\n",
      "Train Epoch: 18 | Batch Status: 45000/49502 (91%) | Loss: 0.373004\n",
      "Train Epoch: 18 | Batch Status: 46000/49502 (93%) | Loss: 0.425056\n",
      "Train Epoch: 18 | Batch Status: 47000/49502 (95%) | Loss: 0.565723\n",
      "Train Epoch: 18 | Batch Status: 48000/49502 (97%) | Loss: 0.271272\n",
      "Train Epoch: 18 | Batch Status: 49000/49502 (99%) | Loss: 0.405220\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.9742, Accuracy: 5549/12376(44.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 19 | Batch Status: 0/49502 (0%) | Loss: 0.064692\n",
      "Train Epoch: 19 | Batch Status: 1000/49502 (2%) | Loss: 0.483682\n",
      "Train Epoch: 19 | Batch Status: 2000/49502 (4%) | Loss: 0.097190\n",
      "Train Epoch: 19 | Batch Status: 3000/49502 (6%) | Loss: 0.600418\n",
      "Train Epoch: 19 | Batch Status: 4000/49502 (8%) | Loss: 0.354429\n",
      "Train Epoch: 19 | Batch Status: 5000/49502 (10%) | Loss: 0.373678\n",
      "Train Epoch: 19 | Batch Status: 6000/49502 (12%) | Loss: 0.585756\n",
      "Train Epoch: 19 | Batch Status: 7000/49502 (14%) | Loss: 0.242597\n",
      "Train Epoch: 19 | Batch Status: 8000/49502 (16%) | Loss: 0.619406\n",
      "Train Epoch: 19 | Batch Status: 9000/49502 (18%) | Loss: 0.665087\n",
      "Train Epoch: 19 | Batch Status: 10000/49502 (20%) | Loss: 0.083522\n",
      "Train Epoch: 19 | Batch Status: 11000/49502 (22%) | Loss: 0.271665\n",
      "Train Epoch: 19 | Batch Status: 12000/49502 (24%) | Loss: 0.280966\n",
      "Train Epoch: 19 | Batch Status: 13000/49502 (26%) | Loss: 1.291535\n",
      "Train Epoch: 19 | Batch Status: 14000/49502 (28%) | Loss: 0.441624\n",
      "Train Epoch: 19 | Batch Status: 15000/49502 (30%) | Loss: 0.416327\n",
      "Train Epoch: 19 | Batch Status: 16000/49502 (32%) | Loss: 0.201877\n",
      "Train Epoch: 19 | Batch Status: 17000/49502 (34%) | Loss: 0.341974\n",
      "Train Epoch: 19 | Batch Status: 18000/49502 (36%) | Loss: 0.770254\n",
      "Train Epoch: 19 | Batch Status: 19000/49502 (38%) | Loss: 0.144615\n",
      "Train Epoch: 19 | Batch Status: 20000/49502 (40%) | Loss: 0.590799\n",
      "Train Epoch: 19 | Batch Status: 21000/49502 (42%) | Loss: 0.689257\n",
      "Train Epoch: 19 | Batch Status: 22000/49502 (44%) | Loss: 0.350604\n",
      "Train Epoch: 19 | Batch Status: 23000/49502 (46%) | Loss: 0.820386\n",
      "Train Epoch: 19 | Batch Status: 24000/49502 (48%) | Loss: 0.146954\n",
      "Train Epoch: 19 | Batch Status: 25000/49502 (50%) | Loss: 0.517021\n",
      "Train Epoch: 19 | Batch Status: 26000/49502 (53%) | Loss: 0.411460\n",
      "Train Epoch: 19 | Batch Status: 27000/49502 (55%) | Loss: 0.708809\n",
      "Train Epoch: 19 | Batch Status: 28000/49502 (57%) | Loss: 0.214181\n",
      "Train Epoch: 19 | Batch Status: 29000/49502 (59%) | Loss: 0.482243\n",
      "Train Epoch: 19 | Batch Status: 30000/49502 (61%) | Loss: 0.531139\n",
      "Train Epoch: 19 | Batch Status: 31000/49502 (63%) | Loss: 0.067768\n",
      "Train Epoch: 19 | Batch Status: 32000/49502 (65%) | Loss: 0.964055\n",
      "Train Epoch: 19 | Batch Status: 33000/49502 (67%) | Loss: 0.187671\n",
      "Train Epoch: 19 | Batch Status: 34000/49502 (69%) | Loss: 0.547798\n",
      "Train Epoch: 19 | Batch Status: 35000/49502 (71%) | Loss: 0.142719\n",
      "Train Epoch: 19 | Batch Status: 36000/49502 (73%) | Loss: 0.211212\n",
      "Train Epoch: 19 | Batch Status: 37000/49502 (75%) | Loss: 0.432012\n",
      "Train Epoch: 19 | Batch Status: 38000/49502 (77%) | Loss: 0.408055\n",
      "Train Epoch: 19 | Batch Status: 39000/49502 (79%) | Loss: 0.526269\n",
      "Train Epoch: 19 | Batch Status: 40000/49502 (81%) | Loss: 0.390007\n",
      "Train Epoch: 19 | Batch Status: 41000/49502 (83%) | Loss: 0.115772\n",
      "Train Epoch: 19 | Batch Status: 42000/49502 (85%) | Loss: 0.147094\n",
      "Train Epoch: 19 | Batch Status: 43000/49502 (87%) | Loss: 0.503197\n",
      "Train Epoch: 19 | Batch Status: 44000/49502 (89%) | Loss: 0.379042\n",
      "Train Epoch: 19 | Batch Status: 45000/49502 (91%) | Loss: 0.384240\n",
      "Train Epoch: 19 | Batch Status: 46000/49502 (93%) | Loss: 0.429464\n",
      "Train Epoch: 19 | Batch Status: 47000/49502 (95%) | Loss: 0.310437\n",
      "Train Epoch: 19 | Batch Status: 48000/49502 (97%) | Loss: 0.345728\n",
      "Train Epoch: 19 | Batch Status: 49000/49502 (99%) | Loss: 0.609436\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.8296, Accuracy: 5831/12376(47.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 20 | Batch Status: 0/49502 (0%) | Loss: 0.315645\n",
      "Train Epoch: 20 | Batch Status: 1000/49502 (2%) | Loss: 0.698352\n",
      "Train Epoch: 20 | Batch Status: 2000/49502 (4%) | Loss: 0.588603\n",
      "Train Epoch: 20 | Batch Status: 3000/49502 (6%) | Loss: 0.213719\n",
      "Train Epoch: 20 | Batch Status: 4000/49502 (8%) | Loss: 0.371846\n",
      "Train Epoch: 20 | Batch Status: 5000/49502 (10%) | Loss: 0.097698\n",
      "Train Epoch: 20 | Batch Status: 6000/49502 (12%) | Loss: 0.384430\n",
      "Train Epoch: 20 | Batch Status: 7000/49502 (14%) | Loss: 0.212772\n",
      "Train Epoch: 20 | Batch Status: 8000/49502 (16%) | Loss: 0.265545\n",
      "Train Epoch: 20 | Batch Status: 9000/49502 (18%) | Loss: 0.376900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 | Batch Status: 10000/49502 (20%) | Loss: 0.545715\n",
      "Train Epoch: 20 | Batch Status: 11000/49502 (22%) | Loss: 0.230359\n",
      "Train Epoch: 20 | Batch Status: 12000/49502 (24%) | Loss: 0.518248\n",
      "Train Epoch: 20 | Batch Status: 13000/49502 (26%) | Loss: 0.133281\n",
      "Train Epoch: 20 | Batch Status: 14000/49502 (28%) | Loss: 0.501605\n",
      "Train Epoch: 20 | Batch Status: 15000/49502 (30%) | Loss: 0.408712\n",
      "Train Epoch: 20 | Batch Status: 16000/49502 (32%) | Loss: 0.782803\n",
      "Train Epoch: 20 | Batch Status: 17000/49502 (34%) | Loss: 0.518092\n",
      "Train Epoch: 20 | Batch Status: 18000/49502 (36%) | Loss: 0.271506\n",
      "Train Epoch: 20 | Batch Status: 19000/49502 (38%) | Loss: 0.591720\n",
      "Train Epoch: 20 | Batch Status: 20000/49502 (40%) | Loss: 0.439186\n",
      "Train Epoch: 20 | Batch Status: 21000/49502 (42%) | Loss: 1.253576\n",
      "Train Epoch: 20 | Batch Status: 22000/49502 (44%) | Loss: 0.711328\n",
      "Train Epoch: 20 | Batch Status: 23000/49502 (46%) | Loss: 0.278825\n",
      "Train Epoch: 20 | Batch Status: 24000/49502 (48%) | Loss: 0.896073\n",
      "Train Epoch: 20 | Batch Status: 25000/49502 (50%) | Loss: 0.099318\n",
      "Train Epoch: 20 | Batch Status: 26000/49502 (53%) | Loss: 0.478713\n",
      "Train Epoch: 20 | Batch Status: 27000/49502 (55%) | Loss: 0.276595\n",
      "Train Epoch: 20 | Batch Status: 28000/49502 (57%) | Loss: 0.399692\n",
      "Train Epoch: 20 | Batch Status: 29000/49502 (59%) | Loss: 0.491513\n",
      "Train Epoch: 20 | Batch Status: 30000/49502 (61%) | Loss: 0.269679\n",
      "Train Epoch: 20 | Batch Status: 31000/49502 (63%) | Loss: 0.523499\n",
      "Train Epoch: 20 | Batch Status: 32000/49502 (65%) | Loss: 0.233456\n",
      "Train Epoch: 20 | Batch Status: 33000/49502 (67%) | Loss: 0.700585\n",
      "Train Epoch: 20 | Batch Status: 34000/49502 (69%) | Loss: 0.394232\n",
      "Train Epoch: 20 | Batch Status: 35000/49502 (71%) | Loss: 0.310821\n",
      "Train Epoch: 20 | Batch Status: 36000/49502 (73%) | Loss: 0.192386\n",
      "Train Epoch: 20 | Batch Status: 37000/49502 (75%) | Loss: 0.398942\n",
      "Train Epoch: 20 | Batch Status: 38000/49502 (77%) | Loss: 0.383578\n",
      "Train Epoch: 20 | Batch Status: 39000/49502 (79%) | Loss: 0.962493\n",
      "Train Epoch: 20 | Batch Status: 40000/49502 (81%) | Loss: 0.243916\n",
      "Train Epoch: 20 | Batch Status: 41000/49502 (83%) | Loss: 1.141909\n",
      "Train Epoch: 20 | Batch Status: 42000/49502 (85%) | Loss: 0.245481\n",
      "Train Epoch: 20 | Batch Status: 43000/49502 (87%) | Loss: 0.144217\n",
      "Train Epoch: 20 | Batch Status: 44000/49502 (89%) | Loss: 0.585683\n",
      "Train Epoch: 20 | Batch Status: 45000/49502 (91%) | Loss: 0.393520\n",
      "Train Epoch: 20 | Batch Status: 46000/49502 (93%) | Loss: 0.540156\n",
      "Train Epoch: 20 | Batch Status: 47000/49502 (95%) | Loss: 0.469445\n",
      "Train Epoch: 20 | Batch Status: 48000/49502 (97%) | Loss: 0.351099\n",
      "Train Epoch: 20 | Batch Status: 49000/49502 (99%) | Loss: 0.289641\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.0743, Accuracy: 4394/12376(35.000000%)\n",
      "Testing time: 0m 7s\n",
      "Train Epoch: 21 | Batch Status: 0/49502 (0%) | Loss: 0.906706\n",
      "Train Epoch: 21 | Batch Status: 1000/49502 (2%) | Loss: 0.168121\n",
      "Train Epoch: 21 | Batch Status: 2000/49502 (4%) | Loss: 0.088953\n",
      "Train Epoch: 21 | Batch Status: 3000/49502 (6%) | Loss: 0.349887\n",
      "Train Epoch: 21 | Batch Status: 4000/49502 (8%) | Loss: 0.330075\n",
      "Train Epoch: 21 | Batch Status: 5000/49502 (10%) | Loss: 0.294307\n",
      "Train Epoch: 21 | Batch Status: 6000/49502 (12%) | Loss: 0.319209\n",
      "Train Epoch: 21 | Batch Status: 7000/49502 (14%) | Loss: 0.541042\n",
      "Train Epoch: 21 | Batch Status: 8000/49502 (16%) | Loss: 0.408415\n",
      "Train Epoch: 21 | Batch Status: 9000/49502 (18%) | Loss: 0.297460\n",
      "Train Epoch: 21 | Batch Status: 10000/49502 (20%) | Loss: 0.443660\n",
      "Train Epoch: 21 | Batch Status: 11000/49502 (22%) | Loss: 0.949142\n",
      "Train Epoch: 21 | Batch Status: 12000/49502 (24%) | Loss: 0.394844\n",
      "Train Epoch: 21 | Batch Status: 13000/49502 (26%) | Loss: 0.046038\n",
      "Train Epoch: 21 | Batch Status: 14000/49502 (28%) | Loss: 0.276648\n",
      "Train Epoch: 21 | Batch Status: 15000/49502 (30%) | Loss: 0.601394\n",
      "Train Epoch: 21 | Batch Status: 16000/49502 (32%) | Loss: 0.435256\n",
      "Train Epoch: 21 | Batch Status: 17000/49502 (34%) | Loss: 0.473658\n",
      "Train Epoch: 21 | Batch Status: 18000/49502 (36%) | Loss: 0.099590\n",
      "Train Epoch: 21 | Batch Status: 19000/49502 (38%) | Loss: 0.287637\n",
      "Train Epoch: 21 | Batch Status: 20000/49502 (40%) | Loss: 0.681761\n",
      "Train Epoch: 21 | Batch Status: 21000/49502 (42%) | Loss: 0.343929\n",
      "Train Epoch: 21 | Batch Status: 22000/49502 (44%) | Loss: 0.155385\n",
      "Train Epoch: 21 | Batch Status: 23000/49502 (46%) | Loss: 0.388280\n",
      "Train Epoch: 21 | Batch Status: 24000/49502 (48%) | Loss: 0.372337\n",
      "Train Epoch: 21 | Batch Status: 25000/49502 (50%) | Loss: 0.285393\n",
      "Train Epoch: 21 | Batch Status: 26000/49502 (53%) | Loss: 0.424343\n",
      "Train Epoch: 21 | Batch Status: 27000/49502 (55%) | Loss: 0.252180\n",
      "Train Epoch: 21 | Batch Status: 28000/49502 (57%) | Loss: 0.504757\n",
      "Train Epoch: 21 | Batch Status: 29000/49502 (59%) | Loss: 0.165551\n",
      "Train Epoch: 21 | Batch Status: 30000/49502 (61%) | Loss: 0.504198\n",
      "Train Epoch: 21 | Batch Status: 31000/49502 (63%) | Loss: 0.350009\n",
      "Train Epoch: 21 | Batch Status: 32000/49502 (65%) | Loss: 0.688136\n",
      "Train Epoch: 21 | Batch Status: 33000/49502 (67%) | Loss: 0.288022\n",
      "Train Epoch: 21 | Batch Status: 34000/49502 (69%) | Loss: 0.683715\n",
      "Train Epoch: 21 | Batch Status: 35000/49502 (71%) | Loss: 0.456334\n",
      "Train Epoch: 21 | Batch Status: 36000/49502 (73%) | Loss: 0.473127\n",
      "Train Epoch: 21 | Batch Status: 37000/49502 (75%) | Loss: 0.508538\n",
      "Train Epoch: 21 | Batch Status: 38000/49502 (77%) | Loss: 0.377265\n",
      "Train Epoch: 21 | Batch Status: 39000/49502 (79%) | Loss: 1.284924\n",
      "Train Epoch: 21 | Batch Status: 40000/49502 (81%) | Loss: 0.677413\n",
      "Train Epoch: 21 | Batch Status: 41000/49502 (83%) | Loss: 0.153060\n",
      "Train Epoch: 21 | Batch Status: 42000/49502 (85%) | Loss: 0.634981\n",
      "Train Epoch: 21 | Batch Status: 43000/49502 (87%) | Loss: 0.532009\n",
      "Train Epoch: 21 | Batch Status: 44000/49502 (89%) | Loss: 0.395909\n",
      "Train Epoch: 21 | Batch Status: 45000/49502 (91%) | Loss: 0.163508\n",
      "Train Epoch: 21 | Batch Status: 46000/49502 (93%) | Loss: 0.371105\n",
      "Train Epoch: 21 | Batch Status: 47000/49502 (95%) | Loss: 0.355988\n",
      "Train Epoch: 21 | Batch Status: 48000/49502 (97%) | Loss: 0.271719\n",
      "Train Epoch: 21 | Batch Status: 49000/49502 (99%) | Loss: 0.488497\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.9776, Accuracy: 5476/12376(44.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 22 | Batch Status: 0/49502 (0%) | Loss: 0.266885\n",
      "Train Epoch: 22 | Batch Status: 1000/49502 (2%) | Loss: 0.514008\n",
      "Train Epoch: 22 | Batch Status: 2000/49502 (4%) | Loss: 0.447135\n",
      "Train Epoch: 22 | Batch Status: 3000/49502 (6%) | Loss: 0.280446\n",
      "Train Epoch: 22 | Batch Status: 4000/49502 (8%) | Loss: 0.796908\n",
      "Train Epoch: 22 | Batch Status: 5000/49502 (10%) | Loss: 0.268067\n",
      "Train Epoch: 22 | Batch Status: 6000/49502 (12%) | Loss: 0.365374\n",
      "Train Epoch: 22 | Batch Status: 7000/49502 (14%) | Loss: 0.762208\n",
      "Train Epoch: 22 | Batch Status: 8000/49502 (16%) | Loss: 0.289485\n",
      "Train Epoch: 22 | Batch Status: 9000/49502 (18%) | Loss: 0.371000\n",
      "Train Epoch: 22 | Batch Status: 10000/49502 (20%) | Loss: 0.201722\n",
      "Train Epoch: 22 | Batch Status: 11000/49502 (22%) | Loss: 0.476754\n",
      "Train Epoch: 22 | Batch Status: 12000/49502 (24%) | Loss: 0.500410\n",
      "Train Epoch: 22 | Batch Status: 13000/49502 (26%) | Loss: 0.328514\n",
      "Train Epoch: 22 | Batch Status: 14000/49502 (28%) | Loss: 0.047582\n",
      "Train Epoch: 22 | Batch Status: 15000/49502 (30%) | Loss: 0.462711\n",
      "Train Epoch: 22 | Batch Status: 16000/49502 (32%) | Loss: 0.451323\n",
      "Train Epoch: 22 | Batch Status: 17000/49502 (34%) | Loss: 0.176332\n",
      "Train Epoch: 22 | Batch Status: 18000/49502 (36%) | Loss: 0.400014\n",
      "Train Epoch: 22 | Batch Status: 19000/49502 (38%) | Loss: 0.601889\n",
      "Train Epoch: 22 | Batch Status: 20000/49502 (40%) | Loss: 0.427445\n",
      "Train Epoch: 22 | Batch Status: 21000/49502 (42%) | Loss: 0.282418\n",
      "Train Epoch: 22 | Batch Status: 22000/49502 (44%) | Loss: 0.359931\n",
      "Train Epoch: 22 | Batch Status: 23000/49502 (46%) | Loss: 0.449095\n",
      "Train Epoch: 22 | Batch Status: 24000/49502 (48%) | Loss: 0.283067\n",
      "Train Epoch: 22 | Batch Status: 25000/49502 (50%) | Loss: 0.156674\n",
      "Train Epoch: 22 | Batch Status: 26000/49502 (53%) | Loss: 0.476032\n",
      "Train Epoch: 22 | Batch Status: 27000/49502 (55%) | Loss: 0.591004\n",
      "Train Epoch: 22 | Batch Status: 28000/49502 (57%) | Loss: 0.155529\n",
      "Train Epoch: 22 | Batch Status: 29000/49502 (59%) | Loss: 0.089284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 22 | Batch Status: 30000/49502 (61%) | Loss: 0.461836\n",
      "Train Epoch: 22 | Batch Status: 31000/49502 (63%) | Loss: 0.358510\n",
      "Train Epoch: 22 | Batch Status: 32000/49502 (65%) | Loss: 1.047038\n",
      "Train Epoch: 22 | Batch Status: 33000/49502 (67%) | Loss: 0.164630\n",
      "Train Epoch: 22 | Batch Status: 34000/49502 (69%) | Loss: 0.134666\n",
      "Train Epoch: 22 | Batch Status: 35000/49502 (71%) | Loss: 0.543470\n",
      "Train Epoch: 22 | Batch Status: 36000/49502 (73%) | Loss: 0.640231\n",
      "Train Epoch: 22 | Batch Status: 37000/49502 (75%) | Loss: 0.347883\n",
      "Train Epoch: 22 | Batch Status: 38000/49502 (77%) | Loss: 0.371190\n",
      "Train Epoch: 22 | Batch Status: 39000/49502 (79%) | Loss: 0.939488\n",
      "Train Epoch: 22 | Batch Status: 40000/49502 (81%) | Loss: 0.950191\n",
      "Train Epoch: 22 | Batch Status: 41000/49502 (83%) | Loss: 0.351882\n",
      "Train Epoch: 22 | Batch Status: 42000/49502 (85%) | Loss: 0.219856\n",
      "Train Epoch: 22 | Batch Status: 43000/49502 (87%) | Loss: 0.598555\n",
      "Train Epoch: 22 | Batch Status: 44000/49502 (89%) | Loss: 0.558417\n",
      "Train Epoch: 22 | Batch Status: 45000/49502 (91%) | Loss: 0.506460\n",
      "Train Epoch: 22 | Batch Status: 46000/49502 (93%) | Loss: 0.699211\n",
      "Train Epoch: 22 | Batch Status: 47000/49502 (95%) | Loss: 0.143991\n",
      "Train Epoch: 22 | Batch Status: 48000/49502 (97%) | Loss: 0.927950\n",
      "Train Epoch: 22 | Batch Status: 49000/49502 (99%) | Loss: 0.546654\n",
      "Training time: 0m 5s\n",
      "==========================\n",
      "Test set: Average loss: 1.0260, Accuracy: 5395/12376(43.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 23 | Batch Status: 0/49502 (0%) | Loss: 0.573052\n",
      "Train Epoch: 23 | Batch Status: 1000/49502 (2%) | Loss: 0.701971\n",
      "Train Epoch: 23 | Batch Status: 2000/49502 (4%) | Loss: 0.155427\n",
      "Train Epoch: 23 | Batch Status: 3000/49502 (6%) | Loss: 0.458929\n",
      "Train Epoch: 23 | Batch Status: 4000/49502 (8%) | Loss: 0.456999\n",
      "Train Epoch: 23 | Batch Status: 5000/49502 (10%) | Loss: 0.200393\n",
      "Train Epoch: 23 | Batch Status: 6000/49502 (12%) | Loss: 0.551090\n",
      "Train Epoch: 23 | Batch Status: 7000/49502 (14%) | Loss: 0.329003\n",
      "Train Epoch: 23 | Batch Status: 8000/49502 (16%) | Loss: 0.530338\n",
      "Train Epoch: 23 | Batch Status: 9000/49502 (18%) | Loss: 0.123061\n",
      "Train Epoch: 23 | Batch Status: 10000/49502 (20%) | Loss: 0.428476\n",
      "Train Epoch: 23 | Batch Status: 11000/49502 (22%) | Loss: 0.178896\n",
      "Train Epoch: 23 | Batch Status: 12000/49502 (24%) | Loss: 0.260061\n",
      "Train Epoch: 23 | Batch Status: 13000/49502 (26%) | Loss: 0.241723\n",
      "Train Epoch: 23 | Batch Status: 14000/49502 (28%) | Loss: 0.168661\n",
      "Train Epoch: 23 | Batch Status: 15000/49502 (30%) | Loss: 0.629785\n",
      "Train Epoch: 23 | Batch Status: 16000/49502 (32%) | Loss: 0.609804\n",
      "Train Epoch: 23 | Batch Status: 17000/49502 (34%) | Loss: 0.785959\n",
      "Train Epoch: 23 | Batch Status: 18000/49502 (36%) | Loss: 0.108679\n",
      "Train Epoch: 23 | Batch Status: 19000/49502 (38%) | Loss: 0.342065\n",
      "Train Epoch: 23 | Batch Status: 20000/49502 (40%) | Loss: 0.291546\n",
      "Train Epoch: 23 | Batch Status: 21000/49502 (42%) | Loss: 0.534589\n",
      "Train Epoch: 23 | Batch Status: 22000/49502 (44%) | Loss: 0.342386\n",
      "Train Epoch: 23 | Batch Status: 23000/49502 (46%) | Loss: 0.485058\n",
      "Train Epoch: 23 | Batch Status: 24000/49502 (48%) | Loss: 0.928417\n",
      "Train Epoch: 23 | Batch Status: 25000/49502 (50%) | Loss: 0.441939\n",
      "Train Epoch: 23 | Batch Status: 26000/49502 (53%) | Loss: 0.767190\n",
      "Train Epoch: 23 | Batch Status: 27000/49502 (55%) | Loss: 0.255194\n",
      "Train Epoch: 23 | Batch Status: 28000/49502 (57%) | Loss: 0.178709\n",
      "Train Epoch: 23 | Batch Status: 29000/49502 (59%) | Loss: 0.208175\n",
      "Train Epoch: 23 | Batch Status: 30000/49502 (61%) | Loss: 0.693689\n",
      "Train Epoch: 23 | Batch Status: 31000/49502 (63%) | Loss: 0.351216\n",
      "Train Epoch: 23 | Batch Status: 32000/49502 (65%) | Loss: 0.435844\n",
      "Train Epoch: 23 | Batch Status: 33000/49502 (67%) | Loss: 0.323103\n",
      "Train Epoch: 23 | Batch Status: 34000/49502 (69%) | Loss: 0.105622\n",
      "Train Epoch: 23 | Batch Status: 35000/49502 (71%) | Loss: 0.490137\n",
      "Train Epoch: 23 | Batch Status: 36000/49502 (73%) | Loss: 0.430553\n",
      "Train Epoch: 23 | Batch Status: 37000/49502 (75%) | Loss: 0.215923\n",
      "Train Epoch: 23 | Batch Status: 38000/49502 (77%) | Loss: 0.611142\n",
      "Train Epoch: 23 | Batch Status: 39000/49502 (79%) | Loss: 0.609718\n",
      "Train Epoch: 23 | Batch Status: 40000/49502 (81%) | Loss: 0.515031\n",
      "Train Epoch: 23 | Batch Status: 41000/49502 (83%) | Loss: 0.473781\n",
      "Train Epoch: 23 | Batch Status: 42000/49502 (85%) | Loss: 0.104951\n",
      "Train Epoch: 23 | Batch Status: 43000/49502 (87%) | Loss: 0.106799\n",
      "Train Epoch: 23 | Batch Status: 44000/49502 (89%) | Loss: 0.170394\n",
      "Train Epoch: 23 | Batch Status: 45000/49502 (91%) | Loss: 0.355523\n",
      "Train Epoch: 23 | Batch Status: 46000/49502 (93%) | Loss: 0.667718\n",
      "Train Epoch: 23 | Batch Status: 47000/49502 (95%) | Loss: 0.489897\n",
      "Train Epoch: 23 | Batch Status: 48000/49502 (97%) | Loss: 0.405308\n",
      "Train Epoch: 23 | Batch Status: 49000/49502 (99%) | Loss: 1.390373\n",
      "Training time: 0m 5s\n",
      "==========================\n",
      "Test set: Average loss: 1.0272, Accuracy: 5544/12376(44.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 24 | Batch Status: 0/49502 (0%) | Loss: 0.678362\n",
      "Train Epoch: 24 | Batch Status: 1000/49502 (2%) | Loss: 0.266919\n",
      "Train Epoch: 24 | Batch Status: 2000/49502 (4%) | Loss: 0.422048\n",
      "Train Epoch: 24 | Batch Status: 3000/49502 (6%) | Loss: 0.609580\n",
      "Train Epoch: 24 | Batch Status: 4000/49502 (8%) | Loss: 0.795792\n",
      "Train Epoch: 24 | Batch Status: 5000/49502 (10%) | Loss: 0.483279\n",
      "Train Epoch: 24 | Batch Status: 6000/49502 (12%) | Loss: 0.516421\n",
      "Train Epoch: 24 | Batch Status: 7000/49502 (14%) | Loss: 0.763701\n",
      "Train Epoch: 24 | Batch Status: 8000/49502 (16%) | Loss: 0.458397\n",
      "Train Epoch: 24 | Batch Status: 9000/49502 (18%) | Loss: 0.484460\n",
      "Train Epoch: 24 | Batch Status: 10000/49502 (20%) | Loss: 0.851874\n",
      "Train Epoch: 24 | Batch Status: 11000/49502 (22%) | Loss: 0.329819\n",
      "Train Epoch: 24 | Batch Status: 12000/49502 (24%) | Loss: 0.467832\n",
      "Train Epoch: 24 | Batch Status: 13000/49502 (26%) | Loss: 0.252542\n",
      "Train Epoch: 24 | Batch Status: 14000/49502 (28%) | Loss: 0.318079\n",
      "Train Epoch: 24 | Batch Status: 15000/49502 (30%) | Loss: 0.546860\n",
      "Train Epoch: 24 | Batch Status: 16000/49502 (32%) | Loss: 0.347794\n",
      "Train Epoch: 24 | Batch Status: 17000/49502 (34%) | Loss: 0.724639\n",
      "Train Epoch: 24 | Batch Status: 18000/49502 (36%) | Loss: 0.241517\n",
      "Train Epoch: 24 | Batch Status: 19000/49502 (38%) | Loss: 0.751666\n",
      "Train Epoch: 24 | Batch Status: 20000/49502 (40%) | Loss: 0.515903\n",
      "Train Epoch: 24 | Batch Status: 21000/49502 (42%) | Loss: 0.206155\n",
      "Train Epoch: 24 | Batch Status: 22000/49502 (44%) | Loss: 0.190383\n",
      "Train Epoch: 24 | Batch Status: 23000/49502 (46%) | Loss: 0.390815\n",
      "Train Epoch: 24 | Batch Status: 24000/49502 (48%) | Loss: 0.189528\n",
      "Train Epoch: 24 | Batch Status: 25000/49502 (50%) | Loss: 0.596385\n",
      "Train Epoch: 24 | Batch Status: 26000/49502 (53%) | Loss: 0.591962\n",
      "Train Epoch: 24 | Batch Status: 27000/49502 (55%) | Loss: 0.316975\n",
      "Train Epoch: 24 | Batch Status: 28000/49502 (57%) | Loss: 0.462469\n",
      "Train Epoch: 24 | Batch Status: 29000/49502 (59%) | Loss: 0.509389\n",
      "Train Epoch: 24 | Batch Status: 30000/49502 (61%) | Loss: 0.246740\n",
      "Train Epoch: 24 | Batch Status: 31000/49502 (63%) | Loss: 0.196900\n",
      "Train Epoch: 24 | Batch Status: 32000/49502 (65%) | Loss: 0.510490\n",
      "Train Epoch: 24 | Batch Status: 33000/49502 (67%) | Loss: 0.725792\n",
      "Train Epoch: 24 | Batch Status: 34000/49502 (69%) | Loss: 0.461370\n",
      "Train Epoch: 24 | Batch Status: 35000/49502 (71%) | Loss: 0.856027\n",
      "Train Epoch: 24 | Batch Status: 36000/49502 (73%) | Loss: 0.454558\n",
      "Train Epoch: 24 | Batch Status: 37000/49502 (75%) | Loss: 0.183931\n",
      "Train Epoch: 24 | Batch Status: 38000/49502 (77%) | Loss: 0.404362\n",
      "Train Epoch: 24 | Batch Status: 39000/49502 (79%) | Loss: 1.321170\n",
      "Train Epoch: 24 | Batch Status: 40000/49502 (81%) | Loss: 0.309228\n",
      "Train Epoch: 24 | Batch Status: 41000/49502 (83%) | Loss: 0.341418\n",
      "Train Epoch: 24 | Batch Status: 42000/49502 (85%) | Loss: 0.558994\n",
      "Train Epoch: 24 | Batch Status: 43000/49502 (87%) | Loss: 0.167881\n",
      "Train Epoch: 24 | Batch Status: 44000/49502 (89%) | Loss: 0.482512\n",
      "Train Epoch: 24 | Batch Status: 45000/49502 (91%) | Loss: 0.428128\n",
      "Train Epoch: 24 | Batch Status: 46000/49502 (93%) | Loss: 0.474230\n",
      "Train Epoch: 24 | Batch Status: 47000/49502 (95%) | Loss: 0.202149\n",
      "Train Epoch: 24 | Batch Status: 48000/49502 (97%) | Loss: 0.911395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 | Batch Status: 49000/49502 (99%) | Loss: 0.155236\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 0.9893, Accuracy: 5595/12376(45.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 25 | Batch Status: 0/49502 (0%) | Loss: 0.224719\n",
      "Train Epoch: 25 | Batch Status: 1000/49502 (2%) | Loss: 0.345538\n",
      "Train Epoch: 25 | Batch Status: 2000/49502 (4%) | Loss: 0.725696\n",
      "Train Epoch: 25 | Batch Status: 3000/49502 (6%) | Loss: 0.922477\n",
      "Train Epoch: 25 | Batch Status: 4000/49502 (8%) | Loss: 0.422561\n",
      "Train Epoch: 25 | Batch Status: 5000/49502 (10%) | Loss: 0.516349\n",
      "Train Epoch: 25 | Batch Status: 6000/49502 (12%) | Loss: 0.416935\n",
      "Train Epoch: 25 | Batch Status: 7000/49502 (14%) | Loss: 0.404216\n",
      "Train Epoch: 25 | Batch Status: 8000/49502 (16%) | Loss: 0.277045\n",
      "Train Epoch: 25 | Batch Status: 9000/49502 (18%) | Loss: 0.336006\n",
      "Train Epoch: 25 | Batch Status: 10000/49502 (20%) | Loss: 0.417251\n",
      "Train Epoch: 25 | Batch Status: 11000/49502 (22%) | Loss: 0.074796\n",
      "Train Epoch: 25 | Batch Status: 12000/49502 (24%) | Loss: 0.575638\n",
      "Train Epoch: 25 | Batch Status: 13000/49502 (26%) | Loss: 0.501132\n",
      "Train Epoch: 25 | Batch Status: 14000/49502 (28%) | Loss: 0.458054\n",
      "Train Epoch: 25 | Batch Status: 15000/49502 (30%) | Loss: 0.328375\n",
      "Train Epoch: 25 | Batch Status: 16000/49502 (32%) | Loss: 0.752909\n",
      "Train Epoch: 25 | Batch Status: 17000/49502 (34%) | Loss: 0.507470\n",
      "Train Epoch: 25 | Batch Status: 18000/49502 (36%) | Loss: 0.178315\n",
      "Train Epoch: 25 | Batch Status: 19000/49502 (38%) | Loss: 0.479131\n",
      "Train Epoch: 25 | Batch Status: 20000/49502 (40%) | Loss: 0.340807\n",
      "Train Epoch: 25 | Batch Status: 21000/49502 (42%) | Loss: 0.133243\n",
      "Train Epoch: 25 | Batch Status: 22000/49502 (44%) | Loss: 0.394272\n",
      "Train Epoch: 25 | Batch Status: 23000/49502 (46%) | Loss: 0.343545\n",
      "Train Epoch: 25 | Batch Status: 24000/49502 (48%) | Loss: 0.131327\n",
      "Train Epoch: 25 | Batch Status: 25000/49502 (50%) | Loss: 0.581701\n",
      "Train Epoch: 25 | Batch Status: 26000/49502 (53%) | Loss: 0.237726\n",
      "Train Epoch: 25 | Batch Status: 27000/49502 (55%) | Loss: 0.575639\n",
      "Train Epoch: 25 | Batch Status: 28000/49502 (57%) | Loss: 0.488922\n",
      "Train Epoch: 25 | Batch Status: 29000/49502 (59%) | Loss: 0.355617\n",
      "Train Epoch: 25 | Batch Status: 30000/49502 (61%) | Loss: 0.631862\n",
      "Train Epoch: 25 | Batch Status: 31000/49502 (63%) | Loss: 0.511219\n",
      "Train Epoch: 25 | Batch Status: 32000/49502 (65%) | Loss: 0.950556\n",
      "Train Epoch: 25 | Batch Status: 33000/49502 (67%) | Loss: 0.406147\n",
      "Train Epoch: 25 | Batch Status: 34000/49502 (69%) | Loss: 0.183652\n",
      "Train Epoch: 25 | Batch Status: 35000/49502 (71%) | Loss: 0.331424\n",
      "Train Epoch: 25 | Batch Status: 36000/49502 (73%) | Loss: 0.411057\n",
      "Train Epoch: 25 | Batch Status: 37000/49502 (75%) | Loss: 0.508498\n",
      "Train Epoch: 25 | Batch Status: 38000/49502 (77%) | Loss: 0.442167\n",
      "Train Epoch: 25 | Batch Status: 39000/49502 (79%) | Loss: 0.192729\n",
      "Train Epoch: 25 | Batch Status: 40000/49502 (81%) | Loss: 0.304347\n",
      "Train Epoch: 25 | Batch Status: 41000/49502 (83%) | Loss: 0.736765\n",
      "Train Epoch: 25 | Batch Status: 42000/49502 (85%) | Loss: 0.298428\n",
      "Train Epoch: 25 | Batch Status: 43000/49502 (87%) | Loss: 0.202756\n",
      "Train Epoch: 25 | Batch Status: 44000/49502 (89%) | Loss: 1.020094\n",
      "Train Epoch: 25 | Batch Status: 45000/49502 (91%) | Loss: 0.115333\n",
      "Train Epoch: 25 | Batch Status: 46000/49502 (93%) | Loss: 0.538608\n",
      "Train Epoch: 25 | Batch Status: 47000/49502 (95%) | Loss: 1.085073\n",
      "Train Epoch: 25 | Batch Status: 48000/49502 (97%) | Loss: 0.162331\n",
      "Train Epoch: 25 | Batch Status: 49000/49502 (99%) | Loss: 0.340619\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.0654, Accuracy: 5140/12376(41.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 26 | Batch Status: 0/49502 (0%) | Loss: 0.345117\n",
      "Train Epoch: 26 | Batch Status: 1000/49502 (2%) | Loss: 0.093532\n",
      "Train Epoch: 26 | Batch Status: 2000/49502 (4%) | Loss: 0.701371\n",
      "Train Epoch: 26 | Batch Status: 3000/49502 (6%) | Loss: 0.055811\n",
      "Train Epoch: 26 | Batch Status: 4000/49502 (8%) | Loss: 0.433200\n",
      "Train Epoch: 26 | Batch Status: 5000/49502 (10%) | Loss: 0.380206\n",
      "Train Epoch: 26 | Batch Status: 6000/49502 (12%) | Loss: 0.125589\n",
      "Train Epoch: 26 | Batch Status: 7000/49502 (14%) | Loss: 0.536086\n",
      "Train Epoch: 26 | Batch Status: 8000/49502 (16%) | Loss: 0.613583\n",
      "Train Epoch: 26 | Batch Status: 9000/49502 (18%) | Loss: 0.333834\n",
      "Train Epoch: 26 | Batch Status: 10000/49502 (20%) | Loss: 0.344089\n",
      "Train Epoch: 26 | Batch Status: 11000/49502 (22%) | Loss: 0.481541\n",
      "Train Epoch: 26 | Batch Status: 12000/49502 (24%) | Loss: 0.321079\n",
      "Train Epoch: 26 | Batch Status: 13000/49502 (26%) | Loss: 0.204003\n",
      "Train Epoch: 26 | Batch Status: 14000/49502 (28%) | Loss: 0.384333\n",
      "Train Epoch: 26 | Batch Status: 15000/49502 (30%) | Loss: 0.189511\n",
      "Train Epoch: 26 | Batch Status: 16000/49502 (32%) | Loss: 0.714273\n",
      "Train Epoch: 26 | Batch Status: 17000/49502 (34%) | Loss: 0.685055\n",
      "Train Epoch: 26 | Batch Status: 18000/49502 (36%) | Loss: 0.730409\n",
      "Train Epoch: 26 | Batch Status: 19000/49502 (38%) | Loss: 0.399374\n",
      "Train Epoch: 26 | Batch Status: 20000/49502 (40%) | Loss: 0.225978\n",
      "Train Epoch: 26 | Batch Status: 21000/49502 (42%) | Loss: 0.282698\n",
      "Train Epoch: 26 | Batch Status: 22000/49502 (44%) | Loss: 0.295144\n",
      "Train Epoch: 26 | Batch Status: 23000/49502 (46%) | Loss: 0.136327\n",
      "Train Epoch: 26 | Batch Status: 24000/49502 (48%) | Loss: 1.017016\n",
      "Train Epoch: 26 | Batch Status: 25000/49502 (50%) | Loss: 0.436426\n",
      "Train Epoch: 26 | Batch Status: 26000/49502 (53%) | Loss: 0.592077\n",
      "Train Epoch: 26 | Batch Status: 27000/49502 (55%) | Loss: 0.336774\n",
      "Train Epoch: 26 | Batch Status: 28000/49502 (57%) | Loss: 0.140971\n",
      "Train Epoch: 26 | Batch Status: 29000/49502 (59%) | Loss: 0.352850\n",
      "Train Epoch: 26 | Batch Status: 30000/49502 (61%) | Loss: 0.526837\n",
      "Train Epoch: 26 | Batch Status: 31000/49502 (63%) | Loss: 0.291348\n",
      "Train Epoch: 26 | Batch Status: 32000/49502 (65%) | Loss: 0.483079\n",
      "Train Epoch: 26 | Batch Status: 33000/49502 (67%) | Loss: 0.580908\n",
      "Train Epoch: 26 | Batch Status: 34000/49502 (69%) | Loss: 0.512393\n",
      "Train Epoch: 26 | Batch Status: 35000/49502 (71%) | Loss: 1.117419\n",
      "Train Epoch: 26 | Batch Status: 36000/49502 (73%) | Loss: 0.589947\n",
      "Train Epoch: 26 | Batch Status: 37000/49502 (75%) | Loss: 0.671644\n",
      "Train Epoch: 26 | Batch Status: 38000/49502 (77%) | Loss: 0.428857\n",
      "Train Epoch: 26 | Batch Status: 39000/49502 (79%) | Loss: 0.437763\n",
      "Train Epoch: 26 | Batch Status: 40000/49502 (81%) | Loss: 0.270990\n",
      "Train Epoch: 26 | Batch Status: 41000/49502 (83%) | Loss: 0.404562\n",
      "Train Epoch: 26 | Batch Status: 42000/49502 (85%) | Loss: 0.287850\n",
      "Train Epoch: 26 | Batch Status: 43000/49502 (87%) | Loss: 0.914182\n",
      "Train Epoch: 26 | Batch Status: 44000/49502 (89%) | Loss: 0.307165\n",
      "Train Epoch: 26 | Batch Status: 45000/49502 (91%) | Loss: 0.424011\n",
      "Train Epoch: 26 | Batch Status: 46000/49502 (93%) | Loss: 0.306416\n",
      "Train Epoch: 26 | Batch Status: 47000/49502 (95%) | Loss: 0.823141\n",
      "Train Epoch: 26 | Batch Status: 48000/49502 (97%) | Loss: 0.374183\n",
      "Train Epoch: 26 | Batch Status: 49000/49502 (99%) | Loss: 0.570673\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.1503, Accuracy: 5875/12376(47.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 27 | Batch Status: 0/49502 (0%) | Loss: 0.681008\n",
      "Train Epoch: 27 | Batch Status: 1000/49502 (2%) | Loss: 0.626144\n",
      "Train Epoch: 27 | Batch Status: 2000/49502 (4%) | Loss: 1.141856\n",
      "Train Epoch: 27 | Batch Status: 3000/49502 (6%) | Loss: 0.910564\n",
      "Train Epoch: 27 | Batch Status: 4000/49502 (8%) | Loss: 0.609111\n",
      "Train Epoch: 27 | Batch Status: 5000/49502 (10%) | Loss: 0.310435\n",
      "Train Epoch: 27 | Batch Status: 6000/49502 (12%) | Loss: 0.185855\n",
      "Train Epoch: 27 | Batch Status: 7000/49502 (14%) | Loss: 0.182197\n",
      "Train Epoch: 27 | Batch Status: 8000/49502 (16%) | Loss: 1.099092\n",
      "Train Epoch: 27 | Batch Status: 9000/49502 (18%) | Loss: 0.180378\n",
      "Train Epoch: 27 | Batch Status: 10000/49502 (20%) | Loss: 0.590430\n",
      "Train Epoch: 27 | Batch Status: 11000/49502 (22%) | Loss: 0.143564\n",
      "Train Epoch: 27 | Batch Status: 12000/49502 (24%) | Loss: 0.248290\n",
      "Train Epoch: 27 | Batch Status: 13000/49502 (26%) | Loss: 0.363532\n",
      "Train Epoch: 27 | Batch Status: 14000/49502 (28%) | Loss: 0.492907\n",
      "Train Epoch: 27 | Batch Status: 15000/49502 (30%) | Loss: 0.296994\n",
      "Train Epoch: 27 | Batch Status: 16000/49502 (32%) | Loss: 0.006345\n",
      "Train Epoch: 27 | Batch Status: 17000/49502 (34%) | Loss: 0.147329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 27 | Batch Status: 18000/49502 (36%) | Loss: 0.122694\n",
      "Train Epoch: 27 | Batch Status: 19000/49502 (38%) | Loss: 0.764405\n",
      "Train Epoch: 27 | Batch Status: 20000/49502 (40%) | Loss: 0.278872\n",
      "Train Epoch: 27 | Batch Status: 21000/49502 (42%) | Loss: 0.382849\n",
      "Train Epoch: 27 | Batch Status: 22000/49502 (44%) | Loss: 0.188495\n",
      "Train Epoch: 27 | Batch Status: 23000/49502 (46%) | Loss: 0.420835\n",
      "Train Epoch: 27 | Batch Status: 24000/49502 (48%) | Loss: 0.788735\n",
      "Train Epoch: 27 | Batch Status: 25000/49502 (50%) | Loss: 0.636028\n",
      "Train Epoch: 27 | Batch Status: 26000/49502 (53%) | Loss: 0.196926\n",
      "Train Epoch: 27 | Batch Status: 27000/49502 (55%) | Loss: 0.131308\n",
      "Train Epoch: 27 | Batch Status: 28000/49502 (57%) | Loss: 0.218650\n",
      "Train Epoch: 27 | Batch Status: 29000/49502 (59%) | Loss: 0.664887\n",
      "Train Epoch: 27 | Batch Status: 30000/49502 (61%) | Loss: 0.395983\n",
      "Train Epoch: 27 | Batch Status: 31000/49502 (63%) | Loss: 0.300770\n",
      "Train Epoch: 27 | Batch Status: 32000/49502 (65%) | Loss: 0.239591\n",
      "Train Epoch: 27 | Batch Status: 33000/49502 (67%) | Loss: 0.535796\n",
      "Train Epoch: 27 | Batch Status: 34000/49502 (69%) | Loss: 0.755061\n",
      "Train Epoch: 27 | Batch Status: 35000/49502 (71%) | Loss: 0.270188\n",
      "Train Epoch: 27 | Batch Status: 36000/49502 (73%) | Loss: 0.816290\n",
      "Train Epoch: 27 | Batch Status: 37000/49502 (75%) | Loss: 0.114398\n",
      "Train Epoch: 27 | Batch Status: 38000/49502 (77%) | Loss: 0.699370\n",
      "Train Epoch: 27 | Batch Status: 39000/49502 (79%) | Loss: 0.473750\n",
      "Train Epoch: 27 | Batch Status: 40000/49502 (81%) | Loss: 0.492672\n",
      "Train Epoch: 27 | Batch Status: 41000/49502 (83%) | Loss: 0.384531\n",
      "Train Epoch: 27 | Batch Status: 42000/49502 (85%) | Loss: 0.304869\n",
      "Train Epoch: 27 | Batch Status: 43000/49502 (87%) | Loss: 0.171662\n",
      "Train Epoch: 27 | Batch Status: 44000/49502 (89%) | Loss: 0.280663\n",
      "Train Epoch: 27 | Batch Status: 45000/49502 (91%) | Loss: 0.341220\n",
      "Train Epoch: 27 | Batch Status: 46000/49502 (93%) | Loss: 0.471670\n",
      "Train Epoch: 27 | Batch Status: 47000/49502 (95%) | Loss: 0.471961\n",
      "Train Epoch: 27 | Batch Status: 48000/49502 (97%) | Loss: 0.699061\n",
      "Train Epoch: 27 | Batch Status: 49000/49502 (99%) | Loss: 0.245025\n",
      "Training time: 0m 5s\n",
      "==========================\n",
      "Test set: Average loss: 1.1083, Accuracy: 5635/12376(45.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 28 | Batch Status: 0/49502 (0%) | Loss: 0.589405\n",
      "Train Epoch: 28 | Batch Status: 1000/49502 (2%) | Loss: 0.447643\n",
      "Train Epoch: 28 | Batch Status: 2000/49502 (4%) | Loss: 0.096334\n",
      "Train Epoch: 28 | Batch Status: 3000/49502 (6%) | Loss: 1.312538\n",
      "Train Epoch: 28 | Batch Status: 4000/49502 (8%) | Loss: 0.282341\n",
      "Train Epoch: 28 | Batch Status: 5000/49502 (10%) | Loss: 0.473632\n",
      "Train Epoch: 28 | Batch Status: 6000/49502 (12%) | Loss: 0.300183\n",
      "Train Epoch: 28 | Batch Status: 7000/49502 (14%) | Loss: 0.157929\n",
      "Train Epoch: 28 | Batch Status: 8000/49502 (16%) | Loss: 0.691556\n",
      "Train Epoch: 28 | Batch Status: 9000/49502 (18%) | Loss: 0.555004\n",
      "Train Epoch: 28 | Batch Status: 10000/49502 (20%) | Loss: 0.609198\n",
      "Train Epoch: 28 | Batch Status: 11000/49502 (22%) | Loss: 0.241517\n",
      "Train Epoch: 28 | Batch Status: 12000/49502 (24%) | Loss: 0.515112\n",
      "Train Epoch: 28 | Batch Status: 13000/49502 (26%) | Loss: 0.586014\n",
      "Train Epoch: 28 | Batch Status: 14000/49502 (28%) | Loss: 0.287502\n",
      "Train Epoch: 28 | Batch Status: 15000/49502 (30%) | Loss: 0.431415\n",
      "Train Epoch: 28 | Batch Status: 16000/49502 (32%) | Loss: 0.163414\n",
      "Train Epoch: 28 | Batch Status: 17000/49502 (34%) | Loss: 0.498829\n",
      "Train Epoch: 28 | Batch Status: 18000/49502 (36%) | Loss: 0.243701\n",
      "Train Epoch: 28 | Batch Status: 19000/49502 (38%) | Loss: 0.366708\n",
      "Train Epoch: 28 | Batch Status: 20000/49502 (40%) | Loss: 0.505917\n",
      "Train Epoch: 28 | Batch Status: 21000/49502 (42%) | Loss: 0.468495\n",
      "Train Epoch: 28 | Batch Status: 22000/49502 (44%) | Loss: 0.709098\n",
      "Train Epoch: 28 | Batch Status: 23000/49502 (46%) | Loss: 0.134939\n",
      "Train Epoch: 28 | Batch Status: 24000/49502 (48%) | Loss: 0.365022\n",
      "Train Epoch: 28 | Batch Status: 25000/49502 (50%) | Loss: 0.210106\n",
      "Train Epoch: 28 | Batch Status: 26000/49502 (53%) | Loss: 0.619547\n",
      "Train Epoch: 28 | Batch Status: 27000/49502 (55%) | Loss: 0.421377\n",
      "Train Epoch: 28 | Batch Status: 28000/49502 (57%) | Loss: 0.533098\n",
      "Train Epoch: 28 | Batch Status: 29000/49502 (59%) | Loss: 0.417917\n",
      "Train Epoch: 28 | Batch Status: 30000/49502 (61%) | Loss: 0.868489\n",
      "Train Epoch: 28 | Batch Status: 31000/49502 (63%) | Loss: 0.270961\n",
      "Train Epoch: 28 | Batch Status: 32000/49502 (65%) | Loss: 0.911007\n",
      "Train Epoch: 28 | Batch Status: 33000/49502 (67%) | Loss: 0.477882\n",
      "Train Epoch: 28 | Batch Status: 34000/49502 (69%) | Loss: 0.213708\n",
      "Train Epoch: 28 | Batch Status: 35000/49502 (71%) | Loss: 0.872815\n",
      "Train Epoch: 28 | Batch Status: 36000/49502 (73%) | Loss: 0.373601\n",
      "Train Epoch: 28 | Batch Status: 37000/49502 (75%) | Loss: 1.301794\n",
      "Train Epoch: 28 | Batch Status: 38000/49502 (77%) | Loss: 0.379657\n",
      "Train Epoch: 28 | Batch Status: 39000/49502 (79%) | Loss: 0.531093\n",
      "Train Epoch: 28 | Batch Status: 40000/49502 (81%) | Loss: 0.143049\n",
      "Train Epoch: 28 | Batch Status: 41000/49502 (83%) | Loss: 0.669238\n",
      "Train Epoch: 28 | Batch Status: 42000/49502 (85%) | Loss: 0.258441\n",
      "Train Epoch: 28 | Batch Status: 43000/49502 (87%) | Loss: 0.505827\n",
      "Train Epoch: 28 | Batch Status: 44000/49502 (89%) | Loss: 0.186547\n",
      "Train Epoch: 28 | Batch Status: 45000/49502 (91%) | Loss: 0.262842\n",
      "Train Epoch: 28 | Batch Status: 46000/49502 (93%) | Loss: 0.979481\n",
      "Train Epoch: 28 | Batch Status: 47000/49502 (95%) | Loss: 0.330382\n",
      "Train Epoch: 28 | Batch Status: 48000/49502 (97%) | Loss: 0.507893\n",
      "Train Epoch: 28 | Batch Status: 49000/49502 (99%) | Loss: 0.193886\n",
      "Training time: 0m 5s\n",
      "==========================\n",
      "Test set: Average loss: 1.1048, Accuracy: 5845/12376(47.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 29 | Batch Status: 0/49502 (0%) | Loss: 0.589733\n",
      "Train Epoch: 29 | Batch Status: 1000/49502 (2%) | Loss: 0.360364\n",
      "Train Epoch: 29 | Batch Status: 2000/49502 (4%) | Loss: 0.369054\n",
      "Train Epoch: 29 | Batch Status: 3000/49502 (6%) | Loss: 0.320941\n",
      "Train Epoch: 29 | Batch Status: 4000/49502 (8%) | Loss: 0.599710\n",
      "Train Epoch: 29 | Batch Status: 5000/49502 (10%) | Loss: 0.598172\n",
      "Train Epoch: 29 | Batch Status: 6000/49502 (12%) | Loss: 0.117480\n",
      "Train Epoch: 29 | Batch Status: 7000/49502 (14%) | Loss: 0.268039\n",
      "Train Epoch: 29 | Batch Status: 8000/49502 (16%) | Loss: 0.079516\n",
      "Train Epoch: 29 | Batch Status: 9000/49502 (18%) | Loss: 0.210576\n",
      "Train Epoch: 29 | Batch Status: 10000/49502 (20%) | Loss: 0.110531\n",
      "Train Epoch: 29 | Batch Status: 11000/49502 (22%) | Loss: 0.275967\n",
      "Train Epoch: 29 | Batch Status: 12000/49502 (24%) | Loss: 0.121086\n",
      "Train Epoch: 29 | Batch Status: 13000/49502 (26%) | Loss: 0.275927\n",
      "Train Epoch: 29 | Batch Status: 14000/49502 (28%) | Loss: 0.654113\n",
      "Train Epoch: 29 | Batch Status: 15000/49502 (30%) | Loss: 0.247151\n",
      "Train Epoch: 29 | Batch Status: 16000/49502 (32%) | Loss: 0.461158\n",
      "Train Epoch: 29 | Batch Status: 17000/49502 (34%) | Loss: 0.385907\n",
      "Train Epoch: 29 | Batch Status: 18000/49502 (36%) | Loss: 0.351677\n",
      "Train Epoch: 29 | Batch Status: 19000/49502 (38%) | Loss: 0.621051\n",
      "Train Epoch: 29 | Batch Status: 20000/49502 (40%) | Loss: 0.231413\n",
      "Train Epoch: 29 | Batch Status: 21000/49502 (42%) | Loss: 0.327733\n",
      "Train Epoch: 29 | Batch Status: 22000/49502 (44%) | Loss: 0.509665\n",
      "Train Epoch: 29 | Batch Status: 23000/49502 (46%) | Loss: 0.277105\n",
      "Train Epoch: 29 | Batch Status: 24000/49502 (48%) | Loss: 0.311898\n",
      "Train Epoch: 29 | Batch Status: 25000/49502 (50%) | Loss: 0.306074\n",
      "Train Epoch: 29 | Batch Status: 26000/49502 (53%) | Loss: 0.105639\n",
      "Train Epoch: 29 | Batch Status: 27000/49502 (55%) | Loss: 0.322297\n",
      "Train Epoch: 29 | Batch Status: 28000/49502 (57%) | Loss: 0.810050\n",
      "Train Epoch: 29 | Batch Status: 29000/49502 (59%) | Loss: 0.352105\n",
      "Train Epoch: 29 | Batch Status: 30000/49502 (61%) | Loss: 0.418566\n",
      "Train Epoch: 29 | Batch Status: 31000/49502 (63%) | Loss: 0.264176\n",
      "Train Epoch: 29 | Batch Status: 32000/49502 (65%) | Loss: 0.339314\n",
      "Train Epoch: 29 | Batch Status: 33000/49502 (67%) | Loss: 0.540847\n",
      "Train Epoch: 29 | Batch Status: 34000/49502 (69%) | Loss: 0.117582\n",
      "Train Epoch: 29 | Batch Status: 35000/49502 (71%) | Loss: 0.535749\n",
      "Train Epoch: 29 | Batch Status: 36000/49502 (73%) | Loss: 0.216531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 29 | Batch Status: 37000/49502 (75%) | Loss: 0.407482\n",
      "Train Epoch: 29 | Batch Status: 38000/49502 (77%) | Loss: 0.382114\n",
      "Train Epoch: 29 | Batch Status: 39000/49502 (79%) | Loss: 0.169189\n",
      "Train Epoch: 29 | Batch Status: 40000/49502 (81%) | Loss: 0.272187\n",
      "Train Epoch: 29 | Batch Status: 41000/49502 (83%) | Loss: 0.483299\n",
      "Train Epoch: 29 | Batch Status: 42000/49502 (85%) | Loss: 0.324457\n",
      "Train Epoch: 29 | Batch Status: 43000/49502 (87%) | Loss: 0.230480\n",
      "Train Epoch: 29 | Batch Status: 44000/49502 (89%) | Loss: 0.080947\n",
      "Train Epoch: 29 | Batch Status: 45000/49502 (91%) | Loss: 0.642349\n",
      "Train Epoch: 29 | Batch Status: 46000/49502 (93%) | Loss: 0.492233\n",
      "Train Epoch: 29 | Batch Status: 47000/49502 (95%) | Loss: 0.160630\n",
      "Train Epoch: 29 | Batch Status: 48000/49502 (97%) | Loss: 0.240451\n",
      "Train Epoch: 29 | Batch Status: 49000/49502 (99%) | Loss: 0.309435\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.2320, Accuracy: 4924/12376(39.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 30 | Batch Status: 0/49502 (0%) | Loss: 0.593570\n",
      "Train Epoch: 30 | Batch Status: 1000/49502 (2%) | Loss: 0.306650\n",
      "Train Epoch: 30 | Batch Status: 2000/49502 (4%) | Loss: 0.323539\n",
      "Train Epoch: 30 | Batch Status: 3000/49502 (6%) | Loss: 0.297988\n",
      "Train Epoch: 30 | Batch Status: 4000/49502 (8%) | Loss: 0.521787\n",
      "Train Epoch: 30 | Batch Status: 5000/49502 (10%) | Loss: 0.048079\n",
      "Train Epoch: 30 | Batch Status: 6000/49502 (12%) | Loss: 0.152688\n",
      "Train Epoch: 30 | Batch Status: 7000/49502 (14%) | Loss: 0.310847\n",
      "Train Epoch: 30 | Batch Status: 8000/49502 (16%) | Loss: 0.691230\n",
      "Train Epoch: 30 | Batch Status: 9000/49502 (18%) | Loss: 0.366570\n",
      "Train Epoch: 30 | Batch Status: 10000/49502 (20%) | Loss: 0.349869\n",
      "Train Epoch: 30 | Batch Status: 11000/49502 (22%) | Loss: 1.031012\n",
      "Train Epoch: 30 | Batch Status: 12000/49502 (24%) | Loss: 0.512997\n",
      "Train Epoch: 30 | Batch Status: 13000/49502 (26%) | Loss: 0.587542\n",
      "Train Epoch: 30 | Batch Status: 14000/49502 (28%) | Loss: 0.131829\n",
      "Train Epoch: 30 | Batch Status: 15000/49502 (30%) | Loss: 0.592480\n",
      "Train Epoch: 30 | Batch Status: 16000/49502 (32%) | Loss: 0.065852\n",
      "Train Epoch: 30 | Batch Status: 17000/49502 (34%) | Loss: 0.503254\n",
      "Train Epoch: 30 | Batch Status: 18000/49502 (36%) | Loss: 0.501390\n",
      "Train Epoch: 30 | Batch Status: 19000/49502 (38%) | Loss: 0.556091\n",
      "Train Epoch: 30 | Batch Status: 20000/49502 (40%) | Loss: 1.106268\n",
      "Train Epoch: 30 | Batch Status: 21000/49502 (42%) | Loss: 0.344820\n",
      "Train Epoch: 30 | Batch Status: 22000/49502 (44%) | Loss: 0.067128\n",
      "Train Epoch: 30 | Batch Status: 23000/49502 (46%) | Loss: 0.545644\n",
      "Train Epoch: 30 | Batch Status: 24000/49502 (48%) | Loss: 0.346064\n",
      "Train Epoch: 30 | Batch Status: 25000/49502 (50%) | Loss: 0.384560\n",
      "Train Epoch: 30 | Batch Status: 26000/49502 (53%) | Loss: 0.570280\n",
      "Train Epoch: 30 | Batch Status: 27000/49502 (55%) | Loss: 0.254578\n",
      "Train Epoch: 30 | Batch Status: 28000/49502 (57%) | Loss: 0.291284\n",
      "Train Epoch: 30 | Batch Status: 29000/49502 (59%) | Loss: 0.549517\n",
      "Train Epoch: 30 | Batch Status: 30000/49502 (61%) | Loss: 0.243789\n",
      "Train Epoch: 30 | Batch Status: 31000/49502 (63%) | Loss: 0.744402\n",
      "Train Epoch: 30 | Batch Status: 32000/49502 (65%) | Loss: 0.135939\n",
      "Train Epoch: 30 | Batch Status: 33000/49502 (67%) | Loss: 0.341308\n",
      "Train Epoch: 30 | Batch Status: 34000/49502 (69%) | Loss: 0.566085\n",
      "Train Epoch: 30 | Batch Status: 35000/49502 (71%) | Loss: 0.213589\n",
      "Train Epoch: 30 | Batch Status: 36000/49502 (73%) | Loss: 0.332993\n",
      "Train Epoch: 30 | Batch Status: 37000/49502 (75%) | Loss: 0.288039\n",
      "Train Epoch: 30 | Batch Status: 38000/49502 (77%) | Loss: 0.900211\n",
      "Train Epoch: 30 | Batch Status: 39000/49502 (79%) | Loss: 0.452383\n",
      "Train Epoch: 30 | Batch Status: 40000/49502 (81%) | Loss: 0.266197\n",
      "Train Epoch: 30 | Batch Status: 41000/49502 (83%) | Loss: 0.070240\n",
      "Train Epoch: 30 | Batch Status: 42000/49502 (85%) | Loss: 0.428192\n",
      "Train Epoch: 30 | Batch Status: 43000/49502 (87%) | Loss: 0.480828\n",
      "Train Epoch: 30 | Batch Status: 44000/49502 (89%) | Loss: 0.554822\n",
      "Train Epoch: 30 | Batch Status: 45000/49502 (91%) | Loss: 0.267657\n",
      "Train Epoch: 30 | Batch Status: 46000/49502 (93%) | Loss: 0.420859\n",
      "Train Epoch: 30 | Batch Status: 47000/49502 (95%) | Loss: 0.477948\n",
      "Train Epoch: 30 | Batch Status: 48000/49502 (97%) | Loss: 0.406269\n",
      "Train Epoch: 30 | Batch Status: 49000/49502 (99%) | Loss: 0.375550\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.1545, Accuracy: 5692/12376(45.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 31 | Batch Status: 0/49502 (0%) | Loss: 0.171050\n",
      "Train Epoch: 31 | Batch Status: 1000/49502 (2%) | Loss: 0.350513\n",
      "Train Epoch: 31 | Batch Status: 2000/49502 (4%) | Loss: 0.283444\n",
      "Train Epoch: 31 | Batch Status: 3000/49502 (6%) | Loss: 0.472302\n",
      "Train Epoch: 31 | Batch Status: 4000/49502 (8%) | Loss: 0.191657\n",
      "Train Epoch: 31 | Batch Status: 5000/49502 (10%) | Loss: 0.493636\n",
      "Train Epoch: 31 | Batch Status: 6000/49502 (12%) | Loss: 0.504031\n",
      "Train Epoch: 31 | Batch Status: 7000/49502 (14%) | Loss: 0.170271\n",
      "Train Epoch: 31 | Batch Status: 8000/49502 (16%) | Loss: 0.565220\n",
      "Train Epoch: 31 | Batch Status: 9000/49502 (18%) | Loss: 0.493762\n",
      "Train Epoch: 31 | Batch Status: 10000/49502 (20%) | Loss: 0.390042\n",
      "Train Epoch: 31 | Batch Status: 11000/49502 (22%) | Loss: 0.321790\n",
      "Train Epoch: 31 | Batch Status: 12000/49502 (24%) | Loss: 0.133681\n",
      "Train Epoch: 31 | Batch Status: 13000/49502 (26%) | Loss: 0.423738\n",
      "Train Epoch: 31 | Batch Status: 14000/49502 (28%) | Loss: 0.434737\n",
      "Train Epoch: 31 | Batch Status: 15000/49502 (30%) | Loss: 0.473105\n",
      "Train Epoch: 31 | Batch Status: 16000/49502 (32%) | Loss: 0.357214\n",
      "Train Epoch: 31 | Batch Status: 17000/49502 (34%) | Loss: 0.171989\n",
      "Train Epoch: 31 | Batch Status: 18000/49502 (36%) | Loss: 0.412436\n",
      "Train Epoch: 31 | Batch Status: 19000/49502 (38%) | Loss: 0.608806\n",
      "Train Epoch: 31 | Batch Status: 20000/49502 (40%) | Loss: 0.601473\n",
      "Train Epoch: 31 | Batch Status: 21000/49502 (42%) | Loss: 0.254558\n",
      "Train Epoch: 31 | Batch Status: 22000/49502 (44%) | Loss: 0.386453\n",
      "Train Epoch: 31 | Batch Status: 23000/49502 (46%) | Loss: 0.160378\n",
      "Train Epoch: 31 | Batch Status: 24000/49502 (48%) | Loss: 0.877229\n",
      "Train Epoch: 31 | Batch Status: 25000/49502 (50%) | Loss: 0.429853\n",
      "Train Epoch: 31 | Batch Status: 26000/49502 (53%) | Loss: 0.654187\n",
      "Train Epoch: 31 | Batch Status: 27000/49502 (55%) | Loss: 0.305764\n",
      "Train Epoch: 31 | Batch Status: 28000/49502 (57%) | Loss: 0.165658\n",
      "Train Epoch: 31 | Batch Status: 29000/49502 (59%) | Loss: 0.347330\n",
      "Train Epoch: 31 | Batch Status: 30000/49502 (61%) | Loss: 0.401340\n",
      "Train Epoch: 31 | Batch Status: 31000/49502 (63%) | Loss: 0.311336\n",
      "Train Epoch: 31 | Batch Status: 32000/49502 (65%) | Loss: 0.696346\n",
      "Train Epoch: 31 | Batch Status: 33000/49502 (67%) | Loss: 0.378853\n",
      "Train Epoch: 31 | Batch Status: 34000/49502 (69%) | Loss: 0.583982\n",
      "Train Epoch: 31 | Batch Status: 35000/49502 (71%) | Loss: 0.396806\n",
      "Train Epoch: 31 | Batch Status: 36000/49502 (73%) | Loss: 0.069774\n",
      "Train Epoch: 31 | Batch Status: 37000/49502 (75%) | Loss: 0.373085\n",
      "Train Epoch: 31 | Batch Status: 38000/49502 (77%) | Loss: 0.408395\n",
      "Train Epoch: 31 | Batch Status: 39000/49502 (79%) | Loss: 0.287729\n",
      "Train Epoch: 31 | Batch Status: 40000/49502 (81%) | Loss: 0.900443\n",
      "Train Epoch: 31 | Batch Status: 41000/49502 (83%) | Loss: 0.562442\n",
      "Train Epoch: 31 | Batch Status: 42000/49502 (85%) | Loss: 0.281002\n",
      "Train Epoch: 31 | Batch Status: 43000/49502 (87%) | Loss: 0.664486\n",
      "Train Epoch: 31 | Batch Status: 44000/49502 (89%) | Loss: 0.649084\n",
      "Train Epoch: 31 | Batch Status: 45000/49502 (91%) | Loss: 0.254268\n",
      "Train Epoch: 31 | Batch Status: 46000/49502 (93%) | Loss: 0.301149\n",
      "Train Epoch: 31 | Batch Status: 47000/49502 (95%) | Loss: 1.146231\n",
      "Train Epoch: 31 | Batch Status: 48000/49502 (97%) | Loss: 0.908063\n",
      "Train Epoch: 31 | Batch Status: 49000/49502 (99%) | Loss: 0.081862\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.1019, Accuracy: 5648/12376(45.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 32 | Batch Status: 0/49502 (0%) | Loss: 0.449366\n",
      "Train Epoch: 32 | Batch Status: 1000/49502 (2%) | Loss: 0.354376\n",
      "Train Epoch: 32 | Batch Status: 2000/49502 (4%) | Loss: 0.529074\n",
      "Train Epoch: 32 | Batch Status: 3000/49502 (6%) | Loss: 0.213685\n",
      "Train Epoch: 32 | Batch Status: 4000/49502 (8%) | Loss: 0.182839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 | Batch Status: 5000/49502 (10%) | Loss: 0.084096\n",
      "Train Epoch: 32 | Batch Status: 6000/49502 (12%) | Loss: 0.423778\n",
      "Train Epoch: 32 | Batch Status: 7000/49502 (14%) | Loss: 0.792632\n",
      "Train Epoch: 32 | Batch Status: 8000/49502 (16%) | Loss: 0.303757\n",
      "Train Epoch: 32 | Batch Status: 9000/49502 (18%) | Loss: 0.277460\n",
      "Train Epoch: 32 | Batch Status: 10000/49502 (20%) | Loss: 0.540428\n",
      "Train Epoch: 32 | Batch Status: 11000/49502 (22%) | Loss: 0.422580\n",
      "Train Epoch: 32 | Batch Status: 12000/49502 (24%) | Loss: 0.308313\n",
      "Train Epoch: 32 | Batch Status: 13000/49502 (26%) | Loss: 0.267342\n",
      "Train Epoch: 32 | Batch Status: 14000/49502 (28%) | Loss: 0.565219\n",
      "Train Epoch: 32 | Batch Status: 15000/49502 (30%) | Loss: 0.404840\n",
      "Train Epoch: 32 | Batch Status: 16000/49502 (32%) | Loss: 0.164359\n",
      "Train Epoch: 32 | Batch Status: 17000/49502 (34%) | Loss: 0.394174\n",
      "Train Epoch: 32 | Batch Status: 18000/49502 (36%) | Loss: 0.712290\n",
      "Train Epoch: 32 | Batch Status: 19000/49502 (38%) | Loss: 0.392541\n",
      "Train Epoch: 32 | Batch Status: 20000/49502 (40%) | Loss: 0.696526\n",
      "Train Epoch: 32 | Batch Status: 21000/49502 (42%) | Loss: 0.507048\n",
      "Train Epoch: 32 | Batch Status: 22000/49502 (44%) | Loss: 0.678889\n",
      "Train Epoch: 32 | Batch Status: 23000/49502 (46%) | Loss: 0.718630\n",
      "Train Epoch: 32 | Batch Status: 24000/49502 (48%) | Loss: 0.580696\n",
      "Train Epoch: 32 | Batch Status: 25000/49502 (50%) | Loss: 0.190283\n",
      "Train Epoch: 32 | Batch Status: 26000/49502 (53%) | Loss: 0.199924\n",
      "Train Epoch: 32 | Batch Status: 27000/49502 (55%) | Loss: 0.353885\n",
      "Train Epoch: 32 | Batch Status: 28000/49502 (57%) | Loss: 0.654933\n",
      "Train Epoch: 32 | Batch Status: 29000/49502 (59%) | Loss: 0.183025\n",
      "Train Epoch: 32 | Batch Status: 30000/49502 (61%) | Loss: 0.099907\n",
      "Train Epoch: 32 | Batch Status: 31000/49502 (63%) | Loss: 0.259950\n",
      "Train Epoch: 32 | Batch Status: 32000/49502 (65%) | Loss: 0.438311\n",
      "Train Epoch: 32 | Batch Status: 33000/49502 (67%) | Loss: 0.279231\n",
      "Train Epoch: 32 | Batch Status: 34000/49502 (69%) | Loss: 0.524081\n",
      "Train Epoch: 32 | Batch Status: 35000/49502 (71%) | Loss: 0.792498\n",
      "Train Epoch: 32 | Batch Status: 36000/49502 (73%) | Loss: 0.354700\n",
      "Train Epoch: 32 | Batch Status: 37000/49502 (75%) | Loss: 0.474943\n",
      "Train Epoch: 32 | Batch Status: 38000/49502 (77%) | Loss: 0.227663\n",
      "Train Epoch: 32 | Batch Status: 39000/49502 (79%) | Loss: 0.558419\n",
      "Train Epoch: 32 | Batch Status: 40000/49502 (81%) | Loss: 0.488813\n",
      "Train Epoch: 32 | Batch Status: 41000/49502 (83%) | Loss: 0.375090\n",
      "Train Epoch: 32 | Batch Status: 42000/49502 (85%) | Loss: 0.367918\n",
      "Train Epoch: 32 | Batch Status: 43000/49502 (87%) | Loss: 0.234146\n",
      "Train Epoch: 32 | Batch Status: 44000/49502 (89%) | Loss: 0.882284\n",
      "Train Epoch: 32 | Batch Status: 45000/49502 (91%) | Loss: 0.554866\n",
      "Train Epoch: 32 | Batch Status: 46000/49502 (93%) | Loss: 0.357529\n",
      "Train Epoch: 32 | Batch Status: 47000/49502 (95%) | Loss: 0.393727\n",
      "Train Epoch: 32 | Batch Status: 48000/49502 (97%) | Loss: 0.418250\n",
      "Train Epoch: 32 | Batch Status: 49000/49502 (99%) | Loss: 0.510967\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.2404, Accuracy: 5510/12376(44.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 33 | Batch Status: 0/49502 (0%) | Loss: 0.688622\n",
      "Train Epoch: 33 | Batch Status: 1000/49502 (2%) | Loss: 0.531797\n",
      "Train Epoch: 33 | Batch Status: 2000/49502 (4%) | Loss: 0.124168\n",
      "Train Epoch: 33 | Batch Status: 3000/49502 (6%) | Loss: 0.177193\n",
      "Train Epoch: 33 | Batch Status: 4000/49502 (8%) | Loss: 0.270196\n",
      "Train Epoch: 33 | Batch Status: 5000/49502 (10%) | Loss: 0.566400\n",
      "Train Epoch: 33 | Batch Status: 6000/49502 (12%) | Loss: 0.363242\n",
      "Train Epoch: 33 | Batch Status: 7000/49502 (14%) | Loss: 0.292678\n",
      "Train Epoch: 33 | Batch Status: 8000/49502 (16%) | Loss: 0.107295\n",
      "Train Epoch: 33 | Batch Status: 9000/49502 (18%) | Loss: 0.108901\n",
      "Train Epoch: 33 | Batch Status: 10000/49502 (20%) | Loss: 0.296665\n",
      "Train Epoch: 33 | Batch Status: 11000/49502 (22%) | Loss: 0.238645\n",
      "Train Epoch: 33 | Batch Status: 12000/49502 (24%) | Loss: 0.097845\n",
      "Train Epoch: 33 | Batch Status: 13000/49502 (26%) | Loss: 0.290652\n",
      "Train Epoch: 33 | Batch Status: 14000/49502 (28%) | Loss: 1.077078\n",
      "Train Epoch: 33 | Batch Status: 15000/49502 (30%) | Loss: 0.681025\n",
      "Train Epoch: 33 | Batch Status: 16000/49502 (32%) | Loss: 0.453503\n",
      "Train Epoch: 33 | Batch Status: 17000/49502 (34%) | Loss: 0.112116\n",
      "Train Epoch: 33 | Batch Status: 18000/49502 (36%) | Loss: 0.510432\n",
      "Train Epoch: 33 | Batch Status: 19000/49502 (38%) | Loss: 0.413543\n",
      "Train Epoch: 33 | Batch Status: 20000/49502 (40%) | Loss: 0.272085\n",
      "Train Epoch: 33 | Batch Status: 21000/49502 (42%) | Loss: 0.452976\n",
      "Train Epoch: 33 | Batch Status: 22000/49502 (44%) | Loss: 0.278574\n",
      "Train Epoch: 33 | Batch Status: 23000/49502 (46%) | Loss: 0.285024\n",
      "Train Epoch: 33 | Batch Status: 24000/49502 (48%) | Loss: 0.298137\n",
      "Train Epoch: 33 | Batch Status: 25000/49502 (50%) | Loss: 0.978164\n",
      "Train Epoch: 33 | Batch Status: 26000/49502 (53%) | Loss: 0.208519\n",
      "Train Epoch: 33 | Batch Status: 27000/49502 (55%) | Loss: 0.229840\n",
      "Train Epoch: 33 | Batch Status: 28000/49502 (57%) | Loss: 0.266294\n",
      "Train Epoch: 33 | Batch Status: 29000/49502 (59%) | Loss: 1.123299\n",
      "Train Epoch: 33 | Batch Status: 30000/49502 (61%) | Loss: 0.176111\n",
      "Train Epoch: 33 | Batch Status: 31000/49502 (63%) | Loss: 0.870142\n",
      "Train Epoch: 33 | Batch Status: 32000/49502 (65%) | Loss: 0.223370\n",
      "Train Epoch: 33 | Batch Status: 33000/49502 (67%) | Loss: 0.230771\n",
      "Train Epoch: 33 | Batch Status: 34000/49502 (69%) | Loss: 0.153525\n",
      "Train Epoch: 33 | Batch Status: 35000/49502 (71%) | Loss: 0.508536\n",
      "Train Epoch: 33 | Batch Status: 36000/49502 (73%) | Loss: 0.455829\n",
      "Train Epoch: 33 | Batch Status: 37000/49502 (75%) | Loss: 0.499682\n",
      "Train Epoch: 33 | Batch Status: 38000/49502 (77%) | Loss: 1.002312\n",
      "Train Epoch: 33 | Batch Status: 39000/49502 (79%) | Loss: 0.487126\n",
      "Train Epoch: 33 | Batch Status: 40000/49502 (81%) | Loss: 0.706630\n",
      "Train Epoch: 33 | Batch Status: 41000/49502 (83%) | Loss: 0.353004\n",
      "Train Epoch: 33 | Batch Status: 42000/49502 (85%) | Loss: 1.161207\n",
      "Train Epoch: 33 | Batch Status: 43000/49502 (87%) | Loss: 0.282218\n",
      "Train Epoch: 33 | Batch Status: 44000/49502 (89%) | Loss: 0.842297\n",
      "Train Epoch: 33 | Batch Status: 45000/49502 (91%) | Loss: 0.655548\n",
      "Train Epoch: 33 | Batch Status: 46000/49502 (93%) | Loss: 0.394300\n",
      "Train Epoch: 33 | Batch Status: 47000/49502 (95%) | Loss: 0.326398\n",
      "Train Epoch: 33 | Batch Status: 48000/49502 (97%) | Loss: 0.060070\n",
      "Train Epoch: 33 | Batch Status: 49000/49502 (99%) | Loss: 0.627287\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.1285, Accuracy: 6032/12376(48.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 34 | Batch Status: 0/49502 (0%) | Loss: 0.544967\n",
      "Train Epoch: 34 | Batch Status: 1000/49502 (2%) | Loss: 0.308024\n",
      "Train Epoch: 34 | Batch Status: 2000/49502 (4%) | Loss: 0.116479\n",
      "Train Epoch: 34 | Batch Status: 3000/49502 (6%) | Loss: 0.283871\n",
      "Train Epoch: 34 | Batch Status: 4000/49502 (8%) | Loss: 0.722888\n",
      "Train Epoch: 34 | Batch Status: 5000/49502 (10%) | Loss: 0.398791\n",
      "Train Epoch: 34 | Batch Status: 6000/49502 (12%) | Loss: 0.991596\n",
      "Train Epoch: 34 | Batch Status: 7000/49502 (14%) | Loss: 0.351751\n",
      "Train Epoch: 34 | Batch Status: 8000/49502 (16%) | Loss: 0.317632\n",
      "Train Epoch: 34 | Batch Status: 9000/49502 (18%) | Loss: 0.485237\n",
      "Train Epoch: 34 | Batch Status: 10000/49502 (20%) | Loss: 0.643137\n",
      "Train Epoch: 34 | Batch Status: 11000/49502 (22%) | Loss: 0.389553\n",
      "Train Epoch: 34 | Batch Status: 12000/49502 (24%) | Loss: 0.580612\n",
      "Train Epoch: 34 | Batch Status: 13000/49502 (26%) | Loss: 0.238306\n",
      "Train Epoch: 34 | Batch Status: 14000/49502 (28%) | Loss: 0.407622\n",
      "Train Epoch: 34 | Batch Status: 15000/49502 (30%) | Loss: 0.725042\n",
      "Train Epoch: 34 | Batch Status: 16000/49502 (32%) | Loss: 0.051266\n",
      "Train Epoch: 34 | Batch Status: 17000/49502 (34%) | Loss: 0.689556\n",
      "Train Epoch: 34 | Batch Status: 18000/49502 (36%) | Loss: 0.261375\n",
      "Train Epoch: 34 | Batch Status: 19000/49502 (38%) | Loss: 0.547685\n",
      "Train Epoch: 34 | Batch Status: 20000/49502 (40%) | Loss: 0.701433\n",
      "Train Epoch: 34 | Batch Status: 21000/49502 (42%) | Loss: 0.586195\n",
      "Train Epoch: 34 | Batch Status: 22000/49502 (44%) | Loss: 1.436077\n",
      "Train Epoch: 34 | Batch Status: 23000/49502 (46%) | Loss: 0.512727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 34 | Batch Status: 24000/49502 (48%) | Loss: 0.348632\n",
      "Train Epoch: 34 | Batch Status: 25000/49502 (50%) | Loss: 0.162268\n",
      "Train Epoch: 34 | Batch Status: 26000/49502 (53%) | Loss: 0.342631\n",
      "Train Epoch: 34 | Batch Status: 27000/49502 (55%) | Loss: 0.398056\n",
      "Train Epoch: 34 | Batch Status: 28000/49502 (57%) | Loss: 0.446560\n",
      "Train Epoch: 34 | Batch Status: 29000/49502 (59%) | Loss: 0.488503\n",
      "Train Epoch: 34 | Batch Status: 30000/49502 (61%) | Loss: 0.362074\n",
      "Train Epoch: 34 | Batch Status: 31000/49502 (63%) | Loss: 0.256339\n",
      "Train Epoch: 34 | Batch Status: 32000/49502 (65%) | Loss: 0.785552\n",
      "Train Epoch: 34 | Batch Status: 33000/49502 (67%) | Loss: 1.181145\n",
      "Train Epoch: 34 | Batch Status: 34000/49502 (69%) | Loss: 0.399205\n",
      "Train Epoch: 34 | Batch Status: 35000/49502 (71%) | Loss: 0.505998\n",
      "Train Epoch: 34 | Batch Status: 36000/49502 (73%) | Loss: 0.334033\n",
      "Train Epoch: 34 | Batch Status: 37000/49502 (75%) | Loss: 0.801980\n",
      "Train Epoch: 34 | Batch Status: 38000/49502 (77%) | Loss: 0.426285\n",
      "Train Epoch: 34 | Batch Status: 39000/49502 (79%) | Loss: 0.444653\n",
      "Train Epoch: 34 | Batch Status: 40000/49502 (81%) | Loss: 0.561404\n",
      "Train Epoch: 34 | Batch Status: 41000/49502 (83%) | Loss: 0.738975\n",
      "Train Epoch: 34 | Batch Status: 42000/49502 (85%) | Loss: 0.417636\n",
      "Train Epoch: 34 | Batch Status: 43000/49502 (87%) | Loss: 0.511581\n",
      "Train Epoch: 34 | Batch Status: 44000/49502 (89%) | Loss: 0.095981\n",
      "Train Epoch: 34 | Batch Status: 45000/49502 (91%) | Loss: 0.399403\n",
      "Train Epoch: 34 | Batch Status: 46000/49502 (93%) | Loss: 0.399625\n",
      "Train Epoch: 34 | Batch Status: 47000/49502 (95%) | Loss: 0.285722\n",
      "Train Epoch: 34 | Batch Status: 48000/49502 (97%) | Loss: 1.043572\n",
      "Train Epoch: 34 | Batch Status: 49000/49502 (99%) | Loss: 0.214631\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.2776, Accuracy: 4744/12376(38.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 35 | Batch Status: 0/49502 (0%) | Loss: 0.435561\n",
      "Train Epoch: 35 | Batch Status: 1000/49502 (2%) | Loss: 0.589241\n",
      "Train Epoch: 35 | Batch Status: 2000/49502 (4%) | Loss: 0.288998\n",
      "Train Epoch: 35 | Batch Status: 3000/49502 (6%) | Loss: 0.257895\n",
      "Train Epoch: 35 | Batch Status: 4000/49502 (8%) | Loss: 0.339451\n",
      "Train Epoch: 35 | Batch Status: 5000/49502 (10%) | Loss: 0.973753\n",
      "Train Epoch: 35 | Batch Status: 6000/49502 (12%) | Loss: 0.816702\n",
      "Train Epoch: 35 | Batch Status: 7000/49502 (14%) | Loss: 0.452693\n",
      "Train Epoch: 35 | Batch Status: 8000/49502 (16%) | Loss: 0.409204\n",
      "Train Epoch: 35 | Batch Status: 9000/49502 (18%) | Loss: 0.189175\n",
      "Train Epoch: 35 | Batch Status: 10000/49502 (20%) | Loss: 0.424808\n",
      "Train Epoch: 35 | Batch Status: 11000/49502 (22%) | Loss: 0.397302\n",
      "Train Epoch: 35 | Batch Status: 12000/49502 (24%) | Loss: 0.914923\n",
      "Train Epoch: 35 | Batch Status: 13000/49502 (26%) | Loss: 0.194362\n",
      "Train Epoch: 35 | Batch Status: 14000/49502 (28%) | Loss: 0.574006\n",
      "Train Epoch: 35 | Batch Status: 15000/49502 (30%) | Loss: 0.409917\n",
      "Train Epoch: 35 | Batch Status: 16000/49502 (32%) | Loss: 0.482511\n",
      "Train Epoch: 35 | Batch Status: 17000/49502 (34%) | Loss: 0.884977\n",
      "Train Epoch: 35 | Batch Status: 18000/49502 (36%) | Loss: 0.157223\n",
      "Train Epoch: 35 | Batch Status: 19000/49502 (38%) | Loss: 0.771652\n",
      "Train Epoch: 35 | Batch Status: 20000/49502 (40%) | Loss: 0.119154\n",
      "Train Epoch: 35 | Batch Status: 21000/49502 (42%) | Loss: 0.438229\n",
      "Train Epoch: 35 | Batch Status: 22000/49502 (44%) | Loss: 0.456661\n",
      "Train Epoch: 35 | Batch Status: 23000/49502 (46%) | Loss: 0.522449\n",
      "Train Epoch: 35 | Batch Status: 24000/49502 (48%) | Loss: 0.372614\n",
      "Train Epoch: 35 | Batch Status: 25000/49502 (50%) | Loss: 0.304187\n",
      "Train Epoch: 35 | Batch Status: 26000/49502 (53%) | Loss: 0.846173\n",
      "Train Epoch: 35 | Batch Status: 27000/49502 (55%) | Loss: 0.822244\n",
      "Train Epoch: 35 | Batch Status: 28000/49502 (57%) | Loss: 0.187600\n",
      "Train Epoch: 35 | Batch Status: 29000/49502 (59%) | Loss: 0.216071\n",
      "Train Epoch: 35 | Batch Status: 30000/49502 (61%) | Loss: 0.407308\n",
      "Train Epoch: 35 | Batch Status: 31000/49502 (63%) | Loss: 0.248530\n",
      "Train Epoch: 35 | Batch Status: 32000/49502 (65%) | Loss: 0.174167\n",
      "Train Epoch: 35 | Batch Status: 33000/49502 (67%) | Loss: 0.478344\n",
      "Train Epoch: 35 | Batch Status: 34000/49502 (69%) | Loss: 0.598720\n",
      "Train Epoch: 35 | Batch Status: 35000/49502 (71%) | Loss: 0.194082\n",
      "Train Epoch: 35 | Batch Status: 36000/49502 (73%) | Loss: 0.465396\n",
      "Train Epoch: 35 | Batch Status: 37000/49502 (75%) | Loss: 0.582148\n",
      "Train Epoch: 35 | Batch Status: 38000/49502 (77%) | Loss: 0.193240\n",
      "Train Epoch: 35 | Batch Status: 39000/49502 (79%) | Loss: 0.622428\n",
      "Train Epoch: 35 | Batch Status: 40000/49502 (81%) | Loss: 0.525683\n",
      "Train Epoch: 35 | Batch Status: 41000/49502 (83%) | Loss: 0.461979\n",
      "Train Epoch: 35 | Batch Status: 42000/49502 (85%) | Loss: 0.207101\n",
      "Train Epoch: 35 | Batch Status: 43000/49502 (87%) | Loss: 0.271416\n",
      "Train Epoch: 35 | Batch Status: 44000/49502 (89%) | Loss: 0.582830\n",
      "Train Epoch: 35 | Batch Status: 45000/49502 (91%) | Loss: 0.539170\n",
      "Train Epoch: 35 | Batch Status: 46000/49502 (93%) | Loss: 0.283327\n",
      "Train Epoch: 35 | Batch Status: 47000/49502 (95%) | Loss: 0.224978\n",
      "Train Epoch: 35 | Batch Status: 48000/49502 (97%) | Loss: 0.532174\n",
      "Train Epoch: 35 | Batch Status: 49000/49502 (99%) | Loss: 0.350865\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.2059, Accuracy: 5900/12376(47.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 36 | Batch Status: 0/49502 (0%) | Loss: 0.439804\n",
      "Train Epoch: 36 | Batch Status: 1000/49502 (2%) | Loss: 0.408271\n",
      "Train Epoch: 36 | Batch Status: 2000/49502 (4%) | Loss: 0.370448\n",
      "Train Epoch: 36 | Batch Status: 3000/49502 (6%) | Loss: 0.425408\n",
      "Train Epoch: 36 | Batch Status: 4000/49502 (8%) | Loss: 0.147685\n",
      "Train Epoch: 36 | Batch Status: 5000/49502 (10%) | Loss: 0.606746\n",
      "Train Epoch: 36 | Batch Status: 6000/49502 (12%) | Loss: 0.406275\n",
      "Train Epoch: 36 | Batch Status: 7000/49502 (14%) | Loss: 0.850749\n",
      "Train Epoch: 36 | Batch Status: 8000/49502 (16%) | Loss: 0.349469\n",
      "Train Epoch: 36 | Batch Status: 9000/49502 (18%) | Loss: 0.194819\n",
      "Train Epoch: 36 | Batch Status: 10000/49502 (20%) | Loss: 0.103439\n",
      "Train Epoch: 36 | Batch Status: 11000/49502 (22%) | Loss: 0.485030\n",
      "Train Epoch: 36 | Batch Status: 12000/49502 (24%) | Loss: 0.223804\n",
      "Train Epoch: 36 | Batch Status: 13000/49502 (26%) | Loss: 0.987092\n",
      "Train Epoch: 36 | Batch Status: 14000/49502 (28%) | Loss: 0.242769\n",
      "Train Epoch: 36 | Batch Status: 15000/49502 (30%) | Loss: 0.183377\n",
      "Train Epoch: 36 | Batch Status: 16000/49502 (32%) | Loss: 0.183484\n",
      "Train Epoch: 36 | Batch Status: 17000/49502 (34%) | Loss: 0.282538\n",
      "Train Epoch: 36 | Batch Status: 18000/49502 (36%) | Loss: 0.191711\n",
      "Train Epoch: 36 | Batch Status: 19000/49502 (38%) | Loss: 0.506122\n",
      "Train Epoch: 36 | Batch Status: 20000/49502 (40%) | Loss: 0.177380\n",
      "Train Epoch: 36 | Batch Status: 21000/49502 (42%) | Loss: 0.386641\n",
      "Train Epoch: 36 | Batch Status: 22000/49502 (44%) | Loss: 0.450321\n",
      "Train Epoch: 36 | Batch Status: 23000/49502 (46%) | Loss: 0.863266\n",
      "Train Epoch: 36 | Batch Status: 24000/49502 (48%) | Loss: 0.332504\n",
      "Train Epoch: 36 | Batch Status: 25000/49502 (50%) | Loss: 0.562517\n",
      "Train Epoch: 36 | Batch Status: 26000/49502 (53%) | Loss: 0.773615\n",
      "Train Epoch: 36 | Batch Status: 27000/49502 (55%) | Loss: 0.590330\n",
      "Train Epoch: 36 | Batch Status: 28000/49502 (57%) | Loss: 0.471638\n",
      "Train Epoch: 36 | Batch Status: 29000/49502 (59%) | Loss: 0.278928\n",
      "Train Epoch: 36 | Batch Status: 30000/49502 (61%) | Loss: 0.285121\n",
      "Train Epoch: 36 | Batch Status: 31000/49502 (63%) | Loss: 0.661919\n",
      "Train Epoch: 36 | Batch Status: 32000/49502 (65%) | Loss: 1.161676\n",
      "Train Epoch: 36 | Batch Status: 33000/49502 (67%) | Loss: 0.310149\n",
      "Train Epoch: 36 | Batch Status: 34000/49502 (69%) | Loss: 0.348262\n",
      "Train Epoch: 36 | Batch Status: 35000/49502 (71%) | Loss: 0.243140\n",
      "Train Epoch: 36 | Batch Status: 36000/49502 (73%) | Loss: 0.298003\n",
      "Train Epoch: 36 | Batch Status: 37000/49502 (75%) | Loss: 1.038350\n",
      "Train Epoch: 36 | Batch Status: 38000/49502 (77%) | Loss: 0.375003\n",
      "Train Epoch: 36 | Batch Status: 39000/49502 (79%) | Loss: 0.160168\n",
      "Train Epoch: 36 | Batch Status: 40000/49502 (81%) | Loss: 0.233728\n",
      "Train Epoch: 36 | Batch Status: 41000/49502 (83%) | Loss: 0.458121\n",
      "Train Epoch: 36 | Batch Status: 42000/49502 (85%) | Loss: 0.369699\n",
      "Train Epoch: 36 | Batch Status: 43000/49502 (87%) | Loss: 0.126190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 36 | Batch Status: 44000/49502 (89%) | Loss: 0.590499\n",
      "Train Epoch: 36 | Batch Status: 45000/49502 (91%) | Loss: 0.381276\n",
      "Train Epoch: 36 | Batch Status: 46000/49502 (93%) | Loss: 0.386437\n",
      "Train Epoch: 36 | Batch Status: 47000/49502 (95%) | Loss: 0.722749\n",
      "Train Epoch: 36 | Batch Status: 48000/49502 (97%) | Loss: 0.268615\n",
      "Train Epoch: 36 | Batch Status: 49000/49502 (99%) | Loss: 0.235006\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.3024, Accuracy: 5502/12376(44.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 37 | Batch Status: 0/49502 (0%) | Loss: 0.766384\n",
      "Train Epoch: 37 | Batch Status: 1000/49502 (2%) | Loss: 0.224418\n",
      "Train Epoch: 37 | Batch Status: 2000/49502 (4%) | Loss: 0.567028\n",
      "Train Epoch: 37 | Batch Status: 3000/49502 (6%) | Loss: 0.365241\n",
      "Train Epoch: 37 | Batch Status: 4000/49502 (8%) | Loss: 0.388843\n",
      "Train Epoch: 37 | Batch Status: 5000/49502 (10%) | Loss: 0.309183\n",
      "Train Epoch: 37 | Batch Status: 6000/49502 (12%) | Loss: 0.378056\n",
      "Train Epoch: 37 | Batch Status: 7000/49502 (14%) | Loss: 0.129350\n",
      "Train Epoch: 37 | Batch Status: 8000/49502 (16%) | Loss: 0.308749\n",
      "Train Epoch: 37 | Batch Status: 9000/49502 (18%) | Loss: 0.275139\n",
      "Train Epoch: 37 | Batch Status: 10000/49502 (20%) | Loss: 0.389739\n",
      "Train Epoch: 37 | Batch Status: 11000/49502 (22%) | Loss: 0.312680\n",
      "Train Epoch: 37 | Batch Status: 12000/49502 (24%) | Loss: 0.314815\n",
      "Train Epoch: 37 | Batch Status: 13000/49502 (26%) | Loss: 0.637223\n",
      "Train Epoch: 37 | Batch Status: 14000/49502 (28%) | Loss: 0.203315\n",
      "Train Epoch: 37 | Batch Status: 15000/49502 (30%) | Loss: 0.812618\n",
      "Train Epoch: 37 | Batch Status: 16000/49502 (32%) | Loss: 0.543386\n",
      "Train Epoch: 37 | Batch Status: 17000/49502 (34%) | Loss: 0.334955\n",
      "Train Epoch: 37 | Batch Status: 18000/49502 (36%) | Loss: 0.318155\n",
      "Train Epoch: 37 | Batch Status: 19000/49502 (38%) | Loss: 0.423151\n",
      "Train Epoch: 37 | Batch Status: 20000/49502 (40%) | Loss: 0.255933\n",
      "Train Epoch: 37 | Batch Status: 21000/49502 (42%) | Loss: 0.439235\n",
      "Train Epoch: 37 | Batch Status: 22000/49502 (44%) | Loss: 0.637716\n",
      "Train Epoch: 37 | Batch Status: 23000/49502 (46%) | Loss: 0.427168\n",
      "Train Epoch: 37 | Batch Status: 24000/49502 (48%) | Loss: 0.203817\n",
      "Train Epoch: 37 | Batch Status: 25000/49502 (50%) | Loss: 0.317041\n",
      "Train Epoch: 37 | Batch Status: 26000/49502 (53%) | Loss: 0.687058\n",
      "Train Epoch: 37 | Batch Status: 27000/49502 (55%) | Loss: 0.276247\n",
      "Train Epoch: 37 | Batch Status: 28000/49502 (57%) | Loss: 0.416901\n",
      "Train Epoch: 37 | Batch Status: 29000/49502 (59%) | Loss: 0.309068\n",
      "Train Epoch: 37 | Batch Status: 30000/49502 (61%) | Loss: 0.534403\n",
      "Train Epoch: 37 | Batch Status: 31000/49502 (63%) | Loss: 0.082416\n",
      "Train Epoch: 37 | Batch Status: 32000/49502 (65%) | Loss: 0.323985\n",
      "Train Epoch: 37 | Batch Status: 33000/49502 (67%) | Loss: 0.165244\n",
      "Train Epoch: 37 | Batch Status: 34000/49502 (69%) | Loss: 0.462464\n",
      "Train Epoch: 37 | Batch Status: 35000/49502 (71%) | Loss: 0.353378\n",
      "Train Epoch: 37 | Batch Status: 36000/49502 (73%) | Loss: 0.759778\n",
      "Train Epoch: 37 | Batch Status: 37000/49502 (75%) | Loss: 0.359839\n",
      "Train Epoch: 37 | Batch Status: 38000/49502 (77%) | Loss: 0.406012\n",
      "Train Epoch: 37 | Batch Status: 39000/49502 (79%) | Loss: 0.894725\n",
      "Train Epoch: 37 | Batch Status: 40000/49502 (81%) | Loss: 0.240717\n",
      "Train Epoch: 37 | Batch Status: 41000/49502 (83%) | Loss: 0.523883\n",
      "Train Epoch: 37 | Batch Status: 42000/49502 (85%) | Loss: 0.324889\n",
      "Train Epoch: 37 | Batch Status: 43000/49502 (87%) | Loss: 0.250409\n",
      "Train Epoch: 37 | Batch Status: 44000/49502 (89%) | Loss: 0.814038\n",
      "Train Epoch: 37 | Batch Status: 45000/49502 (91%) | Loss: 0.197318\n",
      "Train Epoch: 37 | Batch Status: 46000/49502 (93%) | Loss: 0.352824\n",
      "Train Epoch: 37 | Batch Status: 47000/49502 (95%) | Loss: 0.162848\n",
      "Train Epoch: 37 | Batch Status: 48000/49502 (97%) | Loss: 0.541069\n",
      "Train Epoch: 37 | Batch Status: 49000/49502 (99%) | Loss: 0.232360\n",
      "Training time: 0m 5s\n",
      "==========================\n",
      "Test set: Average loss: 1.3991, Accuracy: 5623/12376(45.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 38 | Batch Status: 0/49502 (0%) | Loss: 0.219728\n",
      "Train Epoch: 38 | Batch Status: 1000/49502 (2%) | Loss: 0.347250\n",
      "Train Epoch: 38 | Batch Status: 2000/49502 (4%) | Loss: 0.448045\n",
      "Train Epoch: 38 | Batch Status: 3000/49502 (6%) | Loss: 0.230223\n",
      "Train Epoch: 38 | Batch Status: 4000/49502 (8%) | Loss: 0.166941\n",
      "Train Epoch: 38 | Batch Status: 5000/49502 (10%) | Loss: 0.099025\n",
      "Train Epoch: 38 | Batch Status: 6000/49502 (12%) | Loss: 0.395096\n",
      "Train Epoch: 38 | Batch Status: 7000/49502 (14%) | Loss: 0.589718\n",
      "Train Epoch: 38 | Batch Status: 8000/49502 (16%) | Loss: 0.355257\n",
      "Train Epoch: 38 | Batch Status: 9000/49502 (18%) | Loss: 0.348826\n",
      "Train Epoch: 38 | Batch Status: 10000/49502 (20%) | Loss: 0.520238\n",
      "Train Epoch: 38 | Batch Status: 11000/49502 (22%) | Loss: 0.176953\n",
      "Train Epoch: 38 | Batch Status: 12000/49502 (24%) | Loss: 0.183338\n",
      "Train Epoch: 38 | Batch Status: 13000/49502 (26%) | Loss: 0.371595\n",
      "Train Epoch: 38 | Batch Status: 14000/49502 (28%) | Loss: 0.100363\n",
      "Train Epoch: 38 | Batch Status: 15000/49502 (30%) | Loss: 0.427301\n",
      "Train Epoch: 38 | Batch Status: 16000/49502 (32%) | Loss: 0.123120\n",
      "Train Epoch: 38 | Batch Status: 17000/49502 (34%) | Loss: 0.475075\n",
      "Train Epoch: 38 | Batch Status: 18000/49502 (36%) | Loss: 0.545600\n",
      "Train Epoch: 38 | Batch Status: 19000/49502 (38%) | Loss: 0.511838\n",
      "Train Epoch: 38 | Batch Status: 20000/49502 (40%) | Loss: 0.523762\n",
      "Train Epoch: 38 | Batch Status: 21000/49502 (42%) | Loss: 0.334888\n",
      "Train Epoch: 38 | Batch Status: 22000/49502 (44%) | Loss: 0.283152\n",
      "Train Epoch: 38 | Batch Status: 23000/49502 (46%) | Loss: 0.522897\n",
      "Train Epoch: 38 | Batch Status: 24000/49502 (48%) | Loss: 0.217501\n",
      "Train Epoch: 38 | Batch Status: 25000/49502 (50%) | Loss: 0.225535\n",
      "Train Epoch: 38 | Batch Status: 26000/49502 (53%) | Loss: 0.142348\n",
      "Train Epoch: 38 | Batch Status: 27000/49502 (55%) | Loss: 0.142464\n",
      "Train Epoch: 38 | Batch Status: 28000/49502 (57%) | Loss: 0.246581\n",
      "Train Epoch: 38 | Batch Status: 29000/49502 (59%) | Loss: 0.112770\n",
      "Train Epoch: 38 | Batch Status: 30000/49502 (61%) | Loss: 0.850793\n",
      "Train Epoch: 38 | Batch Status: 31000/49502 (63%) | Loss: 0.236690\n",
      "Train Epoch: 38 | Batch Status: 32000/49502 (65%) | Loss: 0.231167\n",
      "Train Epoch: 38 | Batch Status: 33000/49502 (67%) | Loss: 0.186249\n",
      "Train Epoch: 38 | Batch Status: 34000/49502 (69%) | Loss: 0.342898\n",
      "Train Epoch: 38 | Batch Status: 35000/49502 (71%) | Loss: 0.933268\n",
      "Train Epoch: 38 | Batch Status: 36000/49502 (73%) | Loss: 0.133078\n",
      "Train Epoch: 38 | Batch Status: 37000/49502 (75%) | Loss: 0.062539\n",
      "Train Epoch: 38 | Batch Status: 38000/49502 (77%) | Loss: 0.382949\n",
      "Train Epoch: 38 | Batch Status: 39000/49502 (79%) | Loss: 0.211167\n",
      "Train Epoch: 38 | Batch Status: 40000/49502 (81%) | Loss: 0.091040\n",
      "Train Epoch: 38 | Batch Status: 41000/49502 (83%) | Loss: 0.141042\n",
      "Train Epoch: 38 | Batch Status: 42000/49502 (85%) | Loss: 0.340073\n",
      "Train Epoch: 38 | Batch Status: 43000/49502 (87%) | Loss: 0.194462\n",
      "Train Epoch: 38 | Batch Status: 44000/49502 (89%) | Loss: 0.744102\n",
      "Train Epoch: 38 | Batch Status: 45000/49502 (91%) | Loss: 0.627965\n",
      "Train Epoch: 38 | Batch Status: 46000/49502 (93%) | Loss: 0.366986\n",
      "Train Epoch: 38 | Batch Status: 47000/49502 (95%) | Loss: 0.254156\n",
      "Train Epoch: 38 | Batch Status: 48000/49502 (97%) | Loss: 0.277928\n",
      "Train Epoch: 38 | Batch Status: 49000/49502 (99%) | Loss: 0.317073\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.1427, Accuracy: 5503/12376(44.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 39 | Batch Status: 0/49502 (0%) | Loss: 0.357818\n",
      "Train Epoch: 39 | Batch Status: 1000/49502 (2%) | Loss: 0.296340\n",
      "Train Epoch: 39 | Batch Status: 2000/49502 (4%) | Loss: 0.507687\n",
      "Train Epoch: 39 | Batch Status: 3000/49502 (6%) | Loss: 0.530108\n",
      "Train Epoch: 39 | Batch Status: 4000/49502 (8%) | Loss: 0.516911\n",
      "Train Epoch: 39 | Batch Status: 5000/49502 (10%) | Loss: 0.452495\n",
      "Train Epoch: 39 | Batch Status: 6000/49502 (12%) | Loss: 0.194452\n",
      "Train Epoch: 39 | Batch Status: 7000/49502 (14%) | Loss: 0.181999\n",
      "Train Epoch: 39 | Batch Status: 8000/49502 (16%) | Loss: 0.379979\n",
      "Train Epoch: 39 | Batch Status: 9000/49502 (18%) | Loss: 0.438890\n",
      "Train Epoch: 39 | Batch Status: 10000/49502 (20%) | Loss: 0.408026\n",
      "Train Epoch: 39 | Batch Status: 11000/49502 (22%) | Loss: 0.407256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 39 | Batch Status: 12000/49502 (24%) | Loss: 0.469355\n",
      "Train Epoch: 39 | Batch Status: 13000/49502 (26%) | Loss: 0.527296\n",
      "Train Epoch: 39 | Batch Status: 14000/49502 (28%) | Loss: 0.929030\n",
      "Train Epoch: 39 | Batch Status: 15000/49502 (30%) | Loss: 0.382232\n",
      "Train Epoch: 39 | Batch Status: 16000/49502 (32%) | Loss: 0.176999\n",
      "Train Epoch: 39 | Batch Status: 17000/49502 (34%) | Loss: 0.924999\n",
      "Train Epoch: 39 | Batch Status: 18000/49502 (36%) | Loss: 0.159840\n",
      "Train Epoch: 39 | Batch Status: 19000/49502 (38%) | Loss: 0.186678\n",
      "Train Epoch: 39 | Batch Status: 20000/49502 (40%) | Loss: 0.380517\n",
      "Train Epoch: 39 | Batch Status: 21000/49502 (42%) | Loss: 0.494894\n",
      "Train Epoch: 39 | Batch Status: 22000/49502 (44%) | Loss: 0.173580\n",
      "Train Epoch: 39 | Batch Status: 23000/49502 (46%) | Loss: 0.336313\n",
      "Train Epoch: 39 | Batch Status: 24000/49502 (48%) | Loss: 0.806541\n",
      "Train Epoch: 39 | Batch Status: 25000/49502 (50%) | Loss: 0.159198\n",
      "Train Epoch: 39 | Batch Status: 26000/49502 (53%) | Loss: 0.181066\n",
      "Train Epoch: 39 | Batch Status: 27000/49502 (55%) | Loss: 0.437956\n",
      "Train Epoch: 39 | Batch Status: 28000/49502 (57%) | Loss: 0.230768\n",
      "Train Epoch: 39 | Batch Status: 29000/49502 (59%) | Loss: 0.576495\n",
      "Train Epoch: 39 | Batch Status: 30000/49502 (61%) | Loss: 0.189187\n",
      "Train Epoch: 39 | Batch Status: 31000/49502 (63%) | Loss: 0.438402\n",
      "Train Epoch: 39 | Batch Status: 32000/49502 (65%) | Loss: 0.379888\n",
      "Train Epoch: 39 | Batch Status: 33000/49502 (67%) | Loss: 0.107705\n",
      "Train Epoch: 39 | Batch Status: 34000/49502 (69%) | Loss: 0.561645\n",
      "Train Epoch: 39 | Batch Status: 35000/49502 (71%) | Loss: 0.406434\n",
      "Train Epoch: 39 | Batch Status: 36000/49502 (73%) | Loss: 0.254287\n",
      "Train Epoch: 39 | Batch Status: 37000/49502 (75%) | Loss: 0.432573\n",
      "Train Epoch: 39 | Batch Status: 38000/49502 (77%) | Loss: 0.392673\n",
      "Train Epoch: 39 | Batch Status: 39000/49502 (79%) | Loss: 0.287465\n",
      "Train Epoch: 39 | Batch Status: 40000/49502 (81%) | Loss: 0.225304\n",
      "Train Epoch: 39 | Batch Status: 41000/49502 (83%) | Loss: 0.109820\n",
      "Train Epoch: 39 | Batch Status: 42000/49502 (85%) | Loss: 0.532003\n",
      "Train Epoch: 39 | Batch Status: 43000/49502 (87%) | Loss: 0.505610\n",
      "Train Epoch: 39 | Batch Status: 44000/49502 (89%) | Loss: 0.567370\n",
      "Train Epoch: 39 | Batch Status: 45000/49502 (91%) | Loss: 0.635309\n",
      "Train Epoch: 39 | Batch Status: 46000/49502 (93%) | Loss: 0.200705\n",
      "Train Epoch: 39 | Batch Status: 47000/49502 (95%) | Loss: 0.141974\n",
      "Train Epoch: 39 | Batch Status: 48000/49502 (97%) | Loss: 0.833650\n",
      "Train Epoch: 39 | Batch Status: 49000/49502 (99%) | Loss: 0.284442\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.2483, Accuracy: 5922/12376(47.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 40 | Batch Status: 0/49502 (0%) | Loss: 0.540215\n",
      "Train Epoch: 40 | Batch Status: 1000/49502 (2%) | Loss: 0.337563\n",
      "Train Epoch: 40 | Batch Status: 2000/49502 (4%) | Loss: 0.104400\n",
      "Train Epoch: 40 | Batch Status: 3000/49502 (6%) | Loss: 0.277097\n",
      "Train Epoch: 40 | Batch Status: 4000/49502 (8%) | Loss: 0.264896\n",
      "Train Epoch: 40 | Batch Status: 5000/49502 (10%) | Loss: 0.160129\n",
      "Train Epoch: 40 | Batch Status: 6000/49502 (12%) | Loss: 0.264219\n",
      "Train Epoch: 40 | Batch Status: 7000/49502 (14%) | Loss: 0.423831\n",
      "Train Epoch: 40 | Batch Status: 8000/49502 (16%) | Loss: 0.309904\n",
      "Train Epoch: 40 | Batch Status: 9000/49502 (18%) | Loss: 0.139495\n",
      "Train Epoch: 40 | Batch Status: 10000/49502 (20%) | Loss: 0.026281\n",
      "Train Epoch: 40 | Batch Status: 11000/49502 (22%) | Loss: 0.595175\n",
      "Train Epoch: 40 | Batch Status: 12000/49502 (24%) | Loss: 0.300407\n",
      "Train Epoch: 40 | Batch Status: 13000/49502 (26%) | Loss: 0.498654\n",
      "Train Epoch: 40 | Batch Status: 14000/49502 (28%) | Loss: 0.049723\n",
      "Train Epoch: 40 | Batch Status: 15000/49502 (30%) | Loss: 0.383588\n",
      "Train Epoch: 40 | Batch Status: 16000/49502 (32%) | Loss: 0.608984\n",
      "Train Epoch: 40 | Batch Status: 17000/49502 (34%) | Loss: 0.257143\n",
      "Train Epoch: 40 | Batch Status: 18000/49502 (36%) | Loss: 0.564917\n",
      "Train Epoch: 40 | Batch Status: 19000/49502 (38%) | Loss: 0.284336\n",
      "Train Epoch: 40 | Batch Status: 20000/49502 (40%) | Loss: 0.389741\n",
      "Train Epoch: 40 | Batch Status: 21000/49502 (42%) | Loss: 0.358895\n",
      "Train Epoch: 40 | Batch Status: 22000/49502 (44%) | Loss: 0.665854\n",
      "Train Epoch: 40 | Batch Status: 23000/49502 (46%) | Loss: 0.178195\n",
      "Train Epoch: 40 | Batch Status: 24000/49502 (48%) | Loss: 0.487534\n",
      "Train Epoch: 40 | Batch Status: 25000/49502 (50%) | Loss: 0.199212\n",
      "Train Epoch: 40 | Batch Status: 26000/49502 (53%) | Loss: 0.180845\n",
      "Train Epoch: 40 | Batch Status: 27000/49502 (55%) | Loss: 0.229333\n",
      "Train Epoch: 40 | Batch Status: 28000/49502 (57%) | Loss: 0.411881\n",
      "Train Epoch: 40 | Batch Status: 29000/49502 (59%) | Loss: 0.103352\n",
      "Train Epoch: 40 | Batch Status: 30000/49502 (61%) | Loss: 0.860643\n",
      "Train Epoch: 40 | Batch Status: 31000/49502 (63%) | Loss: 0.641811\n",
      "Train Epoch: 40 | Batch Status: 32000/49502 (65%) | Loss: 0.553152\n",
      "Train Epoch: 40 | Batch Status: 33000/49502 (67%) | Loss: 0.641819\n",
      "Train Epoch: 40 | Batch Status: 34000/49502 (69%) | Loss: 0.391396\n",
      "Train Epoch: 40 | Batch Status: 35000/49502 (71%) | Loss: 0.140075\n",
      "Train Epoch: 40 | Batch Status: 36000/49502 (73%) | Loss: 0.666749\n",
      "Train Epoch: 40 | Batch Status: 37000/49502 (75%) | Loss: 0.339774\n",
      "Train Epoch: 40 | Batch Status: 38000/49502 (77%) | Loss: 0.704677\n",
      "Train Epoch: 40 | Batch Status: 39000/49502 (79%) | Loss: 0.777654\n",
      "Train Epoch: 40 | Batch Status: 40000/49502 (81%) | Loss: 0.211977\n",
      "Train Epoch: 40 | Batch Status: 41000/49502 (83%) | Loss: 0.636303\n",
      "Train Epoch: 40 | Batch Status: 42000/49502 (85%) | Loss: 0.436577\n",
      "Train Epoch: 40 | Batch Status: 43000/49502 (87%) | Loss: 0.535072\n",
      "Train Epoch: 40 | Batch Status: 44000/49502 (89%) | Loss: 1.007945\n",
      "Train Epoch: 40 | Batch Status: 45000/49502 (91%) | Loss: 0.429267\n",
      "Train Epoch: 40 | Batch Status: 46000/49502 (93%) | Loss: 0.682261\n",
      "Train Epoch: 40 | Batch Status: 47000/49502 (95%) | Loss: 0.424585\n",
      "Train Epoch: 40 | Batch Status: 48000/49502 (97%) | Loss: 0.385978\n",
      "Train Epoch: 40 | Batch Status: 49000/49502 (99%) | Loss: 0.910058\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.3110, Accuracy: 5034/12376(40.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 41 | Batch Status: 0/49502 (0%) | Loss: 0.382478\n",
      "Train Epoch: 41 | Batch Status: 1000/49502 (2%) | Loss: 0.782783\n",
      "Train Epoch: 41 | Batch Status: 2000/49502 (4%) | Loss: 0.322138\n",
      "Train Epoch: 41 | Batch Status: 3000/49502 (6%) | Loss: 0.099191\n",
      "Train Epoch: 41 | Batch Status: 4000/49502 (8%) | Loss: 0.206026\n",
      "Train Epoch: 41 | Batch Status: 5000/49502 (10%) | Loss: 0.328690\n",
      "Train Epoch: 41 | Batch Status: 6000/49502 (12%) | Loss: 0.163430\n",
      "Train Epoch: 41 | Batch Status: 7000/49502 (14%) | Loss: 0.614246\n",
      "Train Epoch: 41 | Batch Status: 8000/49502 (16%) | Loss: 0.495022\n",
      "Train Epoch: 41 | Batch Status: 9000/49502 (18%) | Loss: 0.304215\n",
      "Train Epoch: 41 | Batch Status: 10000/49502 (20%) | Loss: 0.142216\n",
      "Train Epoch: 41 | Batch Status: 11000/49502 (22%) | Loss: 0.186625\n",
      "Train Epoch: 41 | Batch Status: 12000/49502 (24%) | Loss: 0.957230\n",
      "Train Epoch: 41 | Batch Status: 13000/49502 (26%) | Loss: 0.272984\n",
      "Train Epoch: 41 | Batch Status: 14000/49502 (28%) | Loss: 0.387212\n",
      "Train Epoch: 41 | Batch Status: 15000/49502 (30%) | Loss: 0.552077\n",
      "Train Epoch: 41 | Batch Status: 16000/49502 (32%) | Loss: 0.183213\n",
      "Train Epoch: 41 | Batch Status: 17000/49502 (34%) | Loss: 0.420914\n",
      "Train Epoch: 41 | Batch Status: 18000/49502 (36%) | Loss: 0.343405\n",
      "Train Epoch: 41 | Batch Status: 19000/49502 (38%) | Loss: 0.398386\n",
      "Train Epoch: 41 | Batch Status: 20000/49502 (40%) | Loss: 0.686806\n",
      "Train Epoch: 41 | Batch Status: 21000/49502 (42%) | Loss: 0.302893\n",
      "Train Epoch: 41 | Batch Status: 22000/49502 (44%) | Loss: 0.670133\n",
      "Train Epoch: 41 | Batch Status: 23000/49502 (46%) | Loss: 0.464400\n",
      "Train Epoch: 41 | Batch Status: 24000/49502 (48%) | Loss: 0.401256\n",
      "Train Epoch: 41 | Batch Status: 25000/49502 (50%) | Loss: 0.206373\n",
      "Train Epoch: 41 | Batch Status: 26000/49502 (53%) | Loss: 0.120775\n",
      "Train Epoch: 41 | Batch Status: 27000/49502 (55%) | Loss: 0.672206\n",
      "Train Epoch: 41 | Batch Status: 28000/49502 (57%) | Loss: 0.121948\n",
      "Train Epoch: 41 | Batch Status: 29000/49502 (59%) | Loss: 0.355750\n",
      "Train Epoch: 41 | Batch Status: 30000/49502 (61%) | Loss: 0.501846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 41 | Batch Status: 31000/49502 (63%) | Loss: 0.696766\n",
      "Train Epoch: 41 | Batch Status: 32000/49502 (65%) | Loss: 0.571046\n",
      "Train Epoch: 41 | Batch Status: 33000/49502 (67%) | Loss: 0.569923\n",
      "Train Epoch: 41 | Batch Status: 34000/49502 (69%) | Loss: 0.549826\n",
      "Train Epoch: 41 | Batch Status: 35000/49502 (71%) | Loss: 0.392630\n",
      "Train Epoch: 41 | Batch Status: 36000/49502 (73%) | Loss: 0.263784\n",
      "Train Epoch: 41 | Batch Status: 37000/49502 (75%) | Loss: 0.188230\n",
      "Train Epoch: 41 | Batch Status: 38000/49502 (77%) | Loss: 0.176213\n",
      "Train Epoch: 41 | Batch Status: 39000/49502 (79%) | Loss: 0.679282\n",
      "Train Epoch: 41 | Batch Status: 40000/49502 (81%) | Loss: 0.178473\n",
      "Train Epoch: 41 | Batch Status: 41000/49502 (83%) | Loss: 0.119770\n",
      "Train Epoch: 41 | Batch Status: 42000/49502 (85%) | Loss: 0.594499\n",
      "Train Epoch: 41 | Batch Status: 43000/49502 (87%) | Loss: 0.107437\n",
      "Train Epoch: 41 | Batch Status: 44000/49502 (89%) | Loss: 0.409219\n",
      "Train Epoch: 41 | Batch Status: 45000/49502 (91%) | Loss: 0.756703\n",
      "Train Epoch: 41 | Batch Status: 46000/49502 (93%) | Loss: 0.051932\n",
      "Train Epoch: 41 | Batch Status: 47000/49502 (95%) | Loss: 0.710610\n",
      "Train Epoch: 41 | Batch Status: 48000/49502 (97%) | Loss: 0.200787\n",
      "Train Epoch: 41 | Batch Status: 49000/49502 (99%) | Loss: 0.434118\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.2399, Accuracy: 5529/12376(44.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 42 | Batch Status: 0/49502 (0%) | Loss: 0.172608\n",
      "Train Epoch: 42 | Batch Status: 1000/49502 (2%) | Loss: 0.162347\n",
      "Train Epoch: 42 | Batch Status: 2000/49502 (4%) | Loss: 0.289437\n",
      "Train Epoch: 42 | Batch Status: 3000/49502 (6%) | Loss: 0.092563\n",
      "Train Epoch: 42 | Batch Status: 4000/49502 (8%) | Loss: 0.490284\n",
      "Train Epoch: 42 | Batch Status: 5000/49502 (10%) | Loss: 0.150876\n",
      "Train Epoch: 42 | Batch Status: 6000/49502 (12%) | Loss: 0.326965\n",
      "Train Epoch: 42 | Batch Status: 7000/49502 (14%) | Loss: 0.446537\n",
      "Train Epoch: 42 | Batch Status: 8000/49502 (16%) | Loss: 0.561045\n",
      "Train Epoch: 42 | Batch Status: 9000/49502 (18%) | Loss: 0.202235\n",
      "Train Epoch: 42 | Batch Status: 10000/49502 (20%) | Loss: 0.042798\n",
      "Train Epoch: 42 | Batch Status: 11000/49502 (22%) | Loss: 0.327691\n",
      "Train Epoch: 42 | Batch Status: 12000/49502 (24%) | Loss: 0.321842\n",
      "Train Epoch: 42 | Batch Status: 13000/49502 (26%) | Loss: 0.548381\n",
      "Train Epoch: 42 | Batch Status: 14000/49502 (28%) | Loss: 0.069765\n",
      "Train Epoch: 42 | Batch Status: 15000/49502 (30%) | Loss: 0.359968\n",
      "Train Epoch: 42 | Batch Status: 16000/49502 (32%) | Loss: 0.584434\n",
      "Train Epoch: 42 | Batch Status: 17000/49502 (34%) | Loss: 0.349285\n",
      "Train Epoch: 42 | Batch Status: 18000/49502 (36%) | Loss: 0.500465\n",
      "Train Epoch: 42 | Batch Status: 19000/49502 (38%) | Loss: 0.205657\n",
      "Train Epoch: 42 | Batch Status: 20000/49502 (40%) | Loss: 0.138135\n",
      "Train Epoch: 42 | Batch Status: 21000/49502 (42%) | Loss: 0.271469\n",
      "Train Epoch: 42 | Batch Status: 22000/49502 (44%) | Loss: 0.163375\n",
      "Train Epoch: 42 | Batch Status: 23000/49502 (46%) | Loss: 0.255953\n",
      "Train Epoch: 42 | Batch Status: 24000/49502 (48%) | Loss: 0.230306\n",
      "Train Epoch: 42 | Batch Status: 25000/49502 (50%) | Loss: 0.498766\n",
      "Train Epoch: 42 | Batch Status: 26000/49502 (53%) | Loss: 0.487264\n",
      "Train Epoch: 42 | Batch Status: 27000/49502 (55%) | Loss: 0.442902\n",
      "Train Epoch: 42 | Batch Status: 28000/49502 (57%) | Loss: 0.284990\n",
      "Train Epoch: 42 | Batch Status: 29000/49502 (59%) | Loss: 0.348477\n",
      "Train Epoch: 42 | Batch Status: 30000/49502 (61%) | Loss: 0.486328\n",
      "Train Epoch: 42 | Batch Status: 31000/49502 (63%) | Loss: 0.304912\n",
      "Train Epoch: 42 | Batch Status: 32000/49502 (65%) | Loss: 0.308753\n",
      "Train Epoch: 42 | Batch Status: 33000/49502 (67%) | Loss: 0.487856\n",
      "Train Epoch: 42 | Batch Status: 34000/49502 (69%) | Loss: 1.337445\n",
      "Train Epoch: 42 | Batch Status: 35000/49502 (71%) | Loss: 0.594715\n",
      "Train Epoch: 42 | Batch Status: 36000/49502 (73%) | Loss: 0.286541\n",
      "Train Epoch: 42 | Batch Status: 37000/49502 (75%) | Loss: 0.152029\n",
      "Train Epoch: 42 | Batch Status: 38000/49502 (77%) | Loss: 0.522043\n",
      "Train Epoch: 42 | Batch Status: 39000/49502 (79%) | Loss: 0.258667\n",
      "Train Epoch: 42 | Batch Status: 40000/49502 (81%) | Loss: 0.284392\n",
      "Train Epoch: 42 | Batch Status: 41000/49502 (83%) | Loss: 0.417703\n",
      "Train Epoch: 42 | Batch Status: 42000/49502 (85%) | Loss: 0.239647\n",
      "Train Epoch: 42 | Batch Status: 43000/49502 (87%) | Loss: 0.552457\n",
      "Train Epoch: 42 | Batch Status: 44000/49502 (89%) | Loss: 1.033101\n",
      "Train Epoch: 42 | Batch Status: 45000/49502 (91%) | Loss: 0.416451\n",
      "Train Epoch: 42 | Batch Status: 46000/49502 (93%) | Loss: 0.362001\n",
      "Train Epoch: 42 | Batch Status: 47000/49502 (95%) | Loss: 0.415152\n",
      "Train Epoch: 42 | Batch Status: 48000/49502 (97%) | Loss: 0.447413\n",
      "Train Epoch: 42 | Batch Status: 49000/49502 (99%) | Loss: 0.336589\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.3596, Accuracy: 5509/12376(44.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 43 | Batch Status: 0/49502 (0%) | Loss: 0.343422\n",
      "Train Epoch: 43 | Batch Status: 1000/49502 (2%) | Loss: 0.236244\n",
      "Train Epoch: 43 | Batch Status: 2000/49502 (4%) | Loss: 0.332743\n",
      "Train Epoch: 43 | Batch Status: 3000/49502 (6%) | Loss: 0.413994\n",
      "Train Epoch: 43 | Batch Status: 4000/49502 (8%) | Loss: 0.242214\n",
      "Train Epoch: 43 | Batch Status: 5000/49502 (10%) | Loss: 0.149300\n",
      "Train Epoch: 43 | Batch Status: 6000/49502 (12%) | Loss: 0.234157\n",
      "Train Epoch: 43 | Batch Status: 7000/49502 (14%) | Loss: 0.602900\n",
      "Train Epoch: 43 | Batch Status: 8000/49502 (16%) | Loss: 0.159560\n",
      "Train Epoch: 43 | Batch Status: 9000/49502 (18%) | Loss: 0.316484\n",
      "Train Epoch: 43 | Batch Status: 10000/49502 (20%) | Loss: 0.191042\n",
      "Train Epoch: 43 | Batch Status: 11000/49502 (22%) | Loss: 0.693156\n",
      "Train Epoch: 43 | Batch Status: 12000/49502 (24%) | Loss: 0.127249\n",
      "Train Epoch: 43 | Batch Status: 13000/49502 (26%) | Loss: 0.256034\n",
      "Train Epoch: 43 | Batch Status: 14000/49502 (28%) | Loss: 0.552254\n",
      "Train Epoch: 43 | Batch Status: 15000/49502 (30%) | Loss: 0.284356\n",
      "Train Epoch: 43 | Batch Status: 16000/49502 (32%) | Loss: 0.162204\n",
      "Train Epoch: 43 | Batch Status: 17000/49502 (34%) | Loss: 0.552752\n",
      "Train Epoch: 43 | Batch Status: 18000/49502 (36%) | Loss: 0.305922\n",
      "Train Epoch: 43 | Batch Status: 19000/49502 (38%) | Loss: 0.287214\n",
      "Train Epoch: 43 | Batch Status: 20000/49502 (40%) | Loss: 0.370815\n",
      "Train Epoch: 43 | Batch Status: 21000/49502 (42%) | Loss: 0.424188\n",
      "Train Epoch: 43 | Batch Status: 22000/49502 (44%) | Loss: 0.547533\n",
      "Train Epoch: 43 | Batch Status: 23000/49502 (46%) | Loss: 0.378314\n",
      "Train Epoch: 43 | Batch Status: 24000/49502 (48%) | Loss: 0.396942\n",
      "Train Epoch: 43 | Batch Status: 25000/49502 (50%) | Loss: 0.333352\n",
      "Train Epoch: 43 | Batch Status: 26000/49502 (53%) | Loss: 0.346697\n",
      "Train Epoch: 43 | Batch Status: 27000/49502 (55%) | Loss: 0.163707\n",
      "Train Epoch: 43 | Batch Status: 28000/49502 (57%) | Loss: 0.415527\n",
      "Train Epoch: 43 | Batch Status: 29000/49502 (59%) | Loss: 0.488315\n",
      "Train Epoch: 43 | Batch Status: 30000/49502 (61%) | Loss: 0.514852\n",
      "Train Epoch: 43 | Batch Status: 31000/49502 (63%) | Loss: 0.249004\n",
      "Train Epoch: 43 | Batch Status: 32000/49502 (65%) | Loss: 0.741255\n",
      "Train Epoch: 43 | Batch Status: 33000/49502 (67%) | Loss: 0.809187\n",
      "Train Epoch: 43 | Batch Status: 34000/49502 (69%) | Loss: 0.348244\n",
      "Train Epoch: 43 | Batch Status: 35000/49502 (71%) | Loss: 0.045641\n",
      "Train Epoch: 43 | Batch Status: 36000/49502 (73%) | Loss: 0.771188\n",
      "Train Epoch: 43 | Batch Status: 37000/49502 (75%) | Loss: 0.086428\n",
      "Train Epoch: 43 | Batch Status: 38000/49502 (77%) | Loss: 0.218602\n",
      "Train Epoch: 43 | Batch Status: 39000/49502 (79%) | Loss: 0.449280\n",
      "Train Epoch: 43 | Batch Status: 40000/49502 (81%) | Loss: 0.454973\n",
      "Train Epoch: 43 | Batch Status: 41000/49502 (83%) | Loss: 0.221326\n",
      "Train Epoch: 43 | Batch Status: 42000/49502 (85%) | Loss: 0.180508\n",
      "Train Epoch: 43 | Batch Status: 43000/49502 (87%) | Loss: 0.463877\n",
      "Train Epoch: 43 | Batch Status: 44000/49502 (89%) | Loss: 0.287136\n",
      "Train Epoch: 43 | Batch Status: 45000/49502 (91%) | Loss: 0.444813\n",
      "Train Epoch: 43 | Batch Status: 46000/49502 (93%) | Loss: 0.574863\n",
      "Train Epoch: 43 | Batch Status: 47000/49502 (95%) | Loss: 0.339830\n",
      "Train Epoch: 43 | Batch Status: 48000/49502 (97%) | Loss: 0.151976\n",
      "Train Epoch: 43 | Batch Status: 49000/49502 (99%) | Loss: 0.268306\n",
      "Training time: 0m 6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "Test set: Average loss: 1.3361, Accuracy: 5360/12376(43.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 44 | Batch Status: 0/49502 (0%) | Loss: 0.293431\n",
      "Train Epoch: 44 | Batch Status: 1000/49502 (2%) | Loss: 0.395116\n",
      "Train Epoch: 44 | Batch Status: 2000/49502 (4%) | Loss: 0.257639\n",
      "Train Epoch: 44 | Batch Status: 3000/49502 (6%) | Loss: 0.386530\n",
      "Train Epoch: 44 | Batch Status: 4000/49502 (8%) | Loss: 0.754486\n",
      "Train Epoch: 44 | Batch Status: 5000/49502 (10%) | Loss: 0.763830\n",
      "Train Epoch: 44 | Batch Status: 6000/49502 (12%) | Loss: 0.514983\n",
      "Train Epoch: 44 | Batch Status: 7000/49502 (14%) | Loss: 0.124574\n",
      "Train Epoch: 44 | Batch Status: 8000/49502 (16%) | Loss: 0.354719\n",
      "Train Epoch: 44 | Batch Status: 9000/49502 (18%) | Loss: 0.374835\n",
      "Train Epoch: 44 | Batch Status: 10000/49502 (20%) | Loss: 0.632412\n",
      "Train Epoch: 44 | Batch Status: 11000/49502 (22%) | Loss: 0.546767\n",
      "Train Epoch: 44 | Batch Status: 12000/49502 (24%) | Loss: 0.531787\n",
      "Train Epoch: 44 | Batch Status: 13000/49502 (26%) | Loss: 0.117609\n",
      "Train Epoch: 44 | Batch Status: 14000/49502 (28%) | Loss: 0.359429\n",
      "Train Epoch: 44 | Batch Status: 15000/49502 (30%) | Loss: 0.271014\n",
      "Train Epoch: 44 | Batch Status: 16000/49502 (32%) | Loss: 0.386662\n",
      "Train Epoch: 44 | Batch Status: 17000/49502 (34%) | Loss: 0.101091\n",
      "Train Epoch: 44 | Batch Status: 18000/49502 (36%) | Loss: 0.545524\n",
      "Train Epoch: 44 | Batch Status: 19000/49502 (38%) | Loss: 0.091191\n",
      "Train Epoch: 44 | Batch Status: 20000/49502 (40%) | Loss: 0.321911\n",
      "Train Epoch: 44 | Batch Status: 21000/49502 (42%) | Loss: 0.741505\n",
      "Train Epoch: 44 | Batch Status: 22000/49502 (44%) | Loss: 0.150058\n",
      "Train Epoch: 44 | Batch Status: 23000/49502 (46%) | Loss: 0.252351\n",
      "Train Epoch: 44 | Batch Status: 24000/49502 (48%) | Loss: 0.183092\n",
      "Train Epoch: 44 | Batch Status: 25000/49502 (50%) | Loss: 0.181049\n",
      "Train Epoch: 44 | Batch Status: 26000/49502 (53%) | Loss: 0.540151\n",
      "Train Epoch: 44 | Batch Status: 27000/49502 (55%) | Loss: 0.486220\n",
      "Train Epoch: 44 | Batch Status: 28000/49502 (57%) | Loss: 0.247261\n",
      "Train Epoch: 44 | Batch Status: 29000/49502 (59%) | Loss: 0.290022\n",
      "Train Epoch: 44 | Batch Status: 30000/49502 (61%) | Loss: 0.174027\n",
      "Train Epoch: 44 | Batch Status: 31000/49502 (63%) | Loss: 0.303190\n",
      "Train Epoch: 44 | Batch Status: 32000/49502 (65%) | Loss: 0.390265\n",
      "Train Epoch: 44 | Batch Status: 33000/49502 (67%) | Loss: 0.144539\n",
      "Train Epoch: 44 | Batch Status: 34000/49502 (69%) | Loss: 0.699273\n",
      "Train Epoch: 44 | Batch Status: 35000/49502 (71%) | Loss: 0.360880\n",
      "Train Epoch: 44 | Batch Status: 36000/49502 (73%) | Loss: 0.144365\n",
      "Train Epoch: 44 | Batch Status: 37000/49502 (75%) | Loss: 0.373762\n",
      "Train Epoch: 44 | Batch Status: 38000/49502 (77%) | Loss: 0.676382\n",
      "Train Epoch: 44 | Batch Status: 39000/49502 (79%) | Loss: 0.459967\n",
      "Train Epoch: 44 | Batch Status: 40000/49502 (81%) | Loss: 0.250313\n",
      "Train Epoch: 44 | Batch Status: 41000/49502 (83%) | Loss: 0.539680\n",
      "Train Epoch: 44 | Batch Status: 42000/49502 (85%) | Loss: 0.809032\n",
      "Train Epoch: 44 | Batch Status: 43000/49502 (87%) | Loss: 0.258698\n",
      "Train Epoch: 44 | Batch Status: 44000/49502 (89%) | Loss: 0.228758\n",
      "Train Epoch: 44 | Batch Status: 45000/49502 (91%) | Loss: 0.733933\n",
      "Train Epoch: 44 | Batch Status: 46000/49502 (93%) | Loss: 0.805181\n",
      "Train Epoch: 44 | Batch Status: 47000/49502 (95%) | Loss: 0.284172\n",
      "Train Epoch: 44 | Batch Status: 48000/49502 (97%) | Loss: 0.337210\n",
      "Train Epoch: 44 | Batch Status: 49000/49502 (99%) | Loss: 0.453174\n",
      "Training time: 0m 5s\n",
      "==========================\n",
      "Test set: Average loss: 1.2380, Accuracy: 4682/12376(37.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 45 | Batch Status: 0/49502 (0%) | Loss: 0.490863\n",
      "Train Epoch: 45 | Batch Status: 1000/49502 (2%) | Loss: 0.203038\n",
      "Train Epoch: 45 | Batch Status: 2000/49502 (4%) | Loss: 0.298932\n",
      "Train Epoch: 45 | Batch Status: 3000/49502 (6%) | Loss: 0.358356\n",
      "Train Epoch: 45 | Batch Status: 4000/49502 (8%) | Loss: 0.278864\n",
      "Train Epoch: 45 | Batch Status: 5000/49502 (10%) | Loss: 0.373858\n",
      "Train Epoch: 45 | Batch Status: 6000/49502 (12%) | Loss: 0.114146\n",
      "Train Epoch: 45 | Batch Status: 7000/49502 (14%) | Loss: 0.386226\n",
      "Train Epoch: 45 | Batch Status: 8000/49502 (16%) | Loss: 0.336175\n",
      "Train Epoch: 45 | Batch Status: 9000/49502 (18%) | Loss: 0.686098\n",
      "Train Epoch: 45 | Batch Status: 10000/49502 (20%) | Loss: 0.074371\n",
      "Train Epoch: 45 | Batch Status: 11000/49502 (22%) | Loss: 0.162917\n",
      "Train Epoch: 45 | Batch Status: 12000/49502 (24%) | Loss: 0.193958\n",
      "Train Epoch: 45 | Batch Status: 13000/49502 (26%) | Loss: 0.434731\n",
      "Train Epoch: 45 | Batch Status: 14000/49502 (28%) | Loss: 0.452684\n",
      "Train Epoch: 45 | Batch Status: 15000/49502 (30%) | Loss: 0.307960\n",
      "Train Epoch: 45 | Batch Status: 16000/49502 (32%) | Loss: 0.422975\n",
      "Train Epoch: 45 | Batch Status: 17000/49502 (34%) | Loss: 0.638194\n",
      "Train Epoch: 45 | Batch Status: 18000/49502 (36%) | Loss: 0.740274\n",
      "Train Epoch: 45 | Batch Status: 19000/49502 (38%) | Loss: 0.307479\n",
      "Train Epoch: 45 | Batch Status: 20000/49502 (40%) | Loss: 0.508981\n",
      "Train Epoch: 45 | Batch Status: 21000/49502 (42%) | Loss: 0.348808\n",
      "Train Epoch: 45 | Batch Status: 22000/49502 (44%) | Loss: 0.287695\n",
      "Train Epoch: 45 | Batch Status: 23000/49502 (46%) | Loss: 0.191789\n",
      "Train Epoch: 45 | Batch Status: 24000/49502 (48%) | Loss: 0.392025\n",
      "Train Epoch: 45 | Batch Status: 25000/49502 (50%) | Loss: 0.718579\n",
      "Train Epoch: 45 | Batch Status: 26000/49502 (53%) | Loss: 0.322167\n",
      "Train Epoch: 45 | Batch Status: 27000/49502 (55%) | Loss: 0.555058\n",
      "Train Epoch: 45 | Batch Status: 28000/49502 (57%) | Loss: 0.859936\n",
      "Train Epoch: 45 | Batch Status: 29000/49502 (59%) | Loss: 0.311716\n",
      "Train Epoch: 45 | Batch Status: 30000/49502 (61%) | Loss: 0.377535\n",
      "Train Epoch: 45 | Batch Status: 31000/49502 (63%) | Loss: 0.418297\n",
      "Train Epoch: 45 | Batch Status: 32000/49502 (65%) | Loss: 0.266620\n",
      "Train Epoch: 45 | Batch Status: 33000/49502 (67%) | Loss: 0.135500\n",
      "Train Epoch: 45 | Batch Status: 34000/49502 (69%) | Loss: 0.368339\n",
      "Train Epoch: 45 | Batch Status: 35000/49502 (71%) | Loss: 0.169774\n",
      "Train Epoch: 45 | Batch Status: 36000/49502 (73%) | Loss: 0.155861\n",
      "Train Epoch: 45 | Batch Status: 37000/49502 (75%) | Loss: 0.153587\n",
      "Train Epoch: 45 | Batch Status: 38000/49502 (77%) | Loss: 0.888784\n",
      "Train Epoch: 45 | Batch Status: 39000/49502 (79%) | Loss: 0.256495\n",
      "Train Epoch: 45 | Batch Status: 40000/49502 (81%) | Loss: 0.569544\n",
      "Train Epoch: 45 | Batch Status: 41000/49502 (83%) | Loss: 0.191988\n",
      "Train Epoch: 45 | Batch Status: 42000/49502 (85%) | Loss: 1.353739\n",
      "Train Epoch: 45 | Batch Status: 43000/49502 (87%) | Loss: 0.147805\n",
      "Train Epoch: 45 | Batch Status: 44000/49502 (89%) | Loss: 0.235645\n",
      "Train Epoch: 45 | Batch Status: 45000/49502 (91%) | Loss: 0.367977\n",
      "Train Epoch: 45 | Batch Status: 46000/49502 (93%) | Loss: 0.845374\n",
      "Train Epoch: 45 | Batch Status: 47000/49502 (95%) | Loss: 0.857373\n",
      "Train Epoch: 45 | Batch Status: 48000/49502 (97%) | Loss: 0.675544\n",
      "Train Epoch: 45 | Batch Status: 49000/49502 (99%) | Loss: 0.224981\n",
      "Training time: 0m 5s\n",
      "==========================\n",
      "Test set: Average loss: 1.2563, Accuracy: 5789/12376(46.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 46 | Batch Status: 0/49502 (0%) | Loss: 0.654356\n",
      "Train Epoch: 46 | Batch Status: 1000/49502 (2%) | Loss: 0.641177\n",
      "Train Epoch: 46 | Batch Status: 2000/49502 (4%) | Loss: 0.738982\n",
      "Train Epoch: 46 | Batch Status: 3000/49502 (6%) | Loss: 0.287148\n",
      "Train Epoch: 46 | Batch Status: 4000/49502 (8%) | Loss: 0.292909\n",
      "Train Epoch: 46 | Batch Status: 5000/49502 (10%) | Loss: 0.421517\n",
      "Train Epoch: 46 | Batch Status: 6000/49502 (12%) | Loss: 0.145134\n",
      "Train Epoch: 46 | Batch Status: 7000/49502 (14%) | Loss: 0.564034\n",
      "Train Epoch: 46 | Batch Status: 8000/49502 (16%) | Loss: 0.286899\n",
      "Train Epoch: 46 | Batch Status: 9000/49502 (18%) | Loss: 0.431457\n",
      "Train Epoch: 46 | Batch Status: 10000/49502 (20%) | Loss: 0.121922\n",
      "Train Epoch: 46 | Batch Status: 11000/49502 (22%) | Loss: 0.261157\n",
      "Train Epoch: 46 | Batch Status: 12000/49502 (24%) | Loss: 0.072550\n",
      "Train Epoch: 46 | Batch Status: 13000/49502 (26%) | Loss: 0.195111\n",
      "Train Epoch: 46 | Batch Status: 14000/49502 (28%) | Loss: 0.582961\n",
      "Train Epoch: 46 | Batch Status: 15000/49502 (30%) | Loss: 0.214167\n",
      "Train Epoch: 46 | Batch Status: 16000/49502 (32%) | Loss: 0.188369\n",
      "Train Epoch: 46 | Batch Status: 17000/49502 (34%) | Loss: 0.420261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46 | Batch Status: 18000/49502 (36%) | Loss: 0.107343\n",
      "Train Epoch: 46 | Batch Status: 19000/49502 (38%) | Loss: 0.040474\n",
      "Train Epoch: 46 | Batch Status: 20000/49502 (40%) | Loss: 0.297911\n",
      "Train Epoch: 46 | Batch Status: 21000/49502 (42%) | Loss: 0.974424\n",
      "Train Epoch: 46 | Batch Status: 22000/49502 (44%) | Loss: 0.149221\n",
      "Train Epoch: 46 | Batch Status: 23000/49502 (46%) | Loss: 0.160134\n",
      "Train Epoch: 46 | Batch Status: 24000/49502 (48%) | Loss: 0.520395\n",
      "Train Epoch: 46 | Batch Status: 25000/49502 (50%) | Loss: 0.557926\n",
      "Train Epoch: 46 | Batch Status: 26000/49502 (53%) | Loss: 0.092899\n",
      "Train Epoch: 46 | Batch Status: 27000/49502 (55%) | Loss: 0.550898\n",
      "Train Epoch: 46 | Batch Status: 28000/49502 (57%) | Loss: 0.055201\n",
      "Train Epoch: 46 | Batch Status: 29000/49502 (59%) | Loss: 1.042181\n",
      "Train Epoch: 46 | Batch Status: 30000/49502 (61%) | Loss: 0.330651\n",
      "Train Epoch: 46 | Batch Status: 31000/49502 (63%) | Loss: 0.307339\n",
      "Train Epoch: 46 | Batch Status: 32000/49502 (65%) | Loss: 0.152388\n",
      "Train Epoch: 46 | Batch Status: 33000/49502 (67%) | Loss: 0.723885\n",
      "Train Epoch: 46 | Batch Status: 34000/49502 (69%) | Loss: 0.576827\n",
      "Train Epoch: 46 | Batch Status: 35000/49502 (71%) | Loss: 0.381033\n",
      "Train Epoch: 46 | Batch Status: 36000/49502 (73%) | Loss: 0.945406\n",
      "Train Epoch: 46 | Batch Status: 37000/49502 (75%) | Loss: 0.904975\n",
      "Train Epoch: 46 | Batch Status: 38000/49502 (77%) | Loss: 0.341222\n",
      "Train Epoch: 46 | Batch Status: 39000/49502 (79%) | Loss: 0.612749\n",
      "Train Epoch: 46 | Batch Status: 40000/49502 (81%) | Loss: 0.600923\n",
      "Train Epoch: 46 | Batch Status: 41000/49502 (83%) | Loss: 0.557964\n",
      "Train Epoch: 46 | Batch Status: 42000/49502 (85%) | Loss: 0.605294\n",
      "Train Epoch: 46 | Batch Status: 43000/49502 (87%) | Loss: 0.145290\n",
      "Train Epoch: 46 | Batch Status: 44000/49502 (89%) | Loss: 0.291990\n",
      "Train Epoch: 46 | Batch Status: 45000/49502 (91%) | Loss: 0.221365\n",
      "Train Epoch: 46 | Batch Status: 46000/49502 (93%) | Loss: 0.640128\n",
      "Train Epoch: 46 | Batch Status: 47000/49502 (95%) | Loss: 0.329801\n",
      "Train Epoch: 46 | Batch Status: 48000/49502 (97%) | Loss: 0.212847\n",
      "Train Epoch: 46 | Batch Status: 49000/49502 (99%) | Loss: 0.227600\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.3001, Accuracy: 5231/12376(42.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 47 | Batch Status: 0/49502 (0%) | Loss: 0.256903\n",
      "Train Epoch: 47 | Batch Status: 1000/49502 (2%) | Loss: 0.171693\n",
      "Train Epoch: 47 | Batch Status: 2000/49502 (4%) | Loss: 0.092644\n",
      "Train Epoch: 47 | Batch Status: 3000/49502 (6%) | Loss: 0.234838\n",
      "Train Epoch: 47 | Batch Status: 4000/49502 (8%) | Loss: 0.641353\n",
      "Train Epoch: 47 | Batch Status: 5000/49502 (10%) | Loss: 0.440143\n",
      "Train Epoch: 47 | Batch Status: 6000/49502 (12%) | Loss: 0.448380\n",
      "Train Epoch: 47 | Batch Status: 7000/49502 (14%) | Loss: 0.434959\n",
      "Train Epoch: 47 | Batch Status: 8000/49502 (16%) | Loss: 0.633765\n",
      "Train Epoch: 47 | Batch Status: 9000/49502 (18%) | Loss: 0.320853\n",
      "Train Epoch: 47 | Batch Status: 10000/49502 (20%) | Loss: 0.511314\n",
      "Train Epoch: 47 | Batch Status: 11000/49502 (22%) | Loss: 0.387954\n",
      "Train Epoch: 47 | Batch Status: 12000/49502 (24%) | Loss: 0.785821\n",
      "Train Epoch: 47 | Batch Status: 13000/49502 (26%) | Loss: 0.195548\n",
      "Train Epoch: 47 | Batch Status: 14000/49502 (28%) | Loss: 0.235632\n",
      "Train Epoch: 47 | Batch Status: 15000/49502 (30%) | Loss: 0.567739\n",
      "Train Epoch: 47 | Batch Status: 16000/49502 (32%) | Loss: 0.319802\n",
      "Train Epoch: 47 | Batch Status: 17000/49502 (34%) | Loss: 0.594107\n",
      "Train Epoch: 47 | Batch Status: 18000/49502 (36%) | Loss: 0.538369\n",
      "Train Epoch: 47 | Batch Status: 19000/49502 (38%) | Loss: 0.440390\n",
      "Train Epoch: 47 | Batch Status: 20000/49502 (40%) | Loss: 0.352171\n",
      "Train Epoch: 47 | Batch Status: 21000/49502 (42%) | Loss: 0.448839\n",
      "Train Epoch: 47 | Batch Status: 22000/49502 (44%) | Loss: 0.103417\n",
      "Train Epoch: 47 | Batch Status: 23000/49502 (46%) | Loss: 0.539538\n",
      "Train Epoch: 47 | Batch Status: 24000/49502 (48%) | Loss: 0.198272\n",
      "Train Epoch: 47 | Batch Status: 25000/49502 (50%) | Loss: 0.508842\n",
      "Train Epoch: 47 | Batch Status: 26000/49502 (53%) | Loss: 0.315132\n",
      "Train Epoch: 47 | Batch Status: 27000/49502 (55%) | Loss: 0.293752\n",
      "Train Epoch: 47 | Batch Status: 28000/49502 (57%) | Loss: 0.296481\n",
      "Train Epoch: 47 | Batch Status: 29000/49502 (59%) | Loss: 0.707954\n",
      "Train Epoch: 47 | Batch Status: 30000/49502 (61%) | Loss: 0.280069\n",
      "Train Epoch: 47 | Batch Status: 31000/49502 (63%) | Loss: 0.193049\n",
      "Train Epoch: 47 | Batch Status: 32000/49502 (65%) | Loss: 0.788415\n",
      "Train Epoch: 47 | Batch Status: 33000/49502 (67%) | Loss: 0.076054\n",
      "Train Epoch: 47 | Batch Status: 34000/49502 (69%) | Loss: 0.468997\n",
      "Train Epoch: 47 | Batch Status: 35000/49502 (71%) | Loss: 0.200471\n",
      "Train Epoch: 47 | Batch Status: 36000/49502 (73%) | Loss: 0.900546\n",
      "Train Epoch: 47 | Batch Status: 37000/49502 (75%) | Loss: 0.412351\n",
      "Train Epoch: 47 | Batch Status: 38000/49502 (77%) | Loss: 0.411980\n",
      "Train Epoch: 47 | Batch Status: 39000/49502 (79%) | Loss: 0.509892\n",
      "Train Epoch: 47 | Batch Status: 40000/49502 (81%) | Loss: 0.434104\n",
      "Train Epoch: 47 | Batch Status: 41000/49502 (83%) | Loss: 0.142198\n",
      "Train Epoch: 47 | Batch Status: 42000/49502 (85%) | Loss: 0.364442\n",
      "Train Epoch: 47 | Batch Status: 43000/49502 (87%) | Loss: 0.268863\n",
      "Train Epoch: 47 | Batch Status: 44000/49502 (89%) | Loss: 0.420101\n",
      "Train Epoch: 47 | Batch Status: 45000/49502 (91%) | Loss: 0.517454\n",
      "Train Epoch: 47 | Batch Status: 46000/49502 (93%) | Loss: 0.270810\n",
      "Train Epoch: 47 | Batch Status: 47000/49502 (95%) | Loss: 0.504511\n",
      "Train Epoch: 47 | Batch Status: 48000/49502 (97%) | Loss: 0.444789\n",
      "Train Epoch: 47 | Batch Status: 49000/49502 (99%) | Loss: 0.186430\n",
      "Training time: 0m 5s\n",
      "==========================\n",
      "Test set: Average loss: 1.2927, Accuracy: 5821/12376(47.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 48 | Batch Status: 0/49502 (0%) | Loss: 0.636224\n",
      "Train Epoch: 48 | Batch Status: 1000/49502 (2%) | Loss: 0.215234\n",
      "Train Epoch: 48 | Batch Status: 2000/49502 (4%) | Loss: 0.690427\n",
      "Train Epoch: 48 | Batch Status: 3000/49502 (6%) | Loss: 0.271939\n",
      "Train Epoch: 48 | Batch Status: 4000/49502 (8%) | Loss: 0.056213\n",
      "Train Epoch: 48 | Batch Status: 5000/49502 (10%) | Loss: 0.903162\n",
      "Train Epoch: 48 | Batch Status: 6000/49502 (12%) | Loss: 0.142781\n",
      "Train Epoch: 48 | Batch Status: 7000/49502 (14%) | Loss: 0.346016\n",
      "Train Epoch: 48 | Batch Status: 8000/49502 (16%) | Loss: 0.412994\n",
      "Train Epoch: 48 | Batch Status: 9000/49502 (18%) | Loss: 0.329423\n",
      "Train Epoch: 48 | Batch Status: 10000/49502 (20%) | Loss: 0.356442\n",
      "Train Epoch: 48 | Batch Status: 11000/49502 (22%) | Loss: 0.537049\n",
      "Train Epoch: 48 | Batch Status: 12000/49502 (24%) | Loss: 0.787589\n",
      "Train Epoch: 48 | Batch Status: 13000/49502 (26%) | Loss: 0.603676\n",
      "Train Epoch: 48 | Batch Status: 14000/49502 (28%) | Loss: 0.209650\n",
      "Train Epoch: 48 | Batch Status: 15000/49502 (30%) | Loss: 0.650019\n",
      "Train Epoch: 48 | Batch Status: 16000/49502 (32%) | Loss: 0.362832\n",
      "Train Epoch: 48 | Batch Status: 17000/49502 (34%) | Loss: 0.471953\n",
      "Train Epoch: 48 | Batch Status: 18000/49502 (36%) | Loss: 0.546653\n",
      "Train Epoch: 48 | Batch Status: 19000/49502 (38%) | Loss: 0.586189\n",
      "Train Epoch: 48 | Batch Status: 20000/49502 (40%) | Loss: 0.328118\n",
      "Train Epoch: 48 | Batch Status: 21000/49502 (42%) | Loss: 0.342157\n",
      "Train Epoch: 48 | Batch Status: 22000/49502 (44%) | Loss: 0.225271\n",
      "Train Epoch: 48 | Batch Status: 23000/49502 (46%) | Loss: 0.164341\n",
      "Train Epoch: 48 | Batch Status: 24000/49502 (48%) | Loss: 0.367330\n",
      "Train Epoch: 48 | Batch Status: 25000/49502 (50%) | Loss: 0.201273\n",
      "Train Epoch: 48 | Batch Status: 26000/49502 (53%) | Loss: 0.438108\n",
      "Train Epoch: 48 | Batch Status: 27000/49502 (55%) | Loss: 0.555244\n",
      "Train Epoch: 48 | Batch Status: 28000/49502 (57%) | Loss: 0.167374\n",
      "Train Epoch: 48 | Batch Status: 29000/49502 (59%) | Loss: 0.650261\n",
      "Train Epoch: 48 | Batch Status: 30000/49502 (61%) | Loss: 0.245650\n",
      "Train Epoch: 48 | Batch Status: 31000/49502 (63%) | Loss: 0.741103\n",
      "Train Epoch: 48 | Batch Status: 32000/49502 (65%) | Loss: 0.393226\n",
      "Train Epoch: 48 | Batch Status: 33000/49502 (67%) | Loss: 0.241094\n",
      "Train Epoch: 48 | Batch Status: 34000/49502 (69%) | Loss: 0.387539\n",
      "Train Epoch: 48 | Batch Status: 35000/49502 (71%) | Loss: 0.536657\n",
      "Train Epoch: 48 | Batch Status: 36000/49502 (73%) | Loss: 0.755269\n",
      "Train Epoch: 48 | Batch Status: 37000/49502 (75%) | Loss: 0.310102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 48 | Batch Status: 38000/49502 (77%) | Loss: 0.460484\n",
      "Train Epoch: 48 | Batch Status: 39000/49502 (79%) | Loss: 0.199489\n",
      "Train Epoch: 48 | Batch Status: 40000/49502 (81%) | Loss: 0.284764\n",
      "Train Epoch: 48 | Batch Status: 41000/49502 (83%) | Loss: 0.184922\n",
      "Train Epoch: 48 | Batch Status: 42000/49502 (85%) | Loss: 0.294421\n",
      "Train Epoch: 48 | Batch Status: 43000/49502 (87%) | Loss: 0.547582\n",
      "Train Epoch: 48 | Batch Status: 44000/49502 (89%) | Loss: 0.343031\n",
      "Train Epoch: 48 | Batch Status: 45000/49502 (91%) | Loss: 0.039450\n",
      "Train Epoch: 48 | Batch Status: 46000/49502 (93%) | Loss: 0.227222\n",
      "Train Epoch: 48 | Batch Status: 47000/49502 (95%) | Loss: 0.157766\n",
      "Train Epoch: 48 | Batch Status: 48000/49502 (97%) | Loss: 0.995601\n",
      "Train Epoch: 48 | Batch Status: 49000/49502 (99%) | Loss: 0.357707\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.4114, Accuracy: 5447/12376(44.000000%)\n",
      "Testing time: 0m 6s\n",
      "Train Epoch: 49 | Batch Status: 0/49502 (0%) | Loss: 0.237910\n",
      "Train Epoch: 49 | Batch Status: 1000/49502 (2%) | Loss: 0.334717\n",
      "Train Epoch: 49 | Batch Status: 2000/49502 (4%) | Loss: 0.786202\n",
      "Train Epoch: 49 | Batch Status: 3000/49502 (6%) | Loss: 0.536708\n",
      "Train Epoch: 49 | Batch Status: 4000/49502 (8%) | Loss: 0.294658\n",
      "Train Epoch: 49 | Batch Status: 5000/49502 (10%) | Loss: 0.460582\n",
      "Train Epoch: 49 | Batch Status: 6000/49502 (12%) | Loss: 0.104099\n",
      "Train Epoch: 49 | Batch Status: 7000/49502 (14%) | Loss: 0.331820\n",
      "Train Epoch: 49 | Batch Status: 8000/49502 (16%) | Loss: 0.362921\n",
      "Train Epoch: 49 | Batch Status: 9000/49502 (18%) | Loss: 0.222508\n",
      "Train Epoch: 49 | Batch Status: 10000/49502 (20%) | Loss: 0.221704\n",
      "Train Epoch: 49 | Batch Status: 11000/49502 (22%) | Loss: 0.394910\n",
      "Train Epoch: 49 | Batch Status: 12000/49502 (24%) | Loss: 0.793852\n",
      "Train Epoch: 49 | Batch Status: 13000/49502 (26%) | Loss: 0.398887\n",
      "Train Epoch: 49 | Batch Status: 14000/49502 (28%) | Loss: 0.475026\n",
      "Train Epoch: 49 | Batch Status: 15000/49502 (30%) | Loss: 0.476825\n",
      "Train Epoch: 49 | Batch Status: 16000/49502 (32%) | Loss: 0.344124\n",
      "Train Epoch: 49 | Batch Status: 17000/49502 (34%) | Loss: 0.042395\n",
      "Train Epoch: 49 | Batch Status: 18000/49502 (36%) | Loss: 0.176583\n",
      "Train Epoch: 49 | Batch Status: 19000/49502 (38%) | Loss: 0.048004\n",
      "Train Epoch: 49 | Batch Status: 20000/49502 (40%) | Loss: 0.174803\n",
      "Train Epoch: 49 | Batch Status: 21000/49502 (42%) | Loss: 0.127044\n",
      "Train Epoch: 49 | Batch Status: 22000/49502 (44%) | Loss: 0.903924\n",
      "Train Epoch: 49 | Batch Status: 23000/49502 (46%) | Loss: 0.558595\n",
      "Train Epoch: 49 | Batch Status: 24000/49502 (48%) | Loss: 0.063343\n",
      "Train Epoch: 49 | Batch Status: 25000/49502 (50%) | Loss: 0.235940\n",
      "Train Epoch: 49 | Batch Status: 26000/49502 (53%) | Loss: 0.330982\n",
      "Train Epoch: 49 | Batch Status: 27000/49502 (55%) | Loss: 0.378040\n",
      "Train Epoch: 49 | Batch Status: 28000/49502 (57%) | Loss: 0.484701\n",
      "Train Epoch: 49 | Batch Status: 29000/49502 (59%) | Loss: 0.341677\n",
      "Train Epoch: 49 | Batch Status: 30000/49502 (61%) | Loss: 0.548089\n",
      "Train Epoch: 49 | Batch Status: 31000/49502 (63%) | Loss: 0.084582\n",
      "Train Epoch: 49 | Batch Status: 32000/49502 (65%) | Loss: 0.261221\n",
      "Train Epoch: 49 | Batch Status: 33000/49502 (67%) | Loss: 0.345642\n",
      "Train Epoch: 49 | Batch Status: 34000/49502 (69%) | Loss: 1.008211\n",
      "Train Epoch: 49 | Batch Status: 35000/49502 (71%) | Loss: 0.144848\n",
      "Train Epoch: 49 | Batch Status: 36000/49502 (73%) | Loss: 0.498555\n",
      "Train Epoch: 49 | Batch Status: 37000/49502 (75%) | Loss: 0.320863\n",
      "Train Epoch: 49 | Batch Status: 38000/49502 (77%) | Loss: 0.456503\n",
      "Train Epoch: 49 | Batch Status: 39000/49502 (79%) | Loss: 0.281376\n",
      "Train Epoch: 49 | Batch Status: 40000/49502 (81%) | Loss: 1.444919\n",
      "Train Epoch: 49 | Batch Status: 41000/49502 (83%) | Loss: 0.561721\n",
      "Train Epoch: 49 | Batch Status: 42000/49502 (85%) | Loss: 0.585486\n",
      "Train Epoch: 49 | Batch Status: 43000/49502 (87%) | Loss: 0.532521\n",
      "Train Epoch: 49 | Batch Status: 44000/49502 (89%) | Loss: 0.437042\n",
      "Train Epoch: 49 | Batch Status: 45000/49502 (91%) | Loss: 0.169607\n",
      "Train Epoch: 49 | Batch Status: 46000/49502 (93%) | Loss: 0.358380\n",
      "Train Epoch: 49 | Batch Status: 47000/49502 (95%) | Loss: 0.744925\n",
      "Train Epoch: 49 | Batch Status: 48000/49502 (97%) | Loss: 0.503019\n",
      "Train Epoch: 49 | Batch Status: 49000/49502 (99%) | Loss: 0.274715\n",
      "Training time: 0m 6s\n",
      "==========================\n",
      "Test set: Average loss: 1.2971, Accuracy: 5915/12376(47.000000%)\n",
      "Testing time: 0m 6s\n",
      "Total Time: 5m 20s\n",
      "Model was trained on cpu!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    since = time.time()\n",
    "    for epoch in range(1,50):\n",
    "        epoch_start = time.time()\n",
    "        train(epoch)\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
    "        test()\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
    "    m, s = divmod(time.time() - since, 60)\n",
    "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on cpu!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "1\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "2\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "3\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "4\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "5\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "6\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "7\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "8\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "9\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "10\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "11\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "12\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "13\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "14\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "15\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "16\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "17\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "18\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "19\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "20\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "21\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "22\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "23\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "24\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "25\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "26\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "27\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "28\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "29\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "30\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "31\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "32\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "33\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "34\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "35\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "36\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "37\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "38\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "39\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "40\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "41\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "42\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "43\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "44\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "45\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "46\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "47\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "48\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "49\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "50\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "51\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "52\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "53\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "54\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "55\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "56\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "57\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "58\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "59\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "60\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "61\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "62\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "63\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "64\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "65\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "66\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "67\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "68\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "69\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "70\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "71\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "72\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "73\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "74\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "75\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "76\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "77\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "78\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "79\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "80\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "81\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "82\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "83\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "84\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "85\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "86\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "87\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "88\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "89\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "90\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "91\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "92\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "93\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "94\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "95\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "96\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "97\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "98\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "99\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "100\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "101\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "102\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "103\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "104\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "105\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "106\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "107\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "108\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "109\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "110\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "111\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "112\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "113\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "114\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "115\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "116\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "117\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "118\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "119\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "120\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "121\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "122\n",
      "torch.Size([100, 93])\n",
      "torch.Size([100, 1])\n",
      "123\n",
      "torch.Size([76, 93])\n",
      "torch.Size([76, 1])\n"
     ]
    }
   ],
   "source": [
    "for idx, (data, target) in enumerate(test_loader):\n",
    "    print(idx)\n",
    "    print(data.shape)\n",
    "    print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
